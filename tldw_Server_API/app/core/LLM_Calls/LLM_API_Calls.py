# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import os
import time
from typing import List, Any, Optional, Tuple, Dict, Union
#
# Import 3rd-Party Libraries
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
#
# Import Local libraries
from tldw_Server_API.app.core.config import load_and_log_configs
from tldw_Server_API.app.core.Utils.Utils import logging
from tldw_Server_API.app.core.Chat.Chat_Functions import ChatAuthenticationError, ChatRateLimitError, \
    ChatBadRequestError, ChatProviderError, ChatConfigurationError
#
#######################################################################################################################
# Function Definitions
#

# FIXME: Update to include full arguments

# --- Helper function for safe type conversion ---
def _safe_cast(value: Any, cast_to: type, default: Any = None) -> Any:
    """Safely casts value to specified type, returning default on failure."""
    if value is None:
        return default
    try:
        return cast_to(value)
    except (ValueError, TypeError):
        logging.warning(f"Could not cast '{value}' to {cast_to}. Using default: {default}")
        return default

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def _parse_data_url_for_multimodal(data_url: str) -> Optional[Tuple[str, str]]:
    """Parses a data URL (e.g., data:image/png;base64,xxxx) into (mime_type, base64_data)."""
    if data_url.startswith("data:") and ";base64," in data_url:
        try:
            header, b64_data = data_url.split(";base64,", 1)
            mime_type = header.split("data:", 1)[1]
            return mime_type, b64_data
        except Exception as e:
            logging.warning(f"Could not parse data URL: {data_url[:60]}... Error: {e}")
            return None
    logging.debug(f"Data URL did not match expected format: {data_url[:60]}...")
    return None


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.
    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.
    Returns:
        List[float]: The embeddings generated by the API.
    """
    loaded_config_data = load_and_log_configs()
    api_key = loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI Embeddings: API key not found or is empty")
        raise ValueError("OpenAI Embeddings: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI Embeddings: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI Embeddings: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI Embeddings: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    request_data = {
        "input": input_data,
        "model": model,
    }
    try:
        logging.debug("OpenAI Embeddings: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI Embeddings: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI Embeddings: Embedding data not found in the response")
                raise ValueError("OpenAI Embeddings: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI Embeddings: request failed with status code {response.status_code}")
            logging.error(f"OpenAI Embeddings: Error response: {response.text}")
            # Propagate HTTPError to be caught by chat_api_call's handler (if this were called from there)
            # Or raise specific error if called directly
            response.raise_for_status() # This will raise HTTPError
            # Fallback if raise_for_status doesn't cover it (it should)
            raise ValueError(f"OpenAI Embeddings: Failed to retrieve. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI Embeddings: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI Embeddings: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Unexpected error occurred: {str(e)}")


def chat_with_openai(
        input_data: List[Dict[str, Any]],  # Mapped from 'messages_payload'
        model: Optional[str] = None,  # Mapped from 'model'
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        temp: Optional[float] = None,  # Mapped from 'temp' (temperature)
        maxp: Optional[float] = None,  # Mapped from 'maxp' (top_p)
        streaming: Optional[bool] = False,  # Mapped from 'streaming'
        # New OpenAI specific parameters (and some from original ChatCompletionRequest schema)
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,  # True/False
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,  # This was already implicitly handled by config, now explicit
        n: Optional[int] = None,  # Number of completions
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, str]] = None,  # e.g., {"type": "json_object"}
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        user: Optional[str] = None, # This is the 'user_identifier' mapped
        custom_prompt_arg: Optional[str] = None # Legacy
):
    """
    Sends a chat completion request to the OpenAI API.

    Args:
        input_data: List of message objects (OpenAI format).
        model: ID of the model to use.
        api_key: OpenAI API key.
        system_message: Optional system message to prepend.
        temp: Sampling temperature.
        maxp: Top-p (nucleus) sampling parameter.
        streaming: Whether to stream the response.
        frequency_penalty: Penalizes new tokens based on their existing frequency.
        logit_bias: Modifies the likelihood of specified tokens appearing.
        logprobs: Whether to return log probabilities of output tokens.
        top_logprobs: An integer between 0 and 5 specifying the number of most likely tokens to return at each token position.
        max_tokens: Maximum number of tokens to generate.
        n: How many chat completion choices to generate for each input message.
        presence_penalty: Penalizes new tokens based on whether they appear in the text so far.
        response_format: An object specifying the format that the model must output. e.g. {"type": "json_object"}.
        seed: This feature is in Beta. If specified, the system will make a best effort to sample deterministically.
        stop: Up to 4 sequences where the API will stop generating further tokens.
        tools: A list of tools the model may call.
        tool_choice: Controls which (if any) function is called by the model.
        user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
        custom_prompt_arg: Legacy, largely ignored.
        **kwargs: Catches any unexpected keyword arguments.
    """
    loaded_config_data = load_and_log_configs()
    openai_config = loaded_config_data.get('openai_api', {})

    final_api_key = api_key or openai_config.get('api_key')
    if not final_api_key:
        logging.error("OpenAI: API key is missing.")
        raise ChatConfigurationError(provider="openai", message="OpenAI API Key is required but not found.")

    log_key_display = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(final_api_key) > 9 else "Key Provided"
    logging.debug(f"OpenAI: Using API Key: {log_key_display}")

    # Resolve parameters: User-provided > Function arg default > Config default > Hardcoded default
    final_model = model if model is not None else openai_config.get('model', 'gpt-4o-mini')
    final_temp = temp if temp is not None else float(openai_config.get('temperature', 0.7))
    final_top_p = maxp if maxp is not None else float(
        openai_config.get('top_p', 0.95))  # 'maxp' from chat_api_call maps to 'top_p'

    final_streaming_cfg = openai_config.get('streaming', False)
    final_streaming = streaming if streaming is not None else \
        (str(final_streaming_cfg).lower() == 'true' if isinstance(final_streaming_cfg, str) else bool(final_streaming_cfg))

    final_max_tokens = max_tokens if max_tokens is not None else _safe_cast(openai_config.get('max_tokens'), int)

    if custom_prompt_arg:
        logging.warning(
            "OpenAI: 'custom_prompt_arg' was provided but is generally ignored if 'input_data' and 'system_message' are used correctly.")

    # Construct messages for OpenAI API
    api_messages = []
    has_system_message_in_input = any(msg.get("role") == "system" for msg in input_data)
    if system_message and not has_system_message_in_input:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    payload = {
        "model": final_model,
        "messages": api_messages,
        "stream": final_streaming,
    }
    # Add optional parameters if they have a value
    if final_temp is not None: payload["temperature"] = final_temp
    if final_top_p is not None: payload["top_p"] = final_top_p # OpenAI uses top_p
    if final_max_tokens is not None: payload["max_tokens"] = final_max_tokens
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if logit_bias is not None: payload["logit_bias"] = logit_bias
    if logprobs is not None: payload["logprobs"] = logprobs
    if top_logprobs is not None and payload.get("logprobs") is True:
        payload["top_logprobs"] = top_logprobs
    elif top_logprobs is not None:
         logging.warning("OpenAI: 'top_logprobs' provided but 'logprobs' is not true. 'top_logprobs' will be ignored.")
    if n is not None: payload["n"] = n
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty
    if response_format is not None: payload["response_format"] = response_format
    if seed is not None: payload["seed"] = seed
    if stop is not None: payload["stop"] = stop
    if tools is not None: payload["tools"] = tools
    if tools is not None: payload["tools"] = tools

    # Then conditionally add tool_choice:
    if payload.get("tools") and tool_choice is not None:
        payload["tool_choice"] = tool_choice
    elif tool_choice == "none":  # Allow "none" even if no tools are present
        payload["tool_choice"] = "none"
    if user is not None: payload["user"] = user # 'user' is OpenAI's user identifier field

    headers = {
        'Authorization': f'Bearer {final_api_key}',
        'Content-Type': 'application/json'
    }
    logging.debug(f"OpenAI Request Payload (excluding messages): {{k: v for k, v in payload.items() if k != 'messages'}}")

    api_url = openai_config.get('api_base_url', 'https://api.openai.com/v1').rstrip('/') + '/chat/completions'
    try:
        if final_streaming:
            logging.debug("OpenAI: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=payload, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"  # Adhere to SSE format for client
                        # OpenAI stream itself does not send a final [DONE] in this way,
                        # but our endpoint wrapper expects it.
                        # The actual DONE event should be data: [DONE]\n\n
                    except requests.exceptions.ChunkedEncodingError as e_chunk:
                        logging.error(f"OpenAI: ChunkedEncodingError during stream: {e_chunk}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream connection error: {str(e_chunk)}",
                                                              "type": "openai_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    except Exception as e_stream:
                        logging.error(f"OpenAI: Error during stream iteration: {e_stream}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream iteration error: {str(e_stream)}",
                                                              "type": "openai_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    finally:
                        # Ensure DONE is sent for the endpoint wrapper's logic
                        yield "data: [DONE]\n\n"
                        if response:
                            response.close()

                return stream_generator()

        else:  # Non-streaming
            logging.debug("OpenAI: Posting request (non-streaming)")
            retry_count = int(openai_config.get('api_retries', 3))
            retry_delay = float(openai_config.get('api_retry_delay', 1.0))  # Ensure float

            retry_strategy = Retry(
                total=retry_count,
                backoff_factor=retry_delay,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["POST"]  # Changed from method_whitelist
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                session.mount("http://", adapter)  # Though OpenAI is https
                response = session.post(api_url, headers=headers, json=payload,
                                        timeout=float(openai_config.get('api_timeout', 90.0)))

            logging.debug(f"OpenAI: Full API response status: {response.status_code}")
            response.raise_for_status()  # Raise HTTPError for 4xx/5xx AFTER retries
            response_data = response.json()
            logging.debug("OpenAI: Non-streaming request successful.")
            return response_data

    except requests.exceptions.HTTPError as e:
        error_content_text = "No response text"
        error_content_json = None
        if e.response is not None:
            logging.error(f"OpenAI Full Error Response (status {e.response.status_code}): {e.response.text}")
        else:
            logging.error(f"OpenAI HTTPError with no response object: {e}")
        raise
        # if e.response is not None:
        #     error_content_text = e.response.text
        #     try:
        #         error_content_json = e.response.json()
        #     except json.JSONDecodeError:
        #         pass
        # logging.error(
        #     f"OpenAI HTTPError {e.response.status_code if e.response is not None else 'Unknown'}. Text: {error_content_text}. JSON: {error_content_json}",
        #     exc_info=True)
        # raise
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI RequestException: {e}", exc_info=True)
        raise
    except Exception as e: # Catch any other unexpected error
        logging.error(f"OpenAI: Unexpected error in chat_with_openai: {e}", exc_info=True)
        raise ChatProviderError(provider="openai", message=f"Unexpected error: {e}")


def chat_with_anthropic(
        input_data: List[Dict[str, Any]], # Mapped from 'messages_payload'
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_prompt: Optional[str] = None, # Mapped from 'system_message'
        temp: Optional[float] = None,
        topp: Optional[float] = None,       # Mapped from 'topp' (becomes top_p)
        topk: Optional[int] = None,
        streaming: Optional[bool] = False,
        max_tokens: Optional[int] = None,   # New: Anthropic uses 'max_tokens'
        stop_sequences: Optional[List[str]] = None, # New: Mapped from 'stop'
        tools: Optional[List[Dict[str, Any]]] = None, # New: Anthropic tool format
        # Anthropic doesn't typically use seed, response_format (for JSON object mode directly), n, user identifier, logit_bias,
        # presence_penalty, frequency_penalty, logprobs, top_logprobs in the same way as OpenAI.
        # tool_choice is usually implicit with tools or controlled differently.
        custom_prompt_arg: Optional[str] = None # Legacy
):
    # Assuming load_and_log_configs is defined elsewhere
    loaded_config_data = load_and_log_configs()
    anthropic_config = loaded_config_data.get('anthropic_api', {})
    final_api_key = api_key or anthropic_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="anthropic", message="Anthropic API Key is required.")

    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(final_api_key) > 9 else "Key Provided"
    logging.debug(f"Anthropic: Using API Key: {log_key}")

    current_model = model or anthropic_config.get('model', 'claude-3-haiku-20240307')
    current_temp = temp if temp is not None else float(anthropic_config.get('temperature', 0.7))
    current_top_p = topp
    current_top_k = topk
    current_streaming_cfg = anthropic_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(current_streaming_cfg))

    # Use the passed max_tokens if available, else config, else a default
    default_max_tokens = int(anthropic_config.get('max_tokens_to_sample', anthropic_config.get('max_tokens', 4096)))
    current_max_tokens = max_tokens if max_tokens is not None else default_max_tokens


    anthropic_messages = []
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")
        if role not in ["user", "assistant"]:
            logging.warning(f"Anthropic: Skipping message with unsupported role: {role}")
            continue
        # ... (multimodal content processing for Anthropic from your existing function) ...
        anthropic_content_parts = []
        if isinstance(content, str):
            anthropic_content_parts.append({"type": "text", "text": content})
        elif isinstance(content, list): # OpenAI content part list
            for part in content:
                part_type = part.get("type")
                if part_type == "text":
                    anthropic_content_parts.append({"type": "text", "text": part.get("text", "")})
                elif part_type == "image_url":
                    image_url_obj = part.get("image_url", {})
                    url_str = image_url_obj.get("url", "")
                    parsed_image = _parse_data_url_for_multimodal(url_str)
                    if parsed_image:
                        mime_type, b64_data = parsed_image
                        anthropic_content_parts.append({
                            "type": "image",
                            "source": {"type": "base64", "media_type": mime_type, "data": b64_data}
                        })
        if anthropic_content_parts:
             anthropic_messages.append({"role": role, "content": anthropic_content_parts})


    if not any(m['role'] == 'user' for m in anthropic_messages):
        raise ChatBadRequestError(provider="anthropic", message="No valid user messages found for Anthropic.")

    headers = {
        'x-api-key': final_api_key,
        'anthropic-version': anthropic_config.get('api_version', '2023-06-01'),
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "max_tokens": current_max_tokens, # Changed from max_tokens_to_sample to the parameter
        "messages": anthropic_messages,
        "stream": current_streaming,
    }
    if system_prompt is not None: data["system"] = system_prompt # Anthropic uses 'system' at the top level
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if stop_sequences is not None: data["stop_sequences"] = stop_sequences
    if tools is not None: data["tools"] = tools # Assuming 'tools' is already in Anthropic's required format

    api_url = anthropic_config.get('api_base_url', 'https://api.anthropic.com/v1').rstrip('/') + '/messages'
    logging.debug(f"Anthropic Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        retry_count = int(anthropic_config.get('api_retries', 3))
        retry_delay = float(anthropic_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=current_streaming, timeout=180)
        response.raise_for_status()

        if current_streaming:
            logging.debug("Anthropic: Streaming response received. Normalizing to OpenAI SSE.")
            def stream_generator():
                completion_id = f"chatcmpl-anthropic-{time.time_ns()}"
                created_time = int(time.time())

                event_name = None
                data_buffer = []

                try:
                    for line_bytes in response.iter_lines():  # iter_lines gives bytes
                        line = line_bytes.decode('utf-8').strip()
                        if not line: continue # Skip keep-alive newlines
                        # Anthropic SSE has "event:" and "data:" lines
                        # Parse them and reformat
                        # Example (simplified, actual Anthropic events are more complex):
                        if line.startswith("data:"):
                            event_data_str = line[len("data:"):].strip()
                            try:
                                anthropic_event = json.loads(event_data_str)
                                # Extract delta and finish_reason based on anthropic_event['type']
                                # (e.g., 'content_block_delta', 'message_delta')
                                delta_content = None
                                finish_reason = None
                                if anthropic_event.get("type") == "content_block_delta":
                                    delta = anthropic_event.get("delta", {})
                                    if delta.get("type") == "text_delta":
                                        delta_content = delta.get("text")
                                elif anthropic_event.get("type") == "message_delta":
                                    finish_reason_anth = anthropic_event.get("delta", {}).get("stop_reason")
                                    if finish_reason_anth:
                                        finish_reason_map = {"end_turn": "stop", "max_tokens": "length", "stop_sequence": "stop", "tool_use": "tool_calls"}
                                        finish_reason = finish_reason_map.get(finish_reason_anth, finish_reason_anth)

                                if delta_content:
                                    sse_chunk = {"id": completion_id, "object": "chat.completion.chunk", "created": created_time, "model": current_model, "choices": [{"index": 0, "delta": {"content": delta_content}, "finish_reason": None}]}
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                                if finish_reason:
                                    sse_chunk = {"id": completion_id, "object": "chat.completion.chunk", "created": created_time, "model": current_model, "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}]}
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                            except json.JSONDecodeError:
                                logging.warning(f"Anthropic Stream: Could not decode JSON: {event_data_str}")
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e: # ... error handling ...
                    logging.error(f"Anthropic: ChunkedEncodingError during stream: {e}", exc_info=True)
                    yield f"data: {json.dumps({'error': {'message': f'Stream connection error: {str(e)}', 'type': 'anthropic_stream_error'}})}\n\n"
                except Exception as e: # ... error handling ...
                    logging.error(f"Anthropic: Error during stream iteration: {e}", exc_info=True)
                    yield f"data: {json.dumps({'error': {'message': f'Stream iteration error: {str(e)}', 'type': 'anthropic_stream_error'}})}\n\n"
                finally:
                    yield "data: [DONE]\n\n" # Crucial for client
                    if response: response.close()
            return stream_generator()
        else:
            # ... (non-streaming logic remains the same) ...
            logging.debug("Anthropic: Non-streaming request successful.")
            response_data = response.json()
            logging.debug("Anthropic: Non-streaming request successful. Normalizing response.")
            assistant_content_parts = []
            if response_data.get("content"):
                for part in response_data.get("content", []):
                    if part.get("type") == "text":
                        assistant_content_parts.append(part.get("text", ""))
            full_assistant_content = "\n".join(assistant_content_parts).strip()
            finish_reason_map = {"end_turn": "stop", "max_tokens": "length", "stop_sequence": "stop", "tool_use": "tool_calls"} # Added tool_use
            openai_finish_reason = finish_reason_map.get(response_data.get("stop_reason"), response_data.get("stop_reason"))
            normalized_response = {
                "id": response_data.get("id", f"anthropic-{time.time_ns()}"),
                "object": "chat.completion",
                "created": int(time.time()),
                "model": response_data.get("model", current_model),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": full_assistant_content},
                             "finish_reason": openai_finish_reason}],
                "usage": response_data.get("usage")
            }
            return normalized_response

    except requests.exceptions.HTTPError as e:
        # ... (error handling from your file, ensure provider is "anthropic") ...
        status_code = e.response.status_code if e.response is not None else 500
        error_text = e.response.text if e.response is not None else "No response text"
        if status_code == 401: raise ChatAuthenticationError(provider="anthropic", message=f"Auth failed. Detail: {error_text[:200]}") from e
        elif status_code == 429: raise ChatRateLimitError(provider="anthropic", message=f"Rate limit. Detail: {error_text[:200]}") from e
        elif 400 <= status_code < 500: raise ChatBadRequestError(provider="anthropic", message=f"Bad request ({status_code}). Detail: {error_text[:200]}") from e
        else: raise ChatProviderError(provider="anthropic", message=f"API error ({status_code}). Detail: {error_text[:200]}", status_code=status_code) from e
    except requests.exceptions.RequestException as e:
        raise ChatProviderError(provider="anthropic", message=f"Network error: {str(e)}", status_code=504) from e
    except Exception as e:
        logging.error(f"Anthropic: Unexpected error: {e}", exc_info=True)
        raise ChatProviderError(provider="anthropic", message=f"Unexpected error: {e}")


def chat_with_cohere(
        input_data: List[Dict[str, Any]],  # Mapped from 'messages_payload'
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_prompt: Optional[str] = None,  # Mapped from 'system_message', becomes 'preamble'
        temp: Optional[float] = None,
        topp: Optional[float] = None,  # Mapped from 'topp', becomes 'p'
        topk: Optional[int] = None,  # Mapped from 'topk', becomes 'k'
        streaming: Optional[bool] = False,
        max_tokens: Optional[int] = None,
        stop_sequences: Optional[List[str]] = None,  # Mapped from 'stop'
        seed: Optional[int] = None,
        num_generations: Optional[int] = None,  # Mapped from 'n'
        frequency_penalty: Optional[float] = None,
        presence_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, float]] = None,  # Cohere calls it 'logit_bias' too
        tools: Optional[List[Dict[str, Any]]] = None,
        # Cohere's tool_choice equivalent is 'force_single_tool' or managed via prompt/tool results.
        # For simplicity, direct 'tool_choice' as OpenAI is complex here.
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    loaded_config_data = load_and_log_configs()
    cohere_config = loaded_config_data.get('cohere_api', {})
    final_api_key = api_key or cohere_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="cohere", message="Cohere API Key is required.")

    log_key = f"{final_api_key[:3]}...{final_api_key[-3:]}" if final_api_key and len(
        final_api_key) > 5 else "Key Provided"
    logging.debug(f"Cohere: Using API Key: {log_key}")

    current_model = model or cohere_config.get('model', 'command-r')
    current_temp = temp if temp is not None else float(cohere_config.get('temperature', 0.3))
    current_p = topp  # Cohere uses 'p'
    current_k = topk  # Cohere uses 'k'
    current_streaming_cfg = cohere_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(cohere_config.get('max_tokens'), int)

    chat_history = []
    user_message_text = ""
    # Cohere expects the last message to be the current user query, and previous ones in chat_history
    # System prompt becomes 'preamble'
    if system_prompt:
        # Cohere's 'preamble' is a top-level field.
        pass  # Will be added to 'data' later

    for i, msg in enumerate(input_data):
        role = msg.get("role")
        content = msg.get("content")
        # Cohere only supports text content directly in chat history/message for /v1/chat
        # Multimodal (images) needs different handling or isn't supported by /v1/chat.
        # For simplicity, we'll extract text.
        current_msg_text = ""
        if isinstance(content, str):
            current_msg_text = content
        elif isinstance(content, list):
            for part in content:
                if part.get("type") == "text":
                    current_msg_text += part.get("text", "") + "\n"
                elif part.get("type") == "image_url":
                    logging.warning("Cohere /v1/chat: Images in messages are not directly supported, text extracted.")
            current_msg_text = current_msg_text.strip()

        if not current_msg_text and role != "assistant":  # Allow empty assistant response in history
            logging.warning(f"Cohere: Skipping empty message for role {role}")
            continue

        if i == len(input_data) - 1 and role == "user":
            user_message_text = current_msg_text
        elif role == "user":
            chat_history.append({"role": "USER", "message": current_msg_text})
        elif role == "assistant":
            chat_history.append({"role": "CHATBOT", "message": current_msg_text})
        # System messages are handled by 'preamble'

    if not user_message_text:  # If the last message wasn't a user message or was empty
        if chat_history and chat_history[-1]["role"] == "CHATBOT":  # if last history item was bot
            raise ChatBadRequestError(provider="cohere",
                                      message="Last message must be from USER for Cohere API, or provide a new user message.")
        elif not chat_history:  # No history and no current user message
            raise ChatBadRequestError(provider="cohere", message="No user message provided for Cohere API.")
        # If history exists and last message was user, it might be implicitly the current message
        # but Cohere API prefers explicit 'message' field.
        # For robustness, it's better if the last message in `input_data` is always user for Cohere.

    headers = {'accept': 'application/json', 'content-type': 'application/json',
               'Authorization': f'Bearer {final_api_key}'}
    data = {"model": current_model, "message": user_message_text}  # 'message' is the current user query

    if system_prompt is not None: data["preamble"] = system_prompt
    if chat_history: data["chat_history"] = chat_history
    if current_temp is not None: data["temperature"] = current_temp
    if current_p is not None: data["p"] = current_p
    if current_k is not None: data["k"] = current_k
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if stop_sequences is not None: data["stop_sequences"] = stop_sequences
    if seed is not None: data["seed"] = seed
    if num_generations is not None: data["num_generations"] = num_generations  # 'n' mapped
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if logit_bias is not None: data["logit_bias"] = logit_bias
    if tools is not None: data["tools"] = tools  # Assuming 'tools' is in Cohere's format

    # Cohere v1/chat uses stream as query param. v2/chat uses it in body.
    # Assuming you are using v1/chat based on your original code for chat_with_cohere.
    api_url_base = cohere_config.get('api_base_url', 'https://api.cohere.com/v1').rstrip('/')
    api_url = f"{api_url_base}/chat"
    request_params = {}
    if current_streaming:
        request_params["stream"] = "true"  # Query parameter for v1

    logging.debug(f"Cohere Request Payload: {data}")
    logging.debug(f"Cohere Request URL: {api_url}, Params: {request_params}")

    try:
        # ... (retry logic from your file) ...
        retry_count = int(cohere_config.get('api_retries', 3))
        retry_delay = float(cohere_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                               status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=data, params=request_params,
                                    stream=current_streaming, timeout=180)
        response.raise_for_status()

        if current_streaming:
            # ... (streaming normalization logic from your file for Cohere, ensure "Cohere" in logs) ...
            # (This part looks okay from your provided code, focusing on 'text-generation' and 'stream-end' events)
            logging.debug("Cohere: Streaming response received. Normalizing to OpenAI SSE.")

            def stream_generator():
                # ... (your existing Cohere stream normalization logic) ...
                completion_id = f"chatcmpl-cohere-{time.time_ns()}"
                created_time = int(time.time())
                buffer = ""
                stream_truly_finished_by_cohere = False
                try:
                    for chunk_bytes in response.iter_content(chunk_size=None):  # Cohere stream is line-delimited JSONs
                        if not chunk_bytes: continue
                        buffer += chunk_bytes.decode('utf-8', errors='replace')
                        while '\n' in buffer:
                            line, buffer = buffer.split('\n', 1)
                            if not line.strip(): continue
                            try:
                                cohere_event = json.loads(line)
                                delta_text = None
                                finish_reason_str = None
                                # Cohere v1 streaming format might differ, check their docs
                                # This assumes events like {"text": "...", "is_finished": false}
                                # or {"event_type": "text-generation", "text": "..."}
                                # or {"event_type": "stream-end", "finish_reason": "..."}
                                if "text" in cohere_event and not cohere_event.get("is_finished"):
                                    delta_text = cohere_event.get("text")
                                if cohere_event.get("is_finished"):
                                    stream_truly_finished_by_cohere = True
                                    finish_reason_str = cohere_event.get("finish_reason", "COMPLETE").lower()
                                    if finish_reason_str == "complete":
                                        finish_reason_str = "stop"
                                    elif finish_reason_str == "max_tokens":
                                        finish_reason_str = "length"

                                if delta_text:
                                    sse_chunk = {"id": completion_id, "object": "chat.completion.chunk",
                                                 "created": created_time, "model": current_model, "choices": [
                                            {"index": 0, "delta": {"content": delta_text}, "finish_reason": None}]}
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                                if finish_reason_str:
                                    sse_chunk = {"id": completion_id, "object": "chat.completion.chunk",
                                                 "created": created_time, "model": current_model, "choices": [
                                            {"index": 0, "delta": {}, "finish_reason": finish_reason_str}]}
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                                if stream_truly_finished_by_cohere: break
                            except json.JSONDecodeError:
                                logging.warning(f"Cohere Stream: Could not decode JSON from line: '{line}'")
                        if stream_truly_finished_by_cohere: break
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:  # ... error handling ...
                    logging.error(f"Cohere: ChunkedEncodingError: {e}", exc_info=True)
                    yield f"data: {json.dumps({'error': {'message': f'Stream error: {str(e)}', 'type': 'cohere_stream_error'}})}\n\n"
                except Exception as e:  # ... error handling ...
                    logging.error(f"Cohere: Stream iteration error: {e}", exc_info=True)
                    yield f"data: {json.dumps({'error': {'message': f'Stream iteration error: {str(e)}', 'type': 'cohere_stream_error'}})}\n\n"
                finally:
                    yield "data: [DONE]\n\n"
                    if response: response.close()

            return stream_generator()
        else:
            # ... (non-streaming normalization from your file, ensure "Cohere" in logs) ...
            # (This part looks okay, mapping 'text' and 'finish_reason')
            response_data = response.json()
            logging.debug("Cohere: Non-streaming request successful.")
            finish_reason_map = {"COMPLETE": "stop", "MAX_TOKENS": "length", "ERROR_LIMIT": "length",
                                 "ERROR_TOXIC": "content_filter", "ERROR": "error"}
            cohere_finish_reason = response_data.get("finish_reason", "UNKNOWN")
            openai_finish_reason = finish_reason_map.get(cohere_finish_reason, cohere_finish_reason.lower())
            usage_data = response_data.get("meta", {}).get("billed_units", {})  # Default if not present
            if not usage_data and "token_count" in response_data:  # Older API might have token_count
                usage_data = {
                    "input_tokens": response_data["token_count"].get("prompt_tokens"),
                    "output_tokens": response_data["token_count"].get("response_tokens")  # Or "completion_tokens"
                }

            prompt_tokens = usage_data.get("input_tokens")
            completion_tokens = usage_data.get("output_tokens")
            total_tokens = None
            if prompt_tokens is not None and completion_tokens is not None:
                total_tokens = prompt_tokens + completion_tokens

            normalized_response = {
                "id": response_data.get("generation_id", f"cohere-{time.time_ns()}"),
                "object": "chat.completion", "created": int(time.time()),
                "model": current_model,
                # Cohere response includes "meta":{"api_version":{"version":"1"},"billed_units":{"input_tokens":X,"output_tokens":Y}}
                # but not the model name directly in the main body for /v1/chat
                "choices": [
                    {"index": 0, "message": {"role": "assistant", "content": response_data.get("text", "").strip()},
                     "finish_reason": openai_finish_reason}],
                "usage": {"prompt_tokens": prompt_tokens, "completion_tokens": completion_tokens,
                          "total_tokens": total_tokens}
            }
            # If tools were used and results are in response_data["tool_calls"], normalize them.
            if response_data.get("tool_calls"):
                normalized_response["choices"][0]["message"]["tool_calls"] = response_data.get(
                    "tool_calls")  # Assuming direct pass-through is OpenAI compatible enough

            return normalized_response
    except requests.exceptions.HTTPError as e:
        # ... (error handling from your file, ensure provider is "cohere") ...
        status_code = e.response.status_code if e.response is not None else 500
        error_text = e.response.text if e.response is not None else "No response text"
        if status_code == 401:
            raise ChatAuthenticationError(provider="cohere", message=f"Auth failed. Detail: {error_text[:200]}") from e
        # ... (other status codes) ...
        else:
            raise ChatProviderError(provider="cohere", message=f"API error ({status_code}). Detail: {error_text[:200]}",
                                    status_code=status_code) from e
    except requests.exceptions.RequestException as e:
        raise ChatProviderError(provider="cohere", message=f"Network error: {str(e)}", status_code=504) from e
    except Exception as e:
        logging.error(f"Cohere: Unexpected error: {e}", exc_info=True)
        raise ChatProviderError(provider="cohere", message=f"Unexpected error: {e}")


def chat_with_deepseek(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,  # top_p
        # New OpenAI-compatible params for DeepSeek
        max_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, str]] = None,  # If supported
        n: Optional[int] = None,  # If supported
        user: Optional[str] = None,  # If supported
        tools: Optional[List[Dict[str, Any]]] = None,  # If supported
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,  # If supported
        logit_bias: Optional[Dict[str, float]] = None,  # If supported
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    loaded_config_data = load_and_log_configs()
    deepseek_config = loaded_config_data.get('deepseek_api', {})
    final_api_key = api_key or deepseek_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="deepseek", message="DeepSeek API Key required.")

    # ... (logging key, model, temp, streaming, top_p setup) ...
    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Key Provided"
    logging.debug(f"DeepSeek: Using API Key: {log_key}")
    current_model = model or deepseek_config.get('model', 'deepseek-chat')  # Or deepseek-coder
    current_temp = temp if temp is not None else float(deepseek_config.get('temperature', 0.1))
    current_top_p = topp  # Deepseek uses top_p
    current_streaming_cfg = deepseek_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(deepseek_config.get('max_tokens'), int)

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {'Authorization': f'Bearer {final_api_key}', 'Content-Type': 'application/json'}
    data = {
        "model": current_model, "messages": api_messages, "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if seed is not None: data["seed"] = seed
    if stop is not None: data["stop"] = stop
    if logprobs is not None: data["logprobs"] = logprobs  # DeepSeek uses 'logprobs' (boolean)
    if top_logprobs is not None and data.get("logprobs"): data["top_logprobs"] = top_logprobs
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if response_format is not None: data["response_format"] = response_format
    if n is not None: data["n"] = n
    if user is not None: data["user"] = user
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice
    if logit_bias is not None: data["logit_bias"] = logit_bias

    api_url = deepseek_config.get('api_base_url', 'https://api.deepseek.com').rstrip('/') + '/chat/completions'
    logging.debug(
        f"DeepSeek Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, use "DeepSeek" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming, retry) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(deepseek_config.get('api_retries', 3)),
                                                    backoff_factor=float(deepseek_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="deepseek", message=f"Unexpected error: {e}")


def chat_with_google(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,  # -> system_instruction
        temp: Optional[float] = None,  # -> temperature
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,  # -> topP
        topk: Optional[int] = None,  # -> topK
        max_output_tokens: Optional[int] = None,  # from max_tokens
        stop_sequences: Optional[List[str]] = None,  # from stop
        candidate_count: Optional[int] = None,  # from n
        response_format: Optional[Dict[str, str]] = None,  # for response_mime_type
        # Gemini doesn't directly take seed, user_id, logit_bias, presence/freq_penalty, logprobs via REST in the same way.
        # Tools are handled via a 'tools' field in the payload, with a specific format.
        tools: Optional[List[Dict[str, Any]]] = None,  # Gemini 'tools' config
        custom_prompt_arg: Optional[str] = None
):
    loaded_config_data = load_and_log_configs()
    google_config = loaded_config_data.get('google_api', {})
    # ... (api key, model, temp, streaming, topP, topK setup) ...
    final_api_key = api_key or google_config.get('api_key')
    if not final_api_key: raise ChatConfigurationError(provider="google", message="Google API Key required.")
    current_model = model or google_config.get('model', 'gemini-1.5-flash-latest')
    # ... other param resolutions ...
    current_streaming_cfg = google_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    gemini_contents = []
    # ... (message transformation from input_data to gemini_contents) ...
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")
        gemini_role = "user" if role == "user" else "model" if role == "assistant" else None
        if not gemini_role: continue
        gemini_parts = []
        if isinstance(content, str):
            gemini_parts.append({"text": content})
        elif isinstance(content, list):
            for part_obj in content:
                if part_obj.get("type") == "text":
                    gemini_parts.append({"text": part_obj.get("text", "")})
                elif part_obj.get("type") == "image_url":
                    parsed_image = _parse_data_url_for_multimodal(part_obj.get("image_url", {}).get("url", ""))
                    if parsed_image: gemini_parts.append(
                        {"inline_data": {"mime_type": parsed_image[0], "data": parsed_image[1]}})
        if gemini_parts: gemini_contents.append({"role": gemini_role, "parts": gemini_parts})

    generation_config = {}
    if temp is not None: generation_config["temperature"] = temp
    if topp is not None: generation_config["topP"] = topp
    if topk is not None: generation_config["topK"] = topk
    if max_output_tokens is not None: generation_config["maxOutputTokens"] = max_output_tokens
    if stop_sequences is not None: generation_config["stopSequences"] = stop_sequences
    if candidate_count is not None: generation_config["candidateCount"] = candidate_count
    if response_format and response_format.get("type") == "json_object":
        generation_config["responseMimeType"] = "application/json"

    payload = {"contents": gemini_contents}
    if generation_config: payload["generationConfig"] = generation_config
    if system_message: payload["system_instruction"] = {"parts": [{"text": system_message}]}
    if tools: payload["tools"] = tools  # Assuming 'tools' is in Gemini's specific format

    stream_suffix = ":streamGenerateContent?alt=sse" if current_streaming else ":generateContent"
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{current_model}{stream_suffix}"
    headers = {'x-goog-api-key': final_api_key, 'Content-Type': 'application/json'}
    logging.debug(f"Google Gemini Request Payload: {payload}")

    try:
        # ... (retry logic) ...
        adapter = HTTPAdapter(max_retries=Retry(total=int(google_config.get('api_retries', 3)),
                                                backoff_factor=float(google_config.get('api_retry_delay', 1)),
                                                status_forcelist=[429, 500, 503], allowed_methods=["POST"]))
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=payload, stream=current_streaming, timeout=180)
        response.raise_for_status()

        if current_streaming:
            # ... (Gemini streaming normalization logic from your file) ...
            # (This part looks generally okay, normalizing to OpenAI SSE deltas)
            logging.debug("Google Gemini: Streaming response received.")

            def stream_generator():
                # ... (your existing Gemini stream normalization) ...
                try:
                    for line in response.iter_lines(decode_unicode=True):
                        if line and line.strip().startswith('data:'):
                            json_str = line.strip()[len('data:'):]
                            try:
                                data_chunk_outer = json.loads(json_str)
                                # Gemini stream chunks are sometimes lists, sometimes single objects
                                data_chunks_to_process = data_chunk_outer if isinstance(data_chunk_outer, list) else [
                                    data_chunk_outer]

                                for data_chunk in data_chunks_to_process:
                                    chunk_text = ""
                                    finish_reason = None  # Check for finish reason in Gemini stream
                                    tool_calls_delta = None

                                    candidates = data_chunk.get('candidates', [])
                                    if candidates:
                                        candidate = candidates[0]
                                        # Text content
                                        if candidate.get('content', {}).get('parts', []):
                                            for part in candidate['content']['parts']:
                                                if 'text' in part:
                                                    chunk_text += part.get('text', '')
                                                # Check for functionCall delta if Gemini supports streaming them this way
                                                if 'functionCall' in part:
                                                    # This needs careful mapping to OpenAI's tool_calls delta
                                                    # For now, just logging it if complex
                                                    logging.debug(
                                                        f"Gemini Stream: Received functionCall part: {part['functionCall']}")
                                                    # Example: if part['functionCall'] has 'name' and 'args'
                                                    # tool_calls_delta = [{"index": 0, "id": "call_SOMEID", "type": "function", "function": {"name": part['functionCall']['name'], "arguments": part['functionCall']['args']}}]

                                        # Finish reason
                                        raw_finish_reason = candidate.get("finishReason")
                                        if raw_finish_reason:
                                            finish_reason_map = {"MAX_TOKENS": "length", "STOP": "stop",
                                                                 "SAFETY": "content_filter",
                                                                 "RECITATION": "content_filter", "OTHER": "error",
                                                                 "TOOL_CODE_NOT_FOUND": "error"}  # Simplified
                                            finish_reason = finish_reason_map.get(raw_finish_reason,
                                                                                  raw_finish_reason.lower())

                                    if chunk_text or tool_calls_delta:  # If there's text or tool call delta
                                        delta_payload = {}
                                        if chunk_text: delta_payload["content"] = chunk_text
                                        if tool_calls_delta: delta_payload[
                                            "tool_calls"] = tool_calls_delta  # This needs to be OpenAI's delta format for tool_calls

                                        sse_chunk = {'choices': [{'delta': delta_payload,
                                                                  "finish_reason": finish_reason if finish_reason else None,
                                                                  "index": 0}]}
                                        # Add common SSE fields if needed by client: id, object, created, model
                                        yield f"data: {json.dumps(sse_chunk)}\n\n"
                                    elif finish_reason:  # Only finish reason
                                        sse_chunk = {
                                            'choices': [{'delta': {}, "finish_reason": finish_reason, "index": 0}]}
                                        yield f"data: {json.dumps(sse_chunk)}\n\n"

                            except json.JSONDecodeError:
                                logging.warning(f"Google Gemini: Could not decode JSON line: {json_str}")
                    yield "data: [DONE]\n\n"
                # ... (error handling for stream) ...
                finally:
                    if response: response.close()

            return stream_generator()
        else:
            # ... (non-streaming normalization logic from your file) ...
            # (This part looks okay, normalizing candidates and usageMetadata)
            response_data = response.json()
            logging.debug("Google Gemini: Non-streaming request successful.")
            assistant_content = ""
            finish_reason = "unknown"
            tool_calls = None  # For non-streaming

            if response_data.get("candidates"):
                candidate = response_data["candidates"][0]
                if candidate.get("content", {}).get("parts"):
                    parts = candidate["content"]["parts"]
                    for part in parts:
                        if "text" in part:
                            assistant_content += part.get("text", "")
                        if "functionCall" in part:  # Handle non-streaming tool calls
                            if tool_calls is None: tool_calls = []
                            # Map Gemini functionCall to OpenAI tool_calls format
                            # This needs a unique ID for each call.
                            tool_calls.append({
                                "id": f"call_gemini_{time.time_ns()}_{len(tool_calls)}",  # Placeholder ID
                                "type": "function",
                                "function": {
                                    "name": part["functionCall"].get("name"),
                                    "arguments": json.dumps(part["functionCall"].get("args", {}))
                                    # Ensure args is stringified JSON
                                }
                            })

                raw_finish_reason = candidate.get("finishReason")
                if raw_finish_reason:
                    finish_reason_map = {"MAX_TOKENS": "length", "STOP": "stop", "SAFETY": "content_filter",
                                         "RECITATION": "content_filter", "OTHER": "error",
                                         "TOOL_CODE_NOT_FOUND": "error", "FUNCTION_CALL": "tool_calls"}
                    finish_reason = finish_reason_map.get(raw_finish_reason, raw_finish_reason.lower())

            message_content = {"role": "assistant", "content": assistant_content.strip()}
            if tool_calls:
                message_content["tool_calls"] = tool_calls
                if not assistant_content.strip():  # If only tool_calls, content might be null for OpenAI spec
                    message_content["content"] = None

            normalized_response = {
                "id": f"gemini-{time.time_ns()}", "object": "chat.completion", "created": int(time.time()),
                "model": current_model,
                "choices": [{"index": 0, "message": message_content, "finish_reason": finish_reason}],
                "usage": {
                    "prompt_tokens": response_data.get("usageMetadata", {}).get("promptTokenCount"),
                    "completion_tokens": response_data.get("usageMetadata", {}).get("candidatesTokenCount"),
                    "total_tokens": response_data.get("usageMetadata", {}).get("totalTokenCount")}
            }
            return normalized_response
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="google", message=f"Unexpected error: {e}")



# https://console.groq.com/docs/quickstart
def chat_with_groq(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        maxp: Optional[float] = None,  # top_p
        streaming: Optional[bool] = False,
        max_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        n: Optional[int] = None,
        user: Optional[str] = None,  # user_identifier
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    loaded_config_data = load_and_log_configs()
    groq_config = loaded_config_data.get('groq_api', {})
    final_api_key = api_key or groq_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="groq", message="Groq API Key required.")

    # ... (logging key, model, temp, streaming setup as before) ...
    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Key Provided"
    logging.debug(f"Groq: Using API Key: {log_key}")

    current_model = model or groq_config.get('model', 'llama3-8b-8192')
    current_temp = temp if temp is not None else float(groq_config.get('temperature', 0.2))
    current_top_p = maxp  # Groq uses top_p
    current_streaming_cfg = groq_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(groq_config.get('max_tokens'), int)

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {'Authorization': f'Bearer {final_api_key}', 'Content-Type': 'application/json'}
    data = {
        "model": current_model, "messages": api_messages, "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if seed is not None: data["seed"] = seed
    if stop is not None: data["stop"] = stop
    if response_format is not None: data["response_format"] = response_format
    if n is not None: data["n"] = n
    if user is not None: data["user"] = user
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice
    if logit_bias is not None: data["logit_bias"] = logit_bias
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if logprobs is not None: data["logprobs"] = logprobs
    if top_logprobs is not None and data.get("logprobs") is True: data["top_logprobs"] = top_logprobs

    api_url = groq_config.get('api_base_url', 'https://api.groq.com/openai/v1').rstrip('/') + '/chat/completions'
    logging.debug(f"Groq Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")
    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, ensure "Groq" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:  # ... error handling ...
                        logging.error(f"Groq: ChunkedEncodingError: {e}", exc_info=True)
                        yield f"data: {json.dumps({'error': {'message': f'Stream error: {str(e)}', 'type': 'groq_stream_error'}})}\n\n"
                    except Exception as e:  # ... error handling ...
                        logging.error(f"Groq: Stream iteration error: {e}", exc_info=True)
                        yield f"data: {json.dumps({'error': {'message': f'Stream iteration error: {str(e)}', 'type': 'groq_stream_error'}})}\n\n"
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming logic, retry) ...
            retry_count = int(groq_config.get('api_retries', 3))  # ... retry setup ...
            adapter = HTTPAdapter(
                max_retries=Retry(total=retry_count, backoff_factor=float(groq_config.get('api_retry_delay', 1)),
                                  status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="groq", message=f"Unexpected error: {e}")


def chat_with_huggingface(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,  # This will be the 'model' in the payload
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,  # Mapped from generic system_message
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        # OpenAI-compatible parameters
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        max_tokens: Optional[int] = None,  # Generic 'max_tokens'
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        num_return_sequences: Optional[int] = None,  # Mapped from 'n'
        user: Optional[str] = None,  # Mapped from user_identifier
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        custom_prompt_arg: Optional[str] = None  # Legacy
):
    logging.debug(f"HuggingFace Chat: Request process starting...")
    loaded_config_data = load_and_log_configs()
    hf_config = loaded_config_data.get('huggingface_api', {})

    final_api_key = api_key or hf_config.get('api_key')
    if final_api_key:
        log_key_display = f"{final_api_key[:5]}...{final_api_key[-5:]}" if len(final_api_key) > 9 else "Key Provided"
        logging.debug(f"HuggingFace: Using API Key: {log_key_display}")
    else:
        logging.warning("HuggingFace: API key is missing. Assuming local/unsecured TGI or endpoint allows it.")

    # Model for the payload. The 'model' arg to this function is primary.
    final_model_for_payload = model or hf_config.get('model')  # 'model' from config, not 'model_id'
    if not final_model_for_payload:
        raise ChatConfigurationError(provider="huggingface", message="HuggingFace model for payload is required.")

    # --- URL Construction ---
    # (URL construction logic from your previous version, ensure it uses final_model_for_payload if needed for path)
    api_url: str
    use_router_format = hf_config.get('use_router_url_format', False)
    if use_router_format:
        router_base = hf_config.get('router_base_url', 'https://router.huggingface.co/hf-inference').rstrip('/')
        model_path_part = final_model_for_payload  # Using the model that will be in the payload for path consistency
        chat_path = hf_config.get('api_chat_path', 'v1/chat/completions').strip('/')
        api_url = f"{router_base}/models/{model_path_part}/{chat_path}"
        logging.info(f"HuggingFace: Using Router URL format. Target URL: {api_url}")
    else:
        configured_api_base_url = hf_config.get('api_base_url')
        if not configured_api_base_url:
            raise ChatConfigurationError(provider="huggingface", message="HuggingFace 'api_base_url' not configured.")
        chat_completions_path = hf_config.get('api_chat_path', 'v1/chat/completions').strip('/')
        api_url = f"{configured_api_base_url.rstrip('/')}/{chat_completions_path}"
        logging.info(f"HuggingFace: Using TGI/Endpoint URL format. Target URL: {api_url}")

    # Resolve other parameters
    final_temp = temp if temp is not None else _safe_cast(hf_config.get('temperature'), float, 0.7)
    final_streaming_cfg = hf_config.get('streaming', False)
    final_streaming = streaming if streaming is not None else \
        (str(final_streaming_cfg).lower() == 'true' if isinstance(final_streaming_cfg, str) else bool(
            final_streaming_cfg))

    # For max_tokens, OpenAI standard is 'max_tokens'. TGI often uses 'max_new_tokens'.
    # If the endpoint is OpenAI compatible (e.g. /v1/chat/completions), it likely expects 'max_tokens'.
    # We prioritize the direct 'max_tokens' from chat_api_call.
    final_max_tokens_val = max_tokens if max_tokens is not None \
        else _safe_cast(hf_config.get('max_tokens', hf_config.get('max_new_tokens')), int)

    api_messages = []
    if system_message:  # This is the parameter passed from chat_api_call
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)  # input_data is the messages_payload

    payload = {
        "model": final_model_for_payload,
        "messages": api_messages,  # System message is now part of this
        "stream": final_streaming,
    }

    if final_temp is not None: payload["temperature"] = final_temp
    if top_p is not None: payload["top_p"] = top_p
    if top_k is not None: payload["top_k"] = top_k
    if final_max_tokens_val is not None: payload["max_tokens"] = final_max_tokens_val
    if seed is not None: payload["seed"] = seed
    if stop is not None: payload["stop"] = stop
    if response_format is not None: payload["response_format"] = response_format
    if num_return_sequences is not None: payload["num_return_sequences"] = num_return_sequences
    if user is not None: payload["user"] = user
    if tools is not None: payload["tools"] = tools
    if tool_choice is not None:  # If it's still present after chat.py logic
        payload["tool_choice"] = tool_choice
    # Then conditionally add tool_choice:
    elif tool_choice == "none":  # Allow "none" even if no tools are present
        payload["tool_choice"] = "none"
    if logit_bias is not None: payload["logit_bias"] = logit_bias
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if logprobs is not None: payload["logprobs"] = logprobs
    if top_logprobs is not None and payload.get("logprobs"): payload["top_logprobs"] = top_logprobs

    logging.debug(f"HuggingFace Payload (excluding messages): {{k:v for k,v in payload.items() if k != 'messages'}}")

    try:
        if final_streaming:
            # ... (OpenAI-like streaming logic, use "HuggingFace" in logs) ...
            logging.debug(f"HuggingFace: Posting streaming request to {api_url}")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=payload, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming logic, retry) ...
            logging.debug(f"HuggingFace: Posting non-streaming request to {api_url}")
            # ... (retry setup from your file) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(hf_config.get('api_retries', 3)),
                                                    backoff_factor=float(hf_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter);
                session.mount("http://", adapter)
                response = session.post(api_url, headers=headers, json=payload,
                                        timeout=float(hf_config.get('api_timeout', 120.0)))
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="huggingface", message=f"Unexpected error: {e}")


def chat_with_mistral(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        topp: Optional[float] = None,  # top_p for Mistral
        # Mistral specific and new params
        max_tokens: Optional[int] = None,
        random_seed: Optional[int] = None,  # from generic 'seed'
        top_k: Optional[int] = None,  # from generic 'topk'
        safe_prompt: Optional[bool] = None,  # Mistral specific, already in your config
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[str] = None,  # Mistral: "auto", "any", "none"
        response_format: Optional[Dict[str, str]] = None,  # Mistral: {"type": "json_object"}
        custom_prompt_arg: Optional[str] = None
):
    loaded_config_data = load_and_log_configs()
    mistral_config = loaded_config_data.get('mistral_api', {})
    final_api_key = api_key or mistral_config.get('api_key')
    if not final_api_key:
        raise ChatConfigurationError(provider="mistral", message="Mistral API Key required.")

    # ... (logging key, model, temp, streaming, top_p setup) ...
    log_key = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Key Provided"
    logging.debug(f"Mistral: Using API Key: {log_key}")
    current_model = model or mistral_config.get('model', 'mistral-large-latest')  # or mistral-small, mistral-medium
    current_temp = temp if temp is not None else float(
        mistral_config.get('temperature', 0.1))  # Mistral defaults to 0.7
    current_top_p = topp  # Mistral uses top_p
    current_streaming_cfg = mistral_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    current_max_tokens = max_tokens if max_tokens is not None else _safe_cast(mistral_config.get('max_tokens'), int)
    current_safe_prompt = safe_prompt if safe_prompt is not None else bool(mistral_config.get('safe_prompt', False))

    api_messages = []
    # Mistral expects system message as the first message with role: system if provided
    # However, their latest guidance often shows it as part of the first user message or specific instructions.
    # For OpenAI compatibility, if system_message is given, and not already in input_data, prepend it.
    has_system_in_input = any(msg.get("role") == "system" for msg in input_data)
    if system_message and not has_system_in_input:
        api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {'Authorization': f'Bearer {final_api_key}', 'Content-Type': 'application/json',
               'Accept': 'application/json'}
    data = {"model": current_model, "messages": api_messages, "stream": current_streaming}

    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens
    if random_seed is not None: data["random_seed"] = random_seed  # Mistral uses random_seed
    if top_k is not None: data["top_k"] = top_k  # Mistral has top_k
    if current_safe_prompt is not None: data["safe_prompt"] = current_safe_prompt  # Mistral specific
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice  # "auto", "any", "none"
    if response_format is not None: data["response_format"] = response_format  # {"type": "json_object"}

    api_url = mistral_config.get('api_base_url', 'https://api.mistral.ai/v1').rstrip('/') + '/chat/completions'
    logging.debug(f"Mistral Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, use "Mistral" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming, retry) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(mistral_config.get('api_retries', 3)),
                                                    backoff_factor=float(mistral_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="mistral", message=f"Unexpected error: {e}")


def chat_with_openrouter(
        input_data: List[Dict[str, Any]],
        model: Optional[str] = None,
        api_key: Optional[str] = None,
        system_message: Optional[str] = None,
        temp: Optional[float] = None,
        streaming: Optional[bool] = False,
        # OpenRouter specific names from your map
        top_p: Optional[float] = None,  # from generic topp
        top_k: Optional[int] = None,  # from generic topk
        min_p: Optional[float] = None,  # from generic minp (OpenRouter uses min_p not minp)
        max_tokens: Optional[int] = None,
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        response_format: Optional[Dict[str, str]] = None,
        n: Optional[int] = None,
        user: Optional[str] = None,  # from user_identifier
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        presence_penalty: Optional[float] = None,
        frequency_penalty: Optional[float] = None,
        logprobs: Optional[bool] = None,
        top_logprobs: Optional[int] = None,
        custom_prompt_arg: Optional[str] = None
):
    loaded_config_data = load_and_log_configs()
    openrouter_config = loaded_config_data.get('openrouter_api', {})
    # ... (api key, model, temp, streaming setup) ...
    final_api_key = api_key or openrouter_config.get('api_key')
    if not final_api_key: raise ChatConfigurationError(provider='openrouter', message="OpenRouter API Key required.")
    current_model = model or openrouter_config.get('model', 'mistralai/mistral-7b-instruct:free')
    # ... other param resolutions ...
    current_streaming_cfg = openrouter_config.get('streaming', False)
    current_streaming = streaming if streaming is not None else \
        (str(current_streaming_cfg).lower() == 'true' if isinstance(current_streaming_cfg, str) else bool(
            current_streaming_cfg))

    api_messages = []
    if system_message: api_messages.append({"role": "system", "content": system_message})
    api_messages.extend(input_data)

    headers = {
        "Authorization": f"Bearer {final_api_key}", "Content-Type": "application/json",
        "HTTP-Referer": openrouter_config.get("site_url", "http://localhost"),  # OpenRouter specific
        "X-Title": openrouter_config.get("site_name", "TLDW-API"),  # OpenRouter specific
    }
    data = {"model": current_model, "messages": api_messages, "stream": current_streaming}
    # Add all other accepted parameters to data if they are not None
    if temp is not None: data["temperature"] = temp
    if top_p is not None: data["top_p"] = top_p
    if top_k is not None: data["top_k"] = top_k
    if min_p is not None: data["min_p"] = min_p  # OpenRouter uses min_p
    if max_tokens is not None: data["max_tokens"] = max_tokens
    if seed is not None: data["seed"] = seed
    if stop is not None: data["stop"] = stop
    if response_format is not None: data["response_format"] = response_format
    if n is not None: data["n"] = n
    if user is not None: data["user"] = user
    if tools is not None: data["tools"] = tools
    if tool_choice is not None: data["tool_choice"] = tool_choice
    if logit_bias is not None: data["logit_bias"] = logit_bias
    if presence_penalty is not None: data["presence_penalty"] = presence_penalty
    if frequency_penalty is not None: data["frequency_penalty"] = frequency_penalty
    if logprobs is not None: data["logprobs"] = logprobs
    if top_logprobs is not None and data.get("logprobs"): data["top_logprobs"] = top_logprobs

    api_url = openrouter_config.get('api_base_url', "https://openrouter.ai/api/v1").rstrip('/') + "/chat/completions"
    logging.debug(
        f"OpenRouter Request Payload (excluding messages): {{k: v for k, v in data.items() if k != 'messages'}}")

    try:
        if current_streaming:
            # ... (OpenAI-like streaming logic, ensure "OpenRouter" in logs) ...
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): yield line + "\n\n"
                    # ... (error handling for stream) ...
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()

                return stream_generator()
        else:
            # ... (non-streaming logic) ...
            # ... (retry setup) ...
            adapter = HTTPAdapter(max_retries=Retry(total=int(openrouter_config.get('api_retries', 3)),
                                                    backoff_factor=float(openrouter_config.get('api_retry_delay', 1)),
                                                    status_forcelist=[429, 500, 502, 503, 504],
                                                    allowed_methods=["POST"]))
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            return response.json()  # OpenRouter usually returns OpenAI compatible JSON
    except requests.exceptions.HTTPError as e:  # ... error handling ...
        raise
    except Exception as e:  # ... error handling ...
        raise ChatProviderError(provider="openrouter", message=f"Unexpected error: {e}")

#
#
#######################################################################################################################