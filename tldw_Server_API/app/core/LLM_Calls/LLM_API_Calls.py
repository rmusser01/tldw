# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import os
import time
from typing import List, Any, Optional, Tuple, Dict, Union
#
# Import 3rd-Party Libraries
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
#
# Import Local libraries
from tldw_Server_API.app.core.config import load_and_log_configs
from tldw_Server_API.app.core.Utils.Utils import logging
from tldw_Server_API.app.core.Chat.Chat_Functions import ChatAuthenticationError, ChatRateLimitError, \
    ChatBadRequestError, ChatProviderError, ChatConfigurationError
#
#######################################################################################################################
# Function Definitions
#

# FIXME: Update to include full arguments

# --- Helper function for safe type conversion ---
def _safe_cast(value: Any, cast_to: type, default: Any = None) -> Any:
    """Safely casts value to specified type, returning default on failure."""
    if value is None:
        return default
    try:
        return cast_to(value)
    except (ValueError, TypeError):
        logging.warning(f"Could not cast '{value}' to {cast_to}. Using default: {default}")
        return default

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def _parse_data_url_for_multimodal(data_url: str) -> Optional[Tuple[str, str]]:
    """Parses a data URL (e.g., data:image/png;base64,xxxx) into (mime_type, base64_data)."""
    if data_url.startswith("data:") and ";base64," in data_url:
        try:
            header, b64_data = data_url.split(";base64,", 1)
            mime_type = header.split("data:", 1)[1]
            return mime_type, b64_data
        except Exception as e:
            logging.warning(f"Could not parse data URL: {data_url[:60]}... Error: {e}")
            return None
    logging.debug(f"Data URL did not match expected format: {data_url[:60]}...")
    return None


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.
    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.
    Returns:
        List[float]: The embeddings generated by the API.
    """
    loaded_config_data = load_and_log_configs()
    api_key = loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI Embeddings: API key not found or is empty")
        raise ValueError("OpenAI Embeddings: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI Embeddings: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI Embeddings: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI Embeddings: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    request_data = {
        "input": input_data,
        "model": model,
    }
    try:
        logging.debug("OpenAI Embeddings: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI Embeddings: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI Embeddings: Embedding data not found in the response")
                raise ValueError("OpenAI Embeddings: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI Embeddings: request failed with status code {response.status_code}")
            logging.error(f"OpenAI Embeddings: Error response: {response.text}")
            # Propagate HTTPError to be caught by chat_api_call's handler (if this were called from there)
            # Or raise specific error if called directly
            response.raise_for_status() # This will raise HTTPError
            # Fallback if raise_for_status doesn't cover it (it should)
            raise ValueError(f"OpenAI Embeddings: Failed to retrieve. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI Embeddings: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI Embeddings: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Unexpected error occurred: {str(e)}")


def chat_with_openai(
        input_data: List[Dict[str, Any]],  # Mapped from 'messages_payload'
        model: Optional[str] = None,  # Mapped from 'model'
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        temp: Optional[float] = None,  # Mapped from 'temp' (temperature)
        maxp: Optional[float] = None,  # Mapped from 'maxp' (top_p)
        streaming: Optional[bool] = False,  # Mapped from 'streaming'
        # New OpenAI specific parameters (and some from original ChatCompletionRequest schema)
        frequency_penalty: Optional[float] = None,
        logit_bias: Optional[Dict[str, float]] = None,
        logprobs: Optional[bool] = None,  # True/False
        top_logprobs: Optional[int] = None,
        max_tokens: Optional[int] = None,  # This was already implicitly handled by config, now explicit
        n: Optional[int] = None,  # Number of completions
        presence_penalty: Optional[float] = None,
        response_format: Optional[Dict[str, str]] = None,  # e.g., {"type": "json_object"}
        seed: Optional[int] = None,
        stop: Optional[Union[str, List[str]]] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
        tool_choice: Optional[Union[str, Dict[str, Any]]] = None,
        user: Optional[str] = None,
        # custom_prompt_arg is largely deprecated if system_message and input_data are used correctly
        custom_prompt_arg: Optional[str] = None,
        **kwargs  # To catch any other unexpected kwargs if PROVIDER_PARAM_MAP is too broad
):
    """
    Sends a chat completion request to the OpenAI API.

    Args:
        input_data: List of message objects (OpenAI format).
        model: ID of the model to use.
        api_key: OpenAI API key.
        system_message: Optional system message to prepend.
        temp: Sampling temperature.
        maxp: Top-p (nucleus) sampling parameter.
        streaming: Whether to stream the response.
        frequency_penalty: Penalizes new tokens based on their existing frequency.
        logit_bias: Modifies the likelihood of specified tokens appearing.
        logprobs: Whether to return log probabilities of output tokens.
        top_logprobs: An integer between 0 and 5 specifying the number of most likely tokens to return at each token position.
        max_tokens: Maximum number of tokens to generate.
        n: How many chat completion choices to generate for each input message.
        presence_penalty: Penalizes new tokens based on whether they appear in the text so far.
        response_format: An object specifying the format that the model must output. e.g. {"type": "json_object"}.
        seed: This feature is in Beta. If specified, the system will make a best effort to sample deterministically.
        stop: Up to 4 sequences where the API will stop generating further tokens.
        tools: A list of tools the model may call.
        tool_choice: Controls which (if any) function is called by the model.
        user: A unique identifier representing your end-user, which can help OpenAI to monitor and detect abuse.
        custom_prompt_arg: Legacy, largely ignored.
        **kwargs: Catches any unexpected keyword arguments.
    """
    loaded_config_data = load_and_log_configs()
    openai_config = loaded_config_data.get('openai_api', {})

    final_api_key = api_key or openai_config.get('api_key')
    if not final_api_key:
        logging.error("OpenAI: API key is missing.")
        raise ChatConfigurationError(provider="openai", message="OpenAI API Key is required but not found.")

    log_key_display = f"{final_api_key[:5]}...{final_api_key[-5:]}" if final_api_key and len(
        final_api_key) > 9 else "Provided Key"
    logging.debug(f"OpenAI: Using API Key: {log_key_display}")

    # Resolve parameters: User-provided > Function arg default > Config default > Hardcoded default
    final_model = model if model is not None else openai_config.get('model', 'gpt-4o-mini')
    final_temp = temp if temp is not None else float(openai_config.get('temperature', 0.7))
    final_top_p = maxp if maxp is not None else float(
        openai_config.get('top_p', 0.95))  # 'maxp' from chat_api_call maps to 'top_p'

    # Streaming needs careful boolean conversion
    if streaming is not None:
        final_streaming = streaming
    else:
        config_streaming = openai_config.get('streaming', False)
        if isinstance(config_streaming, str):
            final_streaming = config_streaming.lower() == "true"
        else:
            final_streaming = bool(config_streaming)

    # Other parameters with defaults from config or None
    final_max_tokens = max_tokens if max_tokens is not None else int(
        openai_config.get('max_tokens', 4096))  # OpenAI specific

    if custom_prompt_arg:
        logging.warning(
            "OpenAI: 'custom_prompt_arg' was provided but is generally ignored if 'input_data' and 'system_message' are used correctly.")

    # Construct messages for OpenAI API
    api_messages = []
    has_system_message_in_input = any(msg.get("role") == "system" for msg in input_data)

    if system_message:
        if not has_system_message_in_input:
            api_messages.append({"role": "system", "content": system_message})
            api_messages.extend(input_data)
        else:
            # If input_data already has a system message, prioritize it or decide on a merge strategy.
            # For now, let's assume input_data's system message takes precedence if both are provided.
            # Or, log a warning and use the one from input_data.
            logging.warning(
                "OpenAI: Both 'system_message' argument and a system message in 'input_data' were provided. Using the one from 'input_data'.")
            api_messages.extend(input_data)
    else:  # No explicit system_message argument
        api_messages.extend(input_data)

    # --- Prepare Request Payload ---
    payload = {
        "model": final_model,
        "messages": api_messages,
        "temperature": final_temp,
        "top_p": final_top_p,
        "stream": final_streaming,
    }

    # Add optional parameters if they are not None
    if final_max_tokens is not None: payload["max_tokens"] = final_max_tokens
    if frequency_penalty is not None: payload["frequency_penalty"] = frequency_penalty
    if logit_bias is not None: payload["logit_bias"] = logit_bias
    if logprobs is not None: payload["logprobs"] = logprobs  # Pass directly if not None
    if top_logprobs is not None:  # Requires logprobs=True
        if payload.get("logprobs") is True:
            payload["top_logprobs"] = top_logprobs
        else:
            logging.warning(
                "OpenAI: 'top_logprobs' was provided, but 'logprobs' is not true. 'top_logprobs' will be ignored by the API.")
    if n is not None: payload["n"] = n
    if presence_penalty is not None: payload["presence_penalty"] = presence_penalty
    if response_format is not None: payload["response_format"] = response_format
    if seed is not None: payload["seed"] = seed
    if stop is not None: payload["stop"] = stop
    if tools is not None: payload["tools"] = tools
    if tool_choice is not None:
        # If tool_choice is set (e.g. "auto" or a function name) but no tools are defined,
        # OpenAI API might error. It's safer to only send tool_choice if tools are also present,
        # or if tool_choice is "none".
        if tools or tool_choice == "none":
            payload["tool_choice"] = tool_choice
        else:
            logging.debug(
                f"OpenAI: 'tool_choice' is '{tool_choice}' but no 'tools' provided. Not sending 'tool_choice'.")
    if user is not None: payload["user"] = user

    headers = {
        'Authorization': f'Bearer {final_api_key}',
        'Content-Type': 'application/json'
    }

    logging.debug(
        f"OpenAI Request Payload (excluding messages): { {k: v for k, v in payload.items() if k != 'messages'} }")
    if len(api_messages) > 0:
        logging.debug(
            f"OpenAI Request Messages (first and last): First: {api_messages[0]}, Last: {api_messages[-1] if len(api_messages) > 1 else api_messages[0]}")

    # --- Execute Request ---
    api_url = 'https://api.openai.com/v1/chat/completions'
    try:
        if final_streaming:
            logging.debug("OpenAI: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=payload, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"  # Adhere to SSE format for client
                        # OpenAI stream itself does not send a final [DONE] in this way,
                        # but our endpoint wrapper expects it.
                        # The actual DONE event should be data: [DONE]\n\n
                    except requests.exceptions.ChunkedEncodingError as e_chunk:
                        logging.error(f"OpenAI: ChunkedEncodingError during stream: {e_chunk}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream connection error: {str(e_chunk)}",
                                                              "type": "openai_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    except Exception as e_stream:
                        logging.error(f"OpenAI: Error during stream iteration: {e_stream}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream iteration error: {str(e_stream)}",
                                                              "type": "openai_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    finally:
                        # Ensure DONE is sent for the endpoint wrapper's logic
                        yield "data: [DONE]\n\n"
                        if response:
                            response.close()

                return stream_generator()

        else:  # Non-streaming
            logging.debug("OpenAI: Posting request (non-streaming)")
            retry_count = int(openai_config.get('api_retries', 3))
            retry_delay = float(openai_config.get('api_retry_delay', 1.0))  # Ensure float

            retry_strategy = Retry(
                total=retry_count,
                backoff_factor=retry_delay,
                status_forcelist=[429, 500, 502, 503, 504],
                allowed_methods=["POST"]  # Changed from method_whitelist
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                session.mount("http://", adapter)  # Though OpenAI is https
                response = session.post(api_url, headers=headers, json=payload,
                                        timeout=float(openai_config.get('api_timeout', 90.0)))

            logging.debug(f"OpenAI: Full API response status: {response.status_code}")
            response.raise_for_status()  # Raise HTTPError for 4xx/5xx AFTER retries
            response_data = response.json()
            logging.debug("OpenAI: Non-streaming request successful.")
            return response_data

    except requests.exceptions.HTTPError as e:
        error_content_text = "No response text"
        error_content_json = None
        if e.response is not None:
            error_content_text = e.response.text
            try:
                error_content_json = e.response.json()
            except json.JSONDecodeError:
                pass
        logging.error(
            f"OpenAI HTTPError {e.response.status_code if e.response is not None else 'Unknown'}. Text: {error_content_text}. JSON: {error_content_json}",
            exc_info=True)
        raise
    except requests.exceptions.RequestException as e:
        logging.error(f"OpenAI RequestException: {e}", exc_info=True)
        raise  # Re-raise for chat_api_call
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"OpenAI: Configuration or data error: {e}", exc_info=True)
        raise ValueError(f"OpenAI usage error: {e}") from e  # Re-raise as ValueError for chat_api_call


def chat_with_anthropic(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_prompt: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None,  # Mapped from 'topp' (becomes top_p)
        topk: Optional[int] = None  # Mapped from 'topk'
):
    # Assuming load_and_log_configs is defined elsewhere
    loaded_config_data = load_and_log_configs()
    anthropic_config = loaded_config_data.get('anthropic_api', {})

    # --- API Key Resolution ---
    anthropic_api_key = api_key or anthropic_config.get('api_key')

    if not anthropic_api_key:
        logging.error("Anthropic: API key is missing.")
        raise ChatConfigurationError(provider="anthropic", message="Anthropic API Key is required but not found.")

    log_key = f"{anthropic_api_key[:5]}...{anthropic_api_key[-5:]}" if anthropic_api_key and len(
        anthropic_api_key) > 9 else "Provided Key"
    logging.debug(f"Anthropic: Using API Key: {log_key}")

    current_model = model or anthropic_config.get('model', 'claude-3-haiku-20240307')  # Adjusted default
    current_temp = temp if temp is not None else float(anthropic_config.get('temperature', 0.7))
    current_top_p = topp
    current_top_k = topk
    current_streaming = streaming if streaming is not None else anthropic_config.get('streaming', False)
    current_system_prompt = system_prompt
    max_tokens_config = anthropic_config.get('max_tokens_to_sample', anthropic_config.get('max_tokens', 4096))
    max_tokens = int(max_tokens_config)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="anthropic",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Anthropic: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, TopK: {current_top_k}, MaxTokens: {max_tokens}")
    if custom_prompt_arg:
        logging.warning(
            "Anthropic: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    anthropic_messages = []
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")

        if role not in ["user", "assistant"]:
            logging.warning(f"Anthropic: Skipping message with unsupported role: {role}")
            continue

        anthropic_content_parts = []
        if isinstance(content, str):
            anthropic_content_parts.append({"type": "text", "text": content})
        elif isinstance(content, list):
            for part in content:
                part_type = part.get("type")
                if part_type == "text":
                    anthropic_content_parts.append({"type": "text", "text": part.get("text", "")})
                elif part_type == "image_url":
                    image_url_data = part.get("image_url", {}).get("url", "")
                    parsed_image = _parse_data_url_for_multimodal(image_url_data)  # Ensure this helper is defined
                    if parsed_image:
                        mime_type, b64_data = parsed_image
                        anthropic_content_parts.append({
                            "type": "image",
                            "source": {"type": "base64", "media_type": mime_type, "data": b64_data}
                        })
                    else:
                        logging.warning(
                            f"Anthropic: Could not parse image_url, skipping image: {str(image_url_data)[:60]}...")

        if anthropic_content_parts:
            anthropic_messages.append({"role": role, "content": anthropic_content_parts})
        elif role == "user" and not anthropic_content_parts:
            logging.warning("Anthropic: User message has no processable content parts. Adding placeholder.")
            anthropic_messages.append({"role": role, "content": [
                {"type": "text", "text": "(User input was empty or only contained unparseable images)"}]})

    if not any(m['role'] == 'user' for m in anthropic_messages):
        raise ChatBadRequestError(provider="anthropic",
                                  message="No valid user messages found after processing for Anthropic.")

    headers = {
        'x-api-key': anthropic_api_key,
        'anthropic-version': '2023-06-01',  # Or newer like "2024-05-16" for newer features if needed
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "max_tokens": max_tokens,
        "messages": anthropic_messages,
        "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if current_system_prompt: data["system"] = current_system_prompt

    api_url = 'https://api.anthropic.com/v1/messages'
    try:
        retry_count = int(anthropic_config.get('api_retries', 3))
        retry_delay = float(anthropic_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                               status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=current_streaming,
                                    timeout=180)  # Increased timeout
            response.raise_for_status()

        if current_streaming:
            logging.debug("Anthropic: Streaming response received. Normalizing to OpenAI SSE.")

            def stream_generator():
                completion_id = f"chatcmpl-anthropic-{time.time_ns()}"
                created_time = int(time.time())

                event_name = None
                data_buffer = []

                try:
                    for line_bytes in response.iter_lines():  # iter_lines gives bytes
                        line = line_bytes.decode('utf-8').strip()

                        if not line:  # Empty line signifies end of an event
                            if event_name and data_buffer:
                                data_str = "".join(data_buffer)
                                try:
                                    anthropic_event = json.loads(data_str)

                                    # Process event based on event_name
                                    delta_content = None
                                    finish_reason = None

                                    if event_name == "content_block_delta":
                                        if anthropic_event.get("type") == "content_block_delta":
                                            delta = anthropic_event.get("delta", {})
                                            if delta.get("type") == "text_delta":
                                                delta_content = delta.get("text")
                                    elif event_name == "message_delta":  # Contains usage and stop_reason
                                        stop_reason = anthropic_event.get("delta", {}).get("stop_reason")
                                        if stop_reason:
                                            finish_reason_map = {"end_turn": "stop", "max_tokens_reached": "length",
                                                                 "stop_sequence": "stop", "tool_use": "tool_calls"}
                                            finish_reason = finish_reason_map.get(stop_reason, stop_reason)
                                    # Other events like message_start, content_block_start, content_block_stop, message_stop
                                    # can be used for more granular control or logging if needed.

                                    if delta_content:
                                        sse_chunk = {
                                            "id": completion_id, "object": "chat.completion.chunk",
                                            "created": created_time, "model": current_model,
                                            "choices": [{"index": 0, "delta": {"content": delta_content},
                                                         "finish_reason": None}]
                                        }
                                        yield f"data: {json.dumps(sse_chunk)}\n\n"

                                    if finish_reason:
                                        sse_chunk = {
                                            "id": completion_id, "object": "chat.completion.chunk",
                                            "created": created_time, "model": current_model,
                                            "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}]
                                        }
                                        yield f"data: {json.dumps(sse_chunk)}\n\n"

                                except json.JSONDecodeError:
                                    logging.warning(
                                        f"Anthropic Stream: Could not decode JSON for event '{event_name}': {data_str}")

                            event_name = None
                            data_buffer = []
                            continue

                        if line.startswith("event:"):
                            event_name = line[len("event:"):].strip()
                        elif line.startswith("data:"):
                            data_buffer.append(line[len("data:"):].strip())

                    # After loop, process any remaining buffered event (if stream ends abruptly)
                    if event_name and data_buffer:
                        data_str = "".join(data_buffer)
                        try:
                            anthropic_event = json.loads(data_str)
                            if event_name == "message_stop":  # Typically the actual end
                                pass  # No new content, but could log
                        except json.JSONDecodeError:
                            logging.warning(
                                f"Anthropic Stream: Could not decode JSON for final event '{event_name}': {data_str}")

                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:
                    logging.error(f"Anthropic: ChunkedEncodingError during stream: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logging.error(f"Anthropic: Error during stream iteration: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                finally:
                    if response:
                        response.close()

            return stream_generator()
        else:
            # ... (non-streaming logic remains the same) ...
            logging.debug("Anthropic: Non-streaming request successful.")
            response_data = response.json()
            logging.debug("Anthropic: Non-streaming request successful. Normalizing response.")
            assistant_content_parts = []
            if response_data.get("content"):
                for part in response_data.get("content", []):
                    if part.get("type") == "text":
                        assistant_content_parts.append(part.get("text", ""))
            full_assistant_content = "\n".join(assistant_content_parts).strip()

            finish_reason_map = {"end_turn": "stop", "max_tokens": "length", "max_tokens_reached": "length",
                                 "stop_sequence": "stop", "tool_use": "tool_calls"}
            openai_finish_reason = finish_reason_map.get(response_data.get("stop_reason"),
                                                         response_data.get("stop_reason"))

            normalized_response = {
                "id": response_data.get("id", f"anthropic-{time.time_ns()}"),
                "object": "chat.completion",
                "created": int(time.time()),
                "model": response_data.get("model", current_model),
                "choices": [{"index": 0, "message": {"role": "assistant", "content": full_assistant_content},
                             "finish_reason": openai_finish_reason}],
                "usage": response_data.get("usage")
            }
            return normalized_response

    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code
        error_text = e.response.text
        if status_code == 401:
            raise ChatAuthenticationError(provider="anthropic",
                                          message=f"Authentication failed. Detail: {error_text[:200]}") from e
        elif status_code == 429:
            raise ChatRateLimitError(provider="anthropic",
                                     message=f"Rate limit exceeded. Detail: {error_text[:200]}") from e
        elif 400 <= status_code < 500:
            raise ChatBadRequestError(provider="anthropic",
                                      message=f"Bad request (Status {status_code}). Detail: {error_text[:200]}") from e
        else:  # 5xx or other
            raise ChatProviderError(provider="anthropic",
                                    message=f"API error (Status {status_code}). Detail: {error_text[:200]}",
                                    status_code=status_code) from e
    except requests.exceptions.RequestException as e:  # Catch network errors like ConnectionError, Timeout
        raise ChatProviderError(provider="anthropic", message=f"Network error: {str(e)}",
                                status_code=504) from e  # 504 Gateway Timeout
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Anthropic: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="anthropic", message=f"Anthropic config/data error: {e}") from e


def chat_with_cohere(
        model: Optional[str],
        input_data: List[Dict[str, Any]],
        api_key: Optional[str] = None,
        custom_prompt_arg: Optional[str] = None,
        temp: Optional[float] = None,
        system_prompt: Optional[str] = None,
        streaming: Optional[bool] = None,
        topp: Optional[float] = None,
        topk: Optional[int] = None
):
    loaded_config_data = load_and_log_configs()
    cohere_config = loaded_config_data.get('cohere_api', {})

    cohere_api_key = api_key or cohere_config.get('api_key')
    if not cohere_api_key:
        logging.error("Cohere: API key is missing.")
        raise ChatConfigurationError(provider="cohere", message="Cohere API Key is required but not found.")

    log_key = f"{cohere_api_key[:3]}...{cohere_api_key[-3:]}" if cohere_api_key and len(
        cohere_api_key) > 5 else "Provided Key"
    logging.debug(f"Cohere: Using API Key: {log_key}")

    current_model = model or cohere_config.get('model', 'command-r')
    current_temp = temp if temp is not None else float(cohere_config.get('temperature', 0.3))
    current_p = topp
    current_k = topk
    current_streaming = streaming if streaming is not None else cohere_config.get('streaming', False)
    current_preamble = system_prompt

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="cohere",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Cohere: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, P: {current_p}, K: {current_k}")
    if custom_prompt_arg:
        logging.warning("Cohere: 'custom_prompt_arg' was provided but is generally ignored.")

    chat_history = []
    user_message_text = ""
    processed_messages_count = 0
    for i, msg in enumerate(input_data):
        role = msg.get("role")
        content = msg.get("content")
        current_msg_text_parts = []
        if isinstance(content, str):
            current_msg_text_parts.append(content)
        elif isinstance(content, list):
            has_image = False
            for part in content:
                if part.get("type") == "text":
                    current_msg_text_parts.append(part.get("text", ""))
                elif part.get("type") == "image_url":
                    has_image = True
            if has_image: logging.warning(f"Cohere: Message {i} contained an image; ignored.")
        full_text_for_msg = "\n".join(current_msg_text_parts).strip()

        if not full_text_for_msg and role == "user":
            if i == len(input_data) - 1:
                full_text_for_msg = "(User input empty/unparseable)"
                logging.warning("Cohere: Current user message empty. Sending placeholder.")
            else:
                logging.warning(f"Cohere: Historical message {i} (role: {role}) empty. Skipping.")
                continue
        if i == len(input_data) - 1 and role == "user":
            user_message_text = full_text_for_msg
        elif role == "user":
            chat_history.append({"role": "USER", "message": full_text_for_msg})
        elif role == "assistant":
            chat_history.append({"role": "CHATBOT", "message": full_text_for_msg})
        else:
            logging.warning(f"Cohere: Skipping message with role: {role}")
        processed_messages_count += 1

    if not user_message_text and not chat_history:
        user_message_text = "(No input provided)"
    elif not user_message_text and chat_history:  # If history exists but current message isn't user (e.g. ends with assistant)
        logging.warning(
            "Cohere: No current user message text, but history exists. This might lead to unexpected Cohere behavior or error.")
        # Cohere's API requires a "message" field. If it's empty and history is not, it might work, or might error.
        # Sending a placeholder if critical, or let API handle it. For now, let it be empty.

    headers = {'accept': 'application/json', 'content-type': 'application/json',
               'Authorization': f'Bearer {cohere_api_key}'}
    data = {"model": current_model, "message": user_message_text, "temperature": current_temp}
    if chat_history: data["chat_history"] = chat_history
    if current_preamble: data["preamble"] = current_preamble
    if current_p is not None: data["p"] = current_p
    if current_k is not None: data["k"] = current_k

    # Cohere's /v1/chat uses a query param for streaming, not a body param.
    # The SDK might abstract this, but with raw requests, it's a query param.
    # The provided code uses a query param: api_url + stream_param_suffix
    api_url = 'https://api.cohere.com/v1/chat'
    # For Cohere's newer /v2/chat which might be preferred and uses "stream": True in body:
    # api_url = 'https://api.cohere.com/v2/chat'
    # data["stream"] = current_streaming # If using /v2/chat

    stream_param_suffix = "?stream=true" if current_streaming else ""  # For /v1/chat
    # If using /v2/chat, remove stream_param_suffix and use data["stream"] = current_streaming

    try:
        retry_count = int(cohere_config.get('api_retries', 3))
        retry_delay = float(cohere_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                               status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url + stream_param_suffix, headers=headers, json=data, stream=current_streaming,
                                    timeout=180)
        response.raise_for_status()

        if current_streaming:
            logging.debug("Cohere: Streaming response received. Normalizing to OpenAI SSE.")

            def stream_generator():
                completion_id = f"chatcmpl-cohere-{time.time_ns()}"
                created_time = int(time.time())
                buffer = ""

                # Keep track of whether stream-end has been processed to avoid sending [DONE] prematurely
                # if the stream closes before Cohere sends a "stream-end" event with is_finished: true
                stream_truly_finished_by_cohere = False

                try:
                    for chunk_bytes in response.iter_content(chunk_size=None):
                        if not chunk_bytes: continue
                        buffer += chunk_bytes.decode('utf-8', errors='replace')

                        # Process complete lines from the buffer
                        while '\n' in buffer:
                            line, buffer = buffer.split('\n', 1)
                            if not line.strip(): continue  # Skip empty lines

                            try:
                                cohere_event = json.loads(line)
                                event_type = cohere_event.get("event_type")  # For v2 API /chat
                                # For v1 /chat with stream=true, events are different:
                                # e.g. "text-generation", "stream-end"

                                delta_text = None
                                finish_reason_str = None
                                is_finished_event = cohere_event.get("is_finished", False)  # Common in v1

                                if event_type == "text-generation" or (
                                        not event_type and "text" in cohere_event):  # v1 or general text event
                                    delta_text = cohere_event.get("text")
                                elif event_type == "stream-end":  # v1 or v2
                                    finish_reason_str = cohere_event.get("finish_reason", "UNKNOWN").lower()
                                    is_finished_event = True  # stream-end implies finished
                                    stream_truly_finished_by_cohere = True
                                # Add other Cohere event types here as needed (e.g., citation generation, tool calls)

                                if delta_text:
                                    sse_chunk = {
                                        "id": completion_id, "object": "chat.completion.chunk",
                                        "created": created_time, "model": current_model,
                                        "choices": [
                                            {"index": 0, "delta": {"content": delta_text}, "finish_reason": None}]
                                    }
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"

                                if finish_reason_str:
                                    sse_chunk = {
                                        "id": completion_id, "object": "chat.completion.chunk",
                                        "created": created_time, "model": current_model,
                                        "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason_str}]
                                    }
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"

                                if is_finished_event:  # Break from inner while if Cohere explicitly says it's done
                                    stream_truly_finished_by_cohere = True
                                    break
                            except json.JSONDecodeError:
                                logging.warning(f"Cohere Stream: Could not decode JSON from line: '{line}'")

                        if stream_truly_finished_by_cohere:  # Break from outer for loop if stream ended
                            break

                    # Process any remaining data in the buffer (e.g. if stream cut off without newline)
                    if buffer.strip() and not stream_truly_finished_by_cohere:
                        try:
                            cohere_event = json.loads(buffer.strip())
                            # Repeat parsing logic for the last bit if necessary
                            if cohere_event.get("event_type") == "text-generation" or (
                                    not cohere_event.get("event_type") and "text" in cohere_event):
                                delta_text = cohere_event.get("text")
                                if delta_text:
                                    sse_chunk = {
                                        "id": completion_id, "object": "chat.completion.chunk",
                                        "created": created_time, "model": current_model,
                                        "choices": [
                                            {"index": 0, "delta": {"content": delta_text}, "finish_reason": None}]
                                    }
                                    yield f"data: {json.dumps(sse_chunk)}\n\n"
                            # No finish reason expected here unless it's a stream-end event
                        except json.JSONDecodeError:
                            logging.warning(
                                f"Cohere Stream: Could not decode JSON from final buffer: '{buffer.strip()}'")

                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:
                    logging.error(f"Cohere: ChunkedEncodingError during stream: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n";
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logging.error(f"Cohere: Error during stream iteration: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n";
                    yield "data: [DONE]\n\n"
                finally:
                    if response:
                        response.close()

            return stream_generator()
        else:
            logging.debug("Cohere: Non-streaming request successful.")
            response_data = response.json()

            # Normalize Cohere's non-streaming response to OpenAI's format
            finish_reason_map = {
                "COMPLETE": "stop", "MAX_TOKENS": "length", "ERROR_LIMIT": "length",
                "ERROR_TOXIC": "content_filter", "ERROR": "error", "USER_CANCEL": "stop",
                "TOOL_ERROR": "tool_calls"  # Or map more specifically if needed
            }
            cohere_finish_reason = response_data.get("finish_reason", "UNKNOWN")
            openai_finish_reason = finish_reason_map.get(cohere_finish_reason, cohere_finish_reason.lower())

            # Token details might be in 'meta' or 'tokens' depending on API version/setup
            usage_info = response_data.get("meta", {}).get("billed_units", {})
            if not usage_info:  # Fallback for newer API versions potentially
                usage_info = response_data.get("tokens", {})

            prompt_tokens = usage_info.get("input_tokens")
            completion_tokens = usage_info.get("output_tokens")
            total_tokens = None
            if prompt_tokens is not None and completion_tokens is not None:
                total_tokens = prompt_tokens + completion_tokens

            normalized_response = {
                "id": response_data.get("generation_id", f"cohere-{time.time_ns()}"),
                "object": "chat.completion",
                "created": int(time.time()),
                "model": current_model,
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": response_data.get("text", "").strip()},
                    "finish_reason": openai_finish_reason
                }],
                "usage": {
                    "prompt_tokens": prompt_tokens,
                    "completion_tokens": completion_tokens,
                    "total_tokens": total_tokens
                }
            }
            return normalized_response
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code
        error_text = e.response.text
        if status_code == 401:
            raise ChatAuthenticationError(provider="cohere",
                                          message=f"Authentication failed. Detail: {error_text[:200]}") from e
        elif status_code == 429:
            raise ChatRateLimitError(provider="cohere",
                                     message=f"Rate limit exceeded. Detail: {error_text[:200]}") from e
        elif 400 <= status_code < 500:
            raise ChatBadRequestError(provider="cohere",
                                      message=f"Bad request (Status {status_code}). Detail: {error_text[:200]}") from e
        else:  # 5xx or other
            raise ChatProviderError(provider="cohere",
                                    message=f"API error (Status {status_code}). Detail: {error_text[:200]}",
                                    status_code=status_code) from e
    except requests.exceptions.RequestException as e:
        raise ChatProviderError(provider="cohere", message=f"Network error: {str(e)}", status_code=504) from e
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Cohere: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="cohere", message=f"Cohere config/data error: {e}") from e


# https://console.groq.com/docs/quickstart
def chat_with_groq(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        maxp: Optional[float] = None  # Mapped from 'maxp' (becomes top_p)
):
    logging.debug("Groq: Chat process starting...")
    loaded_config_data = load_and_log_configs()
    groq_config = loaded_config_data.get('groq_api', {})

    groq_api_key = api_key or groq_config.get('api_key')
    if not groq_api_key:
        logging.error("Groq: API key is missing.")
        raise ChatConfigurationError(provider="groq", message="Groq API Key is required but not found.")

    log_key = f"{groq_api_key[:5]}...{groq_api_key[-5:]}" if groq_api_key and len(groq_api_key) > 9 else "Provided Key"
    logging.debug(f"Groq: Using API Key: {log_key}")

    current_model = model or groq_config.get('model', 'llama3-8b-8192')
    current_temp = temp if temp is not None else float(groq_config.get('temperature', 0.2))
    current_top_p = maxp if maxp is not None else float(groq_config.get('top_p', 0.9))  # 'maxp' maps to 'top_p'
    current_streaming = streaming if streaming is not None else groq_config.get('streaming', False)
    current_max_tokens = _safe_cast(groq_config.get('max_tokens'), int)  # Groq uses 'max_tokens'

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="groq",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Groq: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}")
    if custom_prompt_arg:
        logging.warning(
            "Groq: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. Groq is OpenAI compatible, supports multimodal if model does.
    api_messages.extend(input_data)

    headers = {
        'Authorization': f'Bearer {groq_api_key}',
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "temperature": current_temp,
        "stream": current_streaming,
        "top_p": current_top_p,
    }
    if current_max_tokens is not None:
        data["max_tokens"] = current_max_tokens

    api_url = 'https://api.groq.com/openai/v1/chat/completions'
    try:
        if current_streaming:
            logging.debug("Groq: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"Groq: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"Groq: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("Groq: Posting request (non-streaming)")
            retry_count = int(groq_config.get('api_retries', 3))
            retry_delay = float(groq_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("Groq: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Groq: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="groq", message=f"Groq config/data error: {e}") from e


def chat_with_openrouter(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        top_p: Optional[float] = None,  # Mapped from 'topp' (OpenRouter uses 'top_p')
        top_k: Optional[int] = None,  # Mapped from 'topk' (OpenRouter uses 'top_k')
        minp: Optional[float] = None  # Mapped from 'minp' (OpenRouter uses 'min_p')
):
    logging.info("OpenRouter: Chat request started.")
    loaded_config_data = load_and_log_configs()
    openrouter_config = loaded_config_data.get('openrouter_api', {})

    openrouter_api_key = api_key or openrouter_config.get('api_key')
    if not openrouter_api_key:
        logging.error("OpenRouter: API key is missing.")
        raise ChatConfigurationError(provider='openrouter',
                                     message="OpenRouter API Key is required but not configured.")

    log_key = f"{openrouter_api_key[:5]}...{openrouter_api_key[-5:]}" if openrouter_api_key and len(
        openrouter_api_key) > 9 else "Provided Key"
    logging.debug(f"OpenRouter: Using API Key: {log_key}")

    current_model = model or openrouter_config.get('model', 'mistralai/mistral-7b-instruct:free')
    current_temp = temp if temp is not None else float(openrouter_config.get('temperature', 0.7))
    current_top_p = top_p if top_p is not None else float(openrouter_config.get('top_p', 0.95))
    current_top_k = top_k if top_k is not None else _safe_cast(openrouter_config.get('top_k'), int)
    current_min_p = minp  # 'minp' from chat_api_call maps to OpenRouter's 'min_p'
    current_streaming = streaming if streaming is not None else openrouter_config.get('streaming', False)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider='openrouter',
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"OpenRouter: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, TopK: {current_top_k}, MinP: {current_min_p}")
    if custom_prompt_arg:
        logging.warning(
            "OpenRouter: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. OpenRouter is OpenAI compatible, supports multimodal if model does.
    api_messages.extend(input_data)

    if not any(msg.get('role') == 'user' for msg in api_messages):
        raise ChatBadRequestError(provider='openrouter', message="No user message found in the request for OpenRouter.")

    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": openrouter_config.get("site_url", "http://localhost"),
        "X-Title": openrouter_config.get("site_name", "TLDW-API"),
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if current_min_p is not None: data["min_p"] = current_min_p  # OpenRouter uses 'min_p'

    api_url = "https://openrouter.ai/api/v1/chat/completions"
    try:
        if current_streaming:
            logging.debug("OpenRouter: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True,
                                        timeout=int(openrouter_config.get('api_timeout', 180)))
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"OpenRouter: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"OpenRouter: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("OpenRouter: Posting request (non-streaming)")
            retry_count = int(openrouter_config.get('api_retries', 3))
            retry_delay = float(openrouter_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data,
                                        timeout=int(openrouter_config.get('api_timeout', 120)))
            response.raise_for_status()  # Check HTTP status first

            response_data = response.json()
            if isinstance(response_data, dict) and 'error' in response_data:  # Check for errors in JSON payload
                error_info = response_data['error']
                error_message = error_info.get('message', 'Unknown OpenRouter error')
                logging.error(f"OpenRouter: API call failed (reported in JSON): {error_message}")
                # Raise a specific error type based on OpenRouter's error structure if possible
                # For now, generic ChatProviderError
                status_code_in_error = _safe_cast(error_info.get('code'), int, 500)
                if status_code_in_error == 401:
                    raise ChatAuthenticationError(provider='openrouter', message=error_message)
                elif status_code_in_error == 429:
                    raise ChatRateLimitError(provider='openrouter', message=error_message)
                elif 400 <= status_code_in_error < 500:
                    raise ChatBadRequestError(provider='openrouter', message=error_message)
                else:
                    raise ChatProviderError(provider='openrouter', message=error_message,
                                            status_code=status_code_in_error)

            logging.debug("OpenRouter: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"OpenRouter: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider='openrouter', message=f"OpenRouter config/data error: {e}") from e


def chat_with_huggingface(
        model: Optional[str],
        input_data: List[Dict[str, Any]],
        api_key: Optional[str] = None,
        custom_prompt_arg: Optional[str] = None, # Generally ignored
        temp: Optional[float] = None,
        system_prompt: Optional[str] = None, # This is the mapped 'system_message'
        streaming: Optional[bool] = None,
        top_p: Optional[float] = None,
        top_k: Optional[int] = None,
        max_new_tokens: Optional[int] = None,
        **kwargs # Catch-all for other params from chat_api_call if needed
):
    logging.debug(f"HuggingFace Chat: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    hf_config = loaded_config_data.get('huggingface_api', {})

    final_api_key = api_key or hf_config.get('api_key')
    # Note: TGI often doesn't require an API key if it's self-hosted and unsecured,
    # but Inference Endpoints will.
    if not final_api_key:
        logging.warning("HuggingFace: API key is missing. Proceeding without, assuming local/unsecured TGI or endpoint allows it.")
        # Do not raise ChatConfigurationError immediately if key can be optional for some setups.
        # The API call will fail if the endpoint actually requires it.

    if final_api_key:
        log_key_display = f"{final_api_key[:5]}...{final_api_key[-5:]}" if len(final_api_key) > 9 else "Provided Key"
        logging.debug(f"HuggingFace: Using API Key: {log_key_display}")

    # Model ID for the payload. This is what the TGI/endpoint will load.
    # The 'model' arg to this function is the primary source.
    final_model_id_for_payload = model or hf_config.get('model') # 'model' not 'model_id' from config
    if not final_model_id_for_payload:
        raise ChatConfigurationError(provider="huggingface", message="HuggingFace model ID for the payload is required but not found.")

    # Construct the API URL
    # The 'api_base_url' from config MUST be the base URL of your TGI or Inference Endpoint
    # e.g., "http://localhost:8080" or "https://your-endpoint.hf.space"
    configured_api_base_url = hf_config.get('api_base_url')
    if not configured_api_base_url:
        logging.error("HuggingFace: Critical - 'api_base_url' for a chat completions endpoint (TGI/Inference Endpoint) is not configured in 'huggingface_api' settings.")
        raise ChatConfigurationError(provider="huggingface", message="HuggingFace API base URL for chat completions is not configured.")

    # Default chat path for OpenAI-compatible endpoints
    chat_completions_path = hf_config.get('api_chat_path', 'v1/chat/completions').strip('/')
    api_url = f"{configured_api_base_url.rstrip('/')}/{chat_completions_path}"

    logging.debug(f"HuggingFace: Targeting API URL: {api_url}")
    logging.debug(f"HuggingFace: Model for payload: {final_model_id_for_payload}")

    # Model ID is critical. It can be part of the API URL for serverless or a param for TGI-like.
    # The PROVIDER_PARAM_MAP passes 'model' so we use it.
    current_model_id = model or hf_config.get('model_id')  # Use 'model_id' from config as it's more specific
    if not current_model_id:
        raise ChatConfigurationError(provider="huggingface", message="HuggingFace model ID is required but not found.")

    # Resolve parameters
    final_temp = temp if temp is not None else _safe_cast(hf_config.get('temperature'), float, 0.7)

    if streaming is not None:
        final_streaming = streaming
    else:
        config_streaming = hf_config.get('streaming', False)
        final_streaming = str(config_streaming).lower() == "true" if isinstance(config_streaming, str) else bool(config_streaming)

    # Use TGI-common parameter names from config if available, or direct args
    final_top_p = top_p if top_p is not None else _safe_cast(hf_config.get('top_p'), float)
    final_top_k = top_k if top_k is not None else _safe_cast(hf_config.get('top_k'), int)
    # For max tokens, TGI uses 'max_new_tokens'. OpenAI uses 'max_tokens'.
    # ChatCompletionRequest schema has 'max_tokens'. If your chat_api_call maps it to 'max_new_tokens'
    # for this provider, great. Otherwise, handle it here.
    # Let's assume chat_api_call sends 'max_tokens' if present. We will use it as 'max_tokens' if the TGI
    # endpoint is strictly OpenAI compatible, or map to 'max_new_tokens' if that's what your specific TGI needs.
    # For now, using 'max_tokens' from ChatCompletionRequest via **kwargs if present, else from config.
    final_max_tokens = max_new_tokens if max_new_tokens is not None else _safe_cast(hf_config.get('max_tokens', 1024), int)
    # If 'max_tokens' comes from chat_api_call's generic params, it will be in kwargs
    # if not explicitly in this function's signature & PROVIDER_PARAM_MAP.
    # Let's assume 'max_tokens' (OpenAI style) is expected by the TGI endpoint.
    if "max_tokens" in kwargs and kwargs["max_tokens"] is not None: # Passed from chat_api_call
        final_max_tokens = int(kwargs["max_tokens"])

    if custom_prompt_arg:
        logging.warning("HuggingFace: 'custom_prompt_arg' was provided but is generally ignored.")

    api_messages = []
    # system_prompt here maps to generic system_message
    if system_prompt:  # This is the mapped system_message
        api_messages.append({"role": "system", "content": system_prompt})

    # input_data is messages_payload. Assumes TGI OpenAI compatibility for multimodal.
    api_messages.extend(input_data)

    headers = {'Content-Type': 'application/json'}
    if final_api_key: # Only add auth header if a key is provided
        headers["Authorization"] = f"Bearer {final_api_key}"

    payload = {
        "model": final_model_id_for_payload,
        "messages": api_messages,
        "stream": final_streaming,
    }
    # Add optional parameters if they have values
    if final_temp is not None: payload["temperature"] = float(final_temp)
    if final_top_p is not None: payload["top_p"] = float(final_top_p)
    if final_top_k is not None: payload["top_k"] = int(final_top_k)
    if final_max_tokens is not None: payload["max_tokens"] = final_max_tokens # Or "max_new_tokens" depending on your TGI

    # Include other OpenAI compatible params from kwargs if TGI supports them
    for param in ["frequency_penalty", "logit_bias", "logprobs", "top_logprobs",
                  "presence_penalty", "response_format", "seed", "stop", "tools", "tool_choice", "user"]:
        if param in kwargs and kwargs[param] is not None:
            payload[param] = kwargs[param]

    logging.debug(f"HuggingFace Payload (excluding messages): { {k:v for k,v in payload.items() if k != 'messages'} }")

    try:
        if final_streaming:
            logging.debug("HuggingFace: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=payload, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n" # For endpoint wrapper
                    except requests.exceptions.ChunkedEncodingError as e_chunk:
                        logging.error(f"HuggingFace: ChunkedEncodingError during stream: {e_chunk}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream connection error: {str(e_chunk)}", "type": "huggingface_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    except Exception as e_stream:
                        logging.error(f"HuggingFace: Error during stream iteration: {e_stream}", exc_info=True)
                        error_content = json.dumps({"error": {"message": f"Stream iteration error: {str(e_stream)}", "type": "huggingface_stream_error"}})
                        yield f"data: {error_content}\n\n"
                    finally:
                        yield "data: [DONE]\n\n"
                        if response: response.close()
                return stream_generator()
        else:
            logging.debug("HuggingFace: Posting request (non-streaming)")
            retry_count = int(hf_config.get('api_retries', 3))
            retry_delay = float(hf_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                session.mount("http://", adapter)
                response = session.post(api_url, headers=headers, json=payload, timeout=float(hf_config.get('api_timeout', 120.0)))
            response.raise_for_status()
            response_data = response.json()
            logging.debug("HuggingFace: Non-streaming request successful.")
            return response_data

    except requests.exceptions.HTTPError as e:
        error_text = e.response.text if e.response is not None else "No response body"
        logging.error(f"HuggingFace HTTPError {e.response.status_code if e.response is not None else 'Unknown'}: {error_text[:500]}", exc_info=True)
        raise # Re-raise for chat_api_call to map to ChatProviderError etc.
    except requests.exceptions.RequestException as e:
        logging.error(f"HuggingFace RequestException: {e}", exc_info=True)
        raise # Re-raise
    except (ValueError, KeyError, TypeError) as e: # Catch issues from param processing within this func
        logging.error(f"HuggingFace: Internal configuration or data error: {e}", exc_info=True)
        # Raise a more specific error that chat_api_call can map
        raise ChatBadRequestError(provider="huggingface", message=f"HuggingFace data/config error: {e}") from e


def chat_with_deepseek(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None  # Mapped from 'topp' (becomes top_p)
):
    logging.debug("DeepSeek: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    deepseek_config = loaded_config_data.get('deepseek_api', {})

    deepseek_api_key = api_key or deepseek_config.get('api_key')
    if not deepseek_api_key:
        logging.error("DeepSeek: API key is missing.")
        raise ChatConfigurationError(provider="deepseek", message="DeepSeek API Key is required but not found.")

    log_key = f"{deepseek_api_key[:5]}...{deepseek_api_key[-5:]}" if deepseek_api_key and len(
        deepseek_api_key) > 9 else "Provided Key"
    logging.debug(f"DeepSeek: Using API Key: {log_key}")

    current_model = model or deepseek_config.get('model', 'deepseek-chat')
    current_temp = temp if temp is not None else float(
        deepseek_config.get('temperature', 0.1))  # DeepSeek often uses low temp
    current_top_p = topp if topp is not None else float(deepseek_config.get('top_p', 0.95))
    current_streaming = streaming if streaming is not None else deepseek_config.get('streaming', False)
    current_max_tokens = _safe_cast(deepseek_config.get('max_tokens'), int)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="deepseek",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"DeepSeek: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}")
    if custom_prompt_arg:
        logging.warning("DeepSeek: 'custom_prompt_arg' was provided but is generally ignored.")

    # The old logic for reading file from input_data is removed. input_data is messages_payload.
    # if os.path.isfile(input_data): ... This is no longer valid.

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. DeepSeek is OpenAI compatible.
    api_messages.extend(input_data)

    headers = {
        'Authorization': f'Bearer {deepseek_api_key}',
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "stream": current_streaming,
        "temperature": current_temp,
        "top_p": current_top_p,
    }
    if current_max_tokens is not None:
        data["max_tokens"] = current_max_tokens

    api_url = 'https://api.deepseek.com/chat/completions'
    try:
        if current_streaming:
            logging.debug("DeepSeek: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"DeepSeek: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"DeepSeek: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("DeepSeek: Posting request (non-streaming)")
            retry_count = int(deepseek_config.get('api_retries', 3))
            retry_delay = float(deepseek_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("DeepSeek: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"DeepSeek: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="deepseek", message=f"DeepSeek config/data error: {e}") from e


def chat_with_mistral(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None  # Mapped from 'topp' (becomes top_p)
):
    logging.debug("Mistral: Chat request started")
    loaded_config_data = load_and_log_configs()
    mistral_config = loaded_config_data.get('mistral_api', {})

    mistral_api_key = api_key or mistral_config.get('api_key')
    if not mistral_api_key:
        logging.error("Mistral: API key is missing.")
        raise ChatConfigurationError(provider="mistral", message="Mistral API Key is required but not found.")

    log_key = f"{mistral_api_key[:5]}...{mistral_api_key[-5:]}" if mistral_api_key and len(
        mistral_api_key) > 9 else "Provided Key"
    logging.debug(f"Mistral: Using API Key: {log_key}")

    current_model = model or mistral_config.get('model', 'mistral-large-latest')
    current_temp = temp if temp is not None else float(mistral_config.get('temperature', 0.1))
    current_top_p = topp if topp is not None else float(mistral_config.get('top_p', 0.95))
    current_streaming = streaming if streaming is not None else mistral_config.get('streaming', False)
    safe_prompt = bool(mistral_config.get('safe_prompt', False))
    current_max_tokens = _safe_cast(mistral_config.get('max_tokens'), int)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="mistral",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Mistral: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, SafePrompt: {safe_prompt}")
    if custom_prompt_arg:
        logging.warning("Mistral: 'custom_prompt_arg' was provided but is generally ignored.")

    # The old logic for extract_text_from_segments is removed. input_data is messages_payload.

    api_messages = []
    # Mistral API uses system message as the first message in the list if role is "system"
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. Mistral API supports OpenAI message format including multimodal.
    api_messages.extend(input_data)

    headers = {
        'Authorization': f'Bearer {mistral_api_key}',
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "temperature": current_temp,
        "top_p": current_top_p,
        "stream": current_streaming,
        "safe_prompt": safe_prompt
    }
    if current_max_tokens is not None:
        data["max_tokens"] = current_max_tokens

    api_url = 'https://api.mistral.ai/v1/chat/completions'
    try:
        if current_streaming:
            logging.debug("Mistral: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"Mistral: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"Mistral: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("Mistral: Posting request (non-streaming)")
            retry_count = int(mistral_config.get('api_retries', 3))
            retry_delay = float(mistral_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("Mistral: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Mistral: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="mistral", message=f"Mistral config/data error: {e}") from e


def chat_with_google(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message' (becomes system_instruction)
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None,  # Mapped from 'topp' (becomes topP for Gemini)
        topk: Optional[int] = None  # Mapped from 'topk' (becomes topK for Gemini)
):
    logging.debug("Google Gemini: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    google_config = loaded_config_data.get('google_api', {})

    google_api_key = api_key or google_config.get('api_key')
    if not google_api_key:
        logging.error("Google Gemini: API key is missing.")
        raise ChatConfigurationError(provider="google", message="Google API Key is required but not found.")

    log_key = f"{google_api_key[:5]}...{google_api_key[-5:]}" if google_api_key and len(
        google_api_key) > 9 else "Provided Key"
    logging.debug(f"Google Gemini: Using API Key: {log_key}")

    current_model = model or google_config.get('model', 'gemini-1.5-flash-latest')  # Use flash for speed/cost
    current_temp = temp if temp is not None else float(google_config.get('temperature', 0.7))
    current_top_p = topp  # 'topp' from chat_api_call maps to Gemini's 'topP'
    current_top_k = topk  # 'topk' from chat_api_call maps to Gemini's 'topK'
    current_streaming = streaming if streaming is not None else google_config.get('streaming', False)
    current_system_instruction_text = system_message  # 'system_message' from chat_api_call maps to Gemini's system_instruction
    max_output_tokens = _safe_cast(google_config.get('max_output_tokens', 8192), int)  # Gemini uses maxOutputTokens

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="google",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Google Gemini: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, TopK: {current_top_k}")
    if custom_prompt_arg:
        logging.warning("Google Gemini: 'custom_prompt_arg' was provided but is generally ignored.")

    # Transform OpenAI messages_payload to Gemini's 'contents' format
    gemini_contents = []
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")  # Can be string or list of parts

        gemini_role = "user" if role == "user" else "model" if role == "assistant" else None
        if not gemini_role:
            logging.warning(f"Google Gemini: Skipping message with unsupported role: {role}")
            continue

        gemini_parts = []
        if isinstance(content, str):
            gemini_parts.append({"text": content})
        elif isinstance(content, list):
            for part in content:
                part_type = part.get("type")
                if part_type == "text":
                    gemini_parts.append({"text": part.get("text", "")})
                elif part_type == "image_url":
                    image_url_data = part.get("image_url", {}).get("url", "")
                    parsed_image = _parse_data_url_for_multimodal(image_url_data)
                    if parsed_image:
                        mime_type, b64_data = parsed_image
                        gemini_parts.append({
                            "inline_data": {
                                "mime_type": mime_type,
                                "data": b64_data
                            }
                        })
                    else:
                        logging.warning(f"Google Gemini: Could not parse image_url, skipping: {image_url_data[:60]}...")
                        # Optionally add a placeholder text
                        # gemini_parts.append({"text": "[Image data unparseable or not provided]"})

        if gemini_parts:
            gemini_contents.append({"role": gemini_role, "parts": gemini_parts})
        elif gemini_role == "user" and not gemini_parts:
            logging.warning(f"Google Gemini: User message has no processable content parts. Adding placeholder.")
            gemini_contents.append({"role": gemini_role,
                                    "parts": [{"text": "(User input was empty or only contained unparseable images)"}]})

    if not any(c['role'] == 'user' for c in gemini_contents):
        raise ChatBadRequestError(provider="google",
                                  message="No valid user messages found after processing for Google Gemini.")

    generation_config = {}
    if current_temp is not None: generation_config["temperature"] = current_temp
    if current_top_p is not None: generation_config["topP"] = current_top_p
    if current_top_k is not None: generation_config["topK"] = current_top_k
    if max_output_tokens is not None: generation_config["maxOutputTokens"] = max_output_tokens

    payload = {
        "contents": gemini_contents,
    }
    if generation_config: payload["generationConfig"] = generation_config
    if current_system_instruction_text:
        payload["system_instruction"] = {"parts": [{"text": current_system_instruction_text}]}

    stream_suffix = ":streamGenerateContent?alt=sse" if current_streaming else ":generateContent"
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{current_model}{stream_suffix}"
    headers = {
        'x-goog-api-key': google_api_key,
        'Content-Type': 'application/json',
    }

    try:
        retry_count = int(google_config.get('api_retries', 3))
        retry_delay = float(google_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 503],
                               allowed_methods=["POST"])  # Google uses 400 for some quota issues
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=payload, stream=current_streaming, timeout=180)

        response.raise_for_status()

        if current_streaming:
            logging.debug("Google Gemini: Streaming response received.")

            def stream_generator():
                try:
                    for line in response.iter_lines(decode_unicode=True):
                        if line and line.strip().startswith('data:'):
                            json_str = line.strip()[len('data:'):]
                            try:
                                data_chunk = json.loads(json_str)
                                # Gemini stream format is a list of GenerateContentResponse
                                # Each response has candidates -> content -> parts -> text
                                chunk_text = ""
                                if isinstance(data_chunk, list):  # Sometimes it's a list of chunks
                                    for sub_chunk in data_chunk:
                                        candidates = sub_chunk.get('candidates', [])
                                        if candidates and candidates[0].get('content', {}).get('parts', []):
                                            chunk_text += candidates[0]['content']['parts'][0].get('text', '')
                                else:  # Single chunk object
                                    candidates = data_chunk.get('candidates', [])
                                    if candidates and candidates[0].get('content', {}).get('parts', []):
                                        chunk_text = candidates[0]['content']['parts'][0].get('text', '')

                                if chunk_text:
                                    # Yield in OpenAI-like delta format for consistency downstream
                                    yield f"data: {json.dumps({'choices': [{'delta': {'content': chunk_text}}]})}\n\n"
                            except json.JSONDecodeError:
                                logging.warning(f"Google Gemini: Could not decode JSON line from stream: {line}")
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:
                    logging.error(f"Google Gemini: ChunkedEncodingError during stream: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logging.error(f"Google Gemini: Error during stream iteration: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                finally:
                    response.close()

            return stream_generator()
        else:
            logging.debug("Google Gemini: Non-streaming request successful.")
            response_data = response.json()
            assistant_content = ""
            finish_reason = "unknown"
            if response_data.get("candidates"):
                candidate = response_data["candidates"][0]
                if candidate.get("content", {}).get("parts"):
                    for part in candidate["content"]["parts"]:
                        assistant_content += part.get("text", "")
                finish_reason = candidate.get("finishReason", "unknown").lower()
                if finish_reason == "max_tokens":
                    finish_reason = "length"
                elif finish_reason == "stop":
                    finish_reason = "stop"
                # Other mappings as needed

            normalized_response = {
                "id": f"gemini-{time.time_ns()}",  # Google doesn't provide a top-level response ID
                "object": "chat.completion",
                "created": int(time.time()),
                "model": current_model,  # Google response doesn't echo model
                "choices": [{
                    "index": 0,
                    "message": {"role": "assistant", "content": assistant_content.strip()},
                    "finish_reason": finish_reason
                }],
                "usage": {  # Map usageMetadata
                    "prompt_tokens": response_data.get("usageMetadata", {}).get("promptTokenCount"),
                    "completion_tokens": response_data.get("usageMetadata", {}).get("candidatesTokenCount"),
                    # Sum if multiple candidates, but usually one
                    "total_tokens": response_data.get("usageMetadata", {}).get("totalTokenCount")
                }
            }
            return normalized_response
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Google Gemini: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="google", message=f"Google Gemini config/data error: {e}") from e

#
#
#######################################################################################################################