# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import os
import time
from typing import List, Any
#
# Import 3rd-Party Libraries
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from tldw_Server_API.app.core.Chat.Chat_Functions import ChatAuthenticationError, ChatRateLimitError, \
    ChatBadRequestError, ChatProviderError, ChatConfigurationError
#
# Import Local libraries
from tldw_Server_API.app.core.Utils.Utils import load_and_log_configs, logging
#
#######################################################################################################################
# Function Definitions
#

# FIXME: Update to include full arguments

# --- Helper function for safe type conversion ---
def _safe_cast(value: Any, cast_to: type, default: Any = None) -> Any:
    """Safely casts value to specified type, returning default on failure."""
    if value is None:
        return default
    try:
        return cast_to(value)
    except (ValueError, TypeError):
        logging.warning(f"Could not cast '{value}' to {cast_to}. Using default: {default}")
        return default

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.

    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.

    Returns:
        List[float]: The embeddings generated by the API.
    """
    loaded_config_data = load_and_log_configs()
    api_key = loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI: API key not found or is empty")
        raise ValueError("OpenAI: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }

    request_data = {
        "input": input_data,
        "model": model,
    }

    try:
        logging.debug("OpenAI: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI: Embedding data not found in the response")
                raise ValueError("OpenAI: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI: Embeddings request failed with status code {response.status_code}")
            logging.error(f"OpenAI: Error response: {response.text}")
            raise ValueError(f"OpenAI: Failed to retrieve embeddings. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI: Unexpected error occurred: {str(e)}")


def chat_with_openai(api_key, input_data, custom_prompt_arg, temp, system_message, streaming, maxp, model):
    loaded_config_data = load_and_log_configs()
    openai_config = loaded_config_data.get('openai_api', {})

    # --- API Key Resolution ---
    openai_api_key = api_key
    if not openai_api_key:
        logging.info("OpenAI: API key not provided as parameter, checking config.")
        openai_api_key = openai_config.get('api_key') # Get from loaded config

    if not openai_api_key:
        # Let chat_api_call handle raising ChatConfigurationError based on its logic
        logging.error("OpenAI: API key is missing.")
        raise ValueError("OpenAI API Key is required but not found.") # Raise error

    log_key = f"{openai_api_key[:5]}...{openai_api_key[-5:]}" if len(openai_api_key) > 9 else "Provided Key"
    logging.debug(f"OpenAI: Using API Key: {log_key}")

    # --- Parameter Resolution (with fallbacks to config) ---
    if streaming is None: streaming = openai_config.get('streaming', False)
    if temp is None: temp = float(openai_config.get('temperature', 0.7))
    if maxp is None: maxp = float(openai_config.get('top_p', 0.95))
    if model is None: model = openai_config.get('model', 'gpt-4o-mini')
    if system_message is None: system_message = "You are a helpful AI assistant."
    # Note: OpenAI uses max_tokens in the request body, not max_completion_tokens anymore for chat
    max_tokens = int(openai_config.get('max_tokens', 4096))

    # Type validation for streaming
    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"OpenAI: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {maxp}")

    # --- Prepare Request ---
    headers = {
        'Authorization': f'Bearer {openai_api_key}',
        'Content-Type': 'application/json'
    }

    # Handle input_data format (assuming it's typically message list)
    if isinstance(input_data, list):
        messages = input_data # Assume it's already in OpenAI message format
        # Inject system message if not already present
        if not any(msg.get('role') == 'system' for msg in messages):
            messages.insert(0, {"role": "system", "content": system_message})
        # Append custom_prompt_arg to the last user message if provided
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg: # If no user message or only system message
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str): # If only a string is passed
         messages = [
             {"role": "system", "content": system_message},
             {"role": "user", "content": f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for OpenAI. Expected list of messages or string.")

    data = {
        "model": model,
        "messages": messages,
        "max_tokens": max_tokens, # Use max_tokens
        "temperature": temp,
        "stream": streaming,
        "top_p": maxp
    }

    # --- Execute Request ---
    api_url = 'https://api.openai.com/v1/chat/completions'
    try:
        if streaming:
            logging.debug("OpenAI: Posting request (streaming)")
            # Use context manager for session
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=60) # Add timeout
                response.raise_for_status() # IMPORTANT: Check for initial errors (401, 404, etc.)

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line.strip(): # Ensure line is not empty
                                yield line + "\n\n" # Pass through the raw SSE line + ensure double newline
                        # Ensure DONE is sent if loop finishes normally
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"OpenAI: Error during stream iteration: {e}", exc_info=True)
                        # Yield an error chunk if possible, then DONE
                        error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close() # Ensure connection is closed

                return stream_generator() # Return the generator
        else:
            logging.debug("OpenAI: Posting request (non-streaming)")
            # Configure retry strategy
            retry_count = int(openai_config.get('api_retries', 3))
            retry_delay = float(openai_config.get('api_retry_delay', 1))
            retry_strategy = Retry(
                total=retry_count,
                backoff_factor=retry_delay,
                status_forcelist=[429, 500, 502, 503, 504], # Retry on these statuses
                allowed_methods=["POST"] # Important: Allow retries for POST
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            # Use context manager for session
            with requests.Session() as session:
                session.mount("https://", adapter)
                session.mount("http://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=30) # Add timeout

            logging.debug(f"Full API response status: {response.status_code}")
            response.raise_for_status() # Raise HTTPError for 4xx/5xx AFTER retries

            response_data = response.json() # Parse JSON on success
            logging.debug("OpenAI: Non-streaming request successful.")
            #logging.debug(f"OpenAI Raw Response Data: {response_data}") # Optional: Log raw response
            return response_data # Return the full dictionary

    # Let RequestException and HTTPError propagate up to chat_api_call
    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"OpenAI: Configuration or data error: {e}", exc_info=True)
         # Re-raise as ValueError which chat_api_call can map to BadRequest/Config error
         raise ValueError(f"OpenAI config/data error: {e}") from e


def chat_with_anthropic(api_key, input_data, model, custom_prompt_arg,
                        system_prompt=None, temp=None, streaming=False, topp=None, topk=None):
    # Removed default retry params, handled by requests adapter now
    loaded_config_data = load_and_log_configs()
    if loaded_config_data is None:
        # Should not happen if load_and_log_configs handles its errors
        raise ValueError("Anthropic Chat: Failed to load configuration data.")
    anthropic_config = loaded_config_data.get('anthropic_api', {})

    # --- API Key Resolution ---
    anthropic_api_key = api_key
    if not anthropic_api_key:
        logging.info("Anthropic: API key not provided, checking config.")
        anthropic_api_key = anthropic_config.get('api_key')

    if not anthropic_api_key:
        logging.error("Anthropic: API key is missing.")
        raise ValueError("Anthropic API Key is required but not found.")

    log_key = f"{anthropic_api_key[:5]}...{anthropic_api_key[-5:]}" if len(anthropic_api_key) > 9 else "Provided Key"
    logging.debug(f"Anthropic: Using API Key: {log_key}")

    # --- Parameter Resolution ---
    if model is None: model = anthropic_config.get('model', 'claude-3-haiku-20240307')
    if temp is None: temp = float(anthropic_config.get('temperature', 0.7))
    if topp is None: topp = float(anthropic_config.get('top_p', 0.95)) # Anthropic uses top_p
    # top_k is less common, handle if provided
    if topk is not None: topk = int(topk) # Ensure int if passed
    if system_prompt is None: system_prompt = "You are a helpful assistant."
    max_tokens = int(anthropic_config.get('max_tokens', 1024))

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"Anthropic: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {topp}, TopK: {topk}")

    # --- Prepare Request ---
    headers = {
        'x-api-key': anthropic_api_key,
        'anthropic-version': '2023-06-01', # Keep specific version or update as needed
        'Content-Type': 'application/json'
    }

    # Handle input_data (assuming similar logic to OpenAI)
    if isinstance(input_data, list):
        messages = input_data
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg:
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str):
         messages = [
             {"role": "user", "content": f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for Anthropic. Expected list of messages or string.")

    data = {
        "model": model,
        "max_tokens": max_tokens,
        "messages": messages,
        "temperature": temp,
        "stream": streaming,
        "system": system_prompt
        # Conditionally add top_p and top_k if they are not None
        # Anthropic API might error if top_p is None, default is often 1.0
    }
    if topp is not None:
        data["top_p"] = topp
    if topk is not None:
        data["top_k"] = topk


    # --- Execute Request ---
    api_url = 'https://api.anthropic.com/v1/messages'
    try:
        # Configure retry strategy
        retry_count = int(anthropic_config.get('api_retries', 3))
        retry_delay = float(anthropic_config.get('api_retry_delay', 1))
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504], # Retry on these statuses
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER potential retries

            if streaming:
                logging.debug("Anthropic: Streaming response received.")
                def stream_generator():
                    try:
                        # Anthropic streaming uses Server-Sent Events format
                        for line in response.iter_lines(decode_unicode=True):
                            if line.strip():
                                yield line + "\n\n" # Pass through raw SSE line
                        # Anthropic stream ends implicitly when connection closes,
                        # but sending [DONE] is good practice for client handling.
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"Anthropic: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                         response.close()
                return stream_generator()
            else:
                logging.debug("Anthropic: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"Anthropic Raw Response Data: {response_data}") # Optional
                return response_data # Return the full dictionary

    # Let RequestException and HTTPError propagate up to chat_api_call
    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"Anthropic: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"Anthropic config/data error: {e}") from e


# Summarize with Cohere
def chat_with_cohere(api_key, input_data, model=None, custom_prompt_arg=None, system_prompt=None, temp=None, streaming=False, topp=None, topk=None):
    loaded_config_data = load_and_log_configs()
    cohere_config = loaded_config_data.get('cohere_api', {})

    # --- API Key ---
    cohere_api_key = api_key or cohere_config.get('api_key')
    if not cohere_api_key:
        logging.error("Cohere: API key is missing.")
        raise ValueError("Cohere API Key is required but not found.")
    log_key = f"{cohere_api_key[:3]}...{cohere_api_key[-3:]}" if len(cohere_api_key) > 5 else "Provided Key"
    logging.debug(f"Cohere: Using API Key: {log_key}")

    # --- Parameters ---
    if model is None: model = cohere_config.get('model', 'command-r')
    if temp is None: temp = float(cohere_config.get('temperature', 0.3))
    # Cohere uses 'k' for top-k and 'p' for top-p (maps from topp)
    top_k = topk if topk is not None else int(cohere_config.get('top_k', 0)) # Cohere default is 0
    top_p = topp if topp is not None else float(cohere_config.get('max_p', 0.75)) # Map 'topp' or 'max_p' config
    if system_prompt is None: system_prompt = "You are a helpful assistant."

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = cohere_config.get('streaming', False)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"Cohere: Streaming: {streaming}, Model: {model}, Temp: {temp}, P: {top_p}, K: {top_k}")

    # --- Prepare Request ---
    headers = {
        'accept': 'application/json',
        'content-type': 'application/json',
        'Authorization': f'Bearer {cohere_api_key}'
    }

    # Cohere uses 'message' for the latest user input and 'chat_history'
    # We need to adapt the input_data (assuming OpenAI format list)
    chat_history = []
    user_message = ""
    if isinstance(input_data, list):
        for msg in input_data:
            role = msg.get('role', '').upper()
            content = msg.get('content', '')
            if role == 'USER':
                 chat_history.append({"role": "USER", "message": content})
                 user_message = content # Keep track of the last user message
            elif role == 'ASSISTANT' or role == 'CHATBOT': # Map assistant to CHATBOT
                 chat_history.append({"role": "CHATBOT", "message": content})
            # System messages handled separately by Cohere API
    elif isinstance(input_data, str):
         user_message = input_data # Treat string input as the user message
    else:
        raise ValueError("Invalid 'input_data' format for Cohere. Expected list of messages or string.")

    # Append custom prompt to the latest user message
    if custom_prompt_arg:
        user_message += f"\n\n{custom_prompt_arg}"

    # Construct payload - Cohere Chat v2 structure
    data = {
        "model": model,
        "message": user_message, # Latest user message
        "chat_history": chat_history, # Previous turns
        "preamble": system_prompt, # System prompt equivalent
        "temperature": temp,
        "p": top_p,
        "k": top_k,
        # Add other Cohere params if needed e.g., connectors, documents
    }

    api_url = 'https://api.cohere.com/v1/chat' # Use v1 endpoint for now, v2 might differ slightly
    stream_param = "?stream=true" if streaming else "" # Cohere uses query param for streaming

    # --- Execute Request ---
    try:
        retry_count = int(cohere_config.get('api_retries', 3))
        retry_delay = float(cohere_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url + stream_param, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER retries

            if streaming:
                logging.debug("Cohere: Streaming response received.")
                def stream_generator():
                    buffer = ""
                    try:
                        for chunk in response.iter_content(chunk_size=None, decode_unicode=True):
                             buffer += chunk
                             # Cohere stream chunks might not be newline-separated JSON, process buffer
                             while '\n' in buffer:
                                 line, buffer = buffer.split('\n', 1)
                                 if line.strip():
                                     try:
                                         event = json.loads(line)
                                         # Determine event type and yield appropriate data
                                         event_type = event.get('event_type')
                                         if event_type == 'text-generation':
                                             yield f"data: {json.dumps(event)}\n\n" # Yield raw event or extracted text
                                         elif event_type == 'stream-end':
                                             logging.debug("Cohere stream end event received.")
                                             # Optionally yield the final response part if needed
                                             # final_response = event.get('response')
                                             # if final_response: yield f"data: {json.dumps(final_response)}\n\n"
                                             break # End the loop
                                         else:
                                              logging.debug(f"Cohere: Received event type: {event_type}")
                                              yield f"data: {json.dumps(event)}\n\n" # Pass other events through
                                     except json.JSONDecodeError:
                                         logging.warning(f"Cohere: Could not decode JSON line: {line}")
                        yield "data: [DONE]\n\n" # Signal end
                    except Exception as e:
                         logging.error(f"Cohere: Error during stream iteration: {e}", exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    finally:
                        response.close()
                return stream_generator()
            else:
                logging.debug("Cohere: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"Cohere Raw Response Data: {response_data}")
                return response_data # Return full dictionary

    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"Cohere: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"Cohere config/data error: {e}") from e
    # Let RequestException and HTTPError propagate


# https://console.groq.com/docs/quickstart
def chat_with_groq(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, maxp=None):
    logging.debug("Groq: Chat process starting...")
    loaded_config_data = load_and_log_configs()
    if loaded_config_data is None:
        raise ValueError("Groq: Failed to load configuration data.")
    groq_config = loaded_config_data.get('groq_api', {})

    # --- API Key Resolution ---
    groq_api_key = api_key
    if not groq_api_key:
        logging.info("Groq: API key not provided, checking config.")
        groq_api_key = groq_config.get('api_key')

    if not groq_api_key:
        logging.error("Groq: API key is missing.")
        raise ValueError("Groq API Key is required but not found.")

    log_key = f"{groq_api_key[:5]}...{groq_api_key[-5:]}" if len(groq_api_key) > 9 else "Provided Key"
    logging.debug(f"Groq: Using API Key: {log_key}")

    # --- Parameter Resolution ---
    # Groq uses OpenAI compatible API, params similar
    if temp is None: temp = float(groq_config.get('temperature', 0.2))
    # Groq uses 'top_p', map 'maxp' to it if provided, else use config 'top_p'
    top_p = maxp if maxp is not None else float(groq_config.get('top_p', 0.9))
    model = groq_config.get('model', 'llama3-8b-8192') # Get model from config
    if system_message is None: system_message = "You are a helpful AI assistant."

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = groq_config.get('streaming', False)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"Groq: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {top_p}")

    # --- Prepare Request ---
    headers = {
        'Authorization': f'Bearer {groq_api_key}',
        'Content-Type': 'application/json'
    }

    # Handle input_data (assuming similar logic to OpenAI)
    # Note: Your original code had text extraction logic here. Adapt if needed.
    if isinstance(input_data, list):
        messages = input_data
        if not any(msg.get('role') == 'system' for msg in messages):
             messages.insert(0, {"role": "system", "content": system_message})
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg:
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str):
         messages = [
             {"role": "system", "content": system_message},
             {"role": "user", "content": f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for Groq. Expected list of messages or string.")

    data = {
        "model": model,
        "messages": messages,
        "temperature": temp,
        "stream": streaming,
        "top_p": top_p
        # Add other OpenAI compatible params if needed (e.g., max_tokens from config)
    }
    max_tokens = groq_config.get('max_tokens') # Check if max_tokens is configured
    if max_tokens:
        try:
            data["max_tokens"] = int(max_tokens)
        except (ValueError, TypeError):
             logging.warning(f"Groq: Invalid max_tokens value in config: {max_tokens}. Ignoring.")


    # --- Execute Request ---
    # Groq uses an OpenAI compatible endpoint path
    api_url = 'https://api.groq.com/openai/v1/chat/completions'
    try:
        # Configure retry strategy
        retry_count = int(groq_config.get('api_retries', 3))
        retry_delay = float(groq_config.get('api_retry_delay', 1))
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER potential retries

            if streaming:
                logging.debug("Groq: Streaming response received.")
                # Groq stream is OpenAI compatible (SSE)
                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                             if line.strip():
                                 yield line + "\n\n" # Pass through raw SSE line
                        yield "data: [DONE]\n\n" # Ensure DONE is sent
                    except Exception as e:
                        logging.error(f"Groq: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                         response.close()
                return stream_generator()
            else:
                logging.debug("Groq: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"Groq Raw Response Data: {response_data}") # Optional
                return response_data # Return the full dictionary

    # Let RequestException and HTTPError propagate up to chat_api_call
    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"Groq: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"Groq config/data error: {e}") from e


def chat_with_openrouter(api_key=None, input_data=None, custom_prompt_arg=None, temp=None, system_message=None, streaming=False, top_p=None, top_k=None, minp=None, model=None):
    """
    Sends a chat request to the OpenRouter API.

    Handles OpenAI-compatible message format and OpenRouter-specific parameters.
    Checks for errors reported within the JSON payload even on 200 OK responses.
    """
    logging.info("OpenRouter: Chat request started.")
    loaded_config_data = load_and_log_configs()
    openrouter_config = loaded_config_data.get('openrouter_api', {})

    # --- API Key ---
    openrouter_api_key = api_key or openrouter_config.get('api_key')
    if not openrouter_api_key:
        logging.error("OpenRouter: API key is missing.")
        # Raise ChatConfigurationError for missing key on server-side config
        raise ChatConfigurationError(provider='openrouter', message="OpenRouter API Key is required but not configured on the server.")
    log_key = f"{openrouter_api_key[:5]}...{openrouter_api_key[-5:]}" if len(openrouter_api_key) > 9 else "Provided Key"
    logging.debug(f"OpenRouter: Using API Key: {log_key}")

    # --- Parameters ---
    if model is None: model = openrouter_config.get('model', 'mistralai/mistral-7b-instruct:free') # Use a free model as default for safety
    if temp is None: temp = float(openrouter_config.get('temperature', 0.7))
    # Map the 'topp' from chat_api_call (Pydantic 'top_p') to OpenRouter's 'top_p'
    if top_p is None: top_p = float(openrouter_config.get('top_p', 0.95)) # Use 0.95 default if None
    if top_k is None: top_k = int(openrouter_config.get('top_k', 100)) # Use default if None
    # min_p is specific, handle None carefully
    if minp is None:
        minp_config = openrouter_config.get('min_p')
        minp = float(minp_config) if minp_config is not None else None # Keep None if not in config

    if system_message is None: system_message = "You are a helpful AI assistant."

    # --- Streaming Parameter Handling ---
    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = openrouter_config.get('streaming', False)
    if not isinstance(streaming, bool):
        # Raise config error if streaming type is wrong
        raise ChatConfigurationError(provider='openrouter', message=f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"OpenRouter: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {top_p}, TopK: {top_k}, MinP: {minp}")

    # --- Prepare Request (OpenAI Compatible) ---
    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        # OpenRouter specific headers (optional)
        "HTTP-Referer": openrouter_config.get("site_url", "http://localhost"), # Example default
        "X-Title": openrouter_config.get("site_name", "TLDW-API"), # Example default
    }

    # --- Message Formatting ---
    messages = []
    # Add system message if provided and not already present
    has_system = False
    if isinstance(input_data, list):
        for msg in input_data:
            if isinstance(msg, dict) and msg.get('role') == 'system':
                has_system = True
                # Use provided system message if needed, otherwise keep the one from input_data
                messages.append({"role": "system", "content": msg.get("content", system_message)})
            elif isinstance(msg, dict) and msg.get('role'):
                messages.append(msg)
        if not has_system and system_message:
             messages.insert(0, {"role": "system", "content": system_message})
    elif isinstance(input_data, str):
         if system_message:
             messages.append({"role": "system", "content": system_message})
         messages.append({"role": "user", "content": input_data})
    else:
        raise ChatBadRequestError(provider='openrouter', message="Invalid 'input_data' format for OpenRouter. Expected list of messages or string.")

    # Append custom_prompt_arg if provided
    if custom_prompt_arg:
        if messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        else:
             messages.append({"role": "user", "content": custom_prompt_arg})

    # Ensure there's at least one user message
    if not any(msg.get('role') == 'user' for msg in messages):
         raise ChatBadRequestError(provider='openrouter', message="No user message found in the request for OpenRouter.")


    # --- Request Payload ---
    data = {
        "model": model,
        "messages": messages,
        "temperature": temp,
        "stream": streaming,
        "top_p": top_p,
        "top_k": top_k,
        # Only include min_p if it has a value
    }
    if minp is not None:
        data["min_p"] = minp

    api_url = "https://openrouter.ai/api/v1/chat/completions"

    # --- Execute Request ---
    try:
        retry_count = int(openrouter_config.get('api_retries', 3))
        retry_delay = float(openrouter_config.get('api_retry_delay', 1))
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504], # Retry on these direct HTTP statuses
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(
                api_url,
                headers=headers,
                json=data,
                stream=streaming,
                timeout=int(openrouter_config.get('api_timeout', 60)) # Configurable timeout
            )

            # Check for direct HTTP errors first (after retries)
            response.raise_for_status()

            # --- Handle Response ---
            if streaming:
                logging.debug("OpenRouter: Streaming response received (Status %s).", response.status_code)
                # Check header before starting iteration? OpenRouter usually sends errors before stream starts.
                # Let's assume for now errors come via non-200 or JSON payload before stream starts.
                # If errors *can* appear mid-stream, the generator needs more logic.

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                             if line and line.startswith("data:"): # Check if line is not empty and starts with data:
                                 content = line[len("data:"):].strip()
                                 if content == "[DONE]":
                                     yield f"{line}\n\n" # Pass through DONE marker
                                     break
                                 # Potentially check for error structures within the stream data here if necessary
                                 yield f"{line}\n\n" # Pass through raw SSE line
                             elif line: # Log unexpected lines
                                  logging.warning("OpenRouter Stream: Received unexpected line: %s", line)

                        # Ensure DONE is yielded if loop finishes without seeing it (though unlikely for OpenAI format)
                        # yield "data: [DONE]\n\n" # Might be redundant if always broken above

                    except requests.exceptions.ChunkedEncodingError as chunk_err:
                         logging.error("OpenRouter: Stream connection error: %s", chunk_err, exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream connection error: {str(chunk_err)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    except Exception as e:
                         logging.error("OpenRouter: Error during stream iteration: %s", e, exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    finally:
                         response.close() # Ensure connection is closed

                return stream_generator()
            else:
                # Non-streaming: Get JSON and check for 'error' key INSIDE the payload
                response_data = response.json()

                if isinstance(response_data, dict) and 'error' in response_data:
                     error_info = response_data['error']
                     error_code = error_info.get('code') # Could be HTTP status or OpenRouter specific code
                     error_message = error_info.get('message', 'Unknown OpenRouter error')
                     provider_message = f"OpenRouter reported error: {error_message}"

                     # Attempt to extract more detail
                     raw_detail = error_info.get('metadata', {}).get('raw', '')
                     if raw_detail: provider_message += f" (Raw Detail: {str(raw_detail)[:150]})" # Truncate raw detail

                     logging.error("OpenRouter: API call failed (reported in JSON payload): Code %s, Message: %s", error_code, provider_message)

                     # Map internal code to custom exceptions (best effort)
                     # OpenRouter often uses HTTP status codes here
                     if error_code == 401:
                         raise ChatAuthenticationError(provider='openrouter', message=provider_message)
                     elif error_code == 429:
                         raise ChatRateLimitError(provider='openrouter', message=provider_message)
                     elif error_code and 400 <= error_code < 500:
                          raise ChatBadRequestError(provider='openrouter', message=provider_message)
                     elif error_code and 500 <= error_code < 600:
                          raise ChatProviderError(provider='openrouter', message=provider_message, status_code=error_code)
                     else: # Default to provider error if code is missing or unhandled
                          # Use 502 Bad Gateway as a general "proxy received bad response" status
                          raise ChatProviderError(provider='openrouter', message=provider_message, status_code=502)

                # If no 'error' key, assume success
                logging.debug("OpenRouter: Non-streaming request successful.")
                # logging.debug(f"OpenRouter Raw Response Data: {response_data}")
                return response_data # Return successful dictionary

    except (ValueError, KeyError, TypeError) as e:
         # Catch config/data errors before the request
         logging.error("OpenRouter: Configuration or data error before request: %s", e, exc_info=True)
         # Raise as BadRequest or Configuration error depending on context (can be tricky)
         # Let's map most of these to BadRequest for simplicity during call setup
         raise ChatBadRequestError(provider='openrouter', message=f"Configuration/Data error for OpenRouter: {e}") from e

    # Let HTTPError and RequestException raised by session.post or raise_for_status propagate
    # These will be caught by the generic handlers in chat_api_call


def chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, temp=None, streaming=False, model=None): # Added model
    logging.debug(f"HuggingFace Chat: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    hf_config = loaded_config_data.get('huggingface_api', {})

    # --- API Key ---
    # Use passed key first, then config
    huggingface_api_key = api_key or hf_config.get('api_key')
    if not huggingface_api_key:
        logging.error("HuggingFace: API key is missing.")
        raise ValueError("HuggingFace API Key is required but not found.")
    log_key = f"{huggingface_api_key[:5]}...{huggingface_api_key[-5:]}" if len(huggingface_api_key) > 9 else "Provided Key"
    logging.debug(f"HuggingFace: Using API Key: {log_key}")

    # --- Parameters ---
    if model is None: model = hf_config.get('model') # Model ID is critical
    if not model:
        raise ValueError("HuggingFace model ID is required but not found in config or arguments.")
    if temp is None: temp = float(hf_config.get('temperature', 1.0)) # Default temp 1.0 is common
    if system_prompt is None: system_prompt = "You are a helpful AI assistant." # TGI might ignore this depending on template

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = hf_config.get('streaming', False)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"HuggingFace: Streaming: {streaming}, Model: {model}, Temp: {temp}")

    # --- Prepare Request (TGI OpenAI compatible endpoint) ---
    headers = {
        "Authorization": f"Bearer {huggingface_api_key}",
        'Content-Type': 'application/json',
    }

    # Handle input_data format
    if isinstance(input_data, list):
        messages = input_data
        # TGI usually infers system prompt from template, but we can pass it
        if not any(msg.get('role') == 'system' for msg in messages):
             messages.insert(0, {"role": "system", "content": system_prompt})
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg:
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str):
         messages = [
             {"role": "system", "content": system_prompt},
             {"role": "user", "content": f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for HuggingFace. Expected list of messages or string.")

    data = {
        "model": model, # Often ignored by endpoint if model is in URL, but good practice
        "messages": messages,
        "max_tokens": int(hf_config.get('max_tokens', 1024)), # Add max_tokens if configurable
        "stream": streaming,
        "temperature": temp,
        # Add other TGI supported params like top_p, top_k if needed
    }
    top_p = hf_config.get('top_p')
    top_k = hf_config.get('top_k')
    if top_p is not None: data['top_p'] = float(top_p)
    if top_k is not None: data['top_k'] = int(top_k)


    # Construct URL (Assuming TGI endpoint)
    # NOTE: The base URL might differ for Serverless Inference Endpoints
    api_url = f"https://api-inference.huggingface.co/models/{model}" # Base for inference API
    # For TGI compatible chat endpoint, the path is often appended:
    # api_url = f"https://YOUR-ENDPOINT-URL/v1/chat/completions" # Example if using dedicated endpoint
    # Using the standard inference API URL, will use the recommended task if available
    # Let's assume the /v1/chat/completions path for compatibility testing
    # You might need to adjust this based on EXACTLY which HF API you target
    api_url = f"https://api-inference.huggingface.co/v1/chat/completions" # More likely endpoint for chat

    # --- Execute Request ---
    try:
        retry_count = int(hf_config.get('api_retries', 3))
        retry_delay = float(hf_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER retries

            if streaming:
                logging.debug("HuggingFace: Streaming response received.")
                # Stream format is likely OpenAI compatible SSE
                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                             if line.strip():
                                 yield line + "\n\n" # Pass through raw SSE line
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                         logging.error(f"HuggingFace: Error during stream iteration: {e}", exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    finally:
                         response.close()
                return stream_generator()
            else:
                logging.debug("HuggingFace: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"HuggingFace Raw Response Data: {response_data}")
                # Check if response is list (older Inference API format) or dict (TGI format)
                if isinstance(response_data, list) and len(response_data) > 0 and 'generated_text' in response_data[0]:
                     # Adapt older format to OpenAI-like structure for consistency downstream? Or return as is.
                     # Returning as is for now, endpoint might need to normalize.
                     logging.warning("HuggingFace: Received older Inference API list format.")
                elif not isinstance(response_data, dict):
                     logging.error(f"HuggingFace: Unexpected non-dict response format: {type(response_data)}")
                     # Treat as an error? Or try to process if possible.
                     # Raising error for now.
                     raise ValueError(f"Unexpected response format from HuggingFace API: {type(response_data)}")

                return response_data # Return the full dictionary (or list)

    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"HuggingFace: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"HuggingFace config/data error: {e}") from e
    # Let RequestException and HTTPError propagate


def chat_with_deepseek(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, topp=None, model=None): # Added model
    logging.debug("DeepSeek: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    deepseek_config = loaded_config_data.get('deepseek_api', {})

    # --- API Key ---
    deepseek_api_key = api_key or deepseek_config.get('api_key')
    if not deepseek_api_key:
        logging.error("DeepSeek: API key is missing.")
        raise ValueError("DeepSeek API Key is required but not found.")
    log_key = f"{deepseek_api_key[:5]}...{deepseek_api_key[-5:]}" if len(deepseek_api_key) > 9 else "Provided Key"
    logging.debug(f"DeepSeek: Using API Key: {log_key}")

    # --- Parameters (OpenAI compatible) ---
    if model is None: model = deepseek_config.get('model', 'deepseek-chat') # Allow model override
    if temp is None: temp = float(deepseek_config.get('temperature', 0.1))
    if topp is None: topp = float(deepseek_config.get('top_p', 0.95))
    if system_message is None: system_message = "You are a helpful AI assistant."

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = deepseek_config.get('streaming', False)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"DeepSeek: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {topp}")

    # --- Prepare Request ---
    headers = {
        'Authorization': f'Bearer {deepseek_api_key}',
        'Content-Type': 'application/json'
    }

    # Handle input_data format
    if isinstance(input_data, list):
        messages = input_data
        if not any(msg.get('role') == 'system' for msg in messages):
             messages.insert(0, {"role": "system", "content": system_message})
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg:
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str):
         # Attempt to extract text if input_data might be file path or raw text
         text_content = input_data # Assume raw text initially
         if os.path.isfile(input_data):
              try:
                  # Basic text file reading, adjust if other formats needed
                  with open(input_data, 'r', encoding='utf-8') as f:
                      text_content = f.read()
              except Exception as e:
                  logging.warning(f"DeepSeek: Could not read file '{input_data}', treating as raw text. Error: {e}")
         messages = [
             {"role": "system", "content": system_message},
             {"role": "user", "content": f"{text_content}\n\n{custom_prompt_arg}" if custom_prompt_arg else text_content}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for DeepSeek. Expected list of messages or string.")


    data = {
        "model": model,
        "messages": messages,
        "stream": streaming,
        "temperature": temp,
        "top_p": topp
        # Add other DeepSeek/OpenAI params if needed (e.g., max_tokens)
    }
    max_tokens = deepseek_config.get('max_tokens')
    if max_tokens:
        try: data["max_tokens"] = int(max_tokens)
        except: logging.warning(f"DeepSeek: Invalid max_tokens value in config: {max_tokens}")

    api_url = 'https://api.deepseek.com/chat/completions'

    # --- Execute Request ---
    try:
        retry_count = int(deepseek_config.get('api_retries', 3))
        retry_delay = float(deepseek_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER retries

            if streaming:
                logging.debug("DeepSeek: Streaming response received.")
                # Stream format is OpenAI compatible SSE
                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                             if line.strip():
                                 yield line + "\n\n" # Pass through raw SSE line
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                         logging.error(f"DeepSeek: Error during stream iteration: {e}", exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    finally:
                         response.close()
                return stream_generator()
            else:
                logging.debug("DeepSeek: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"DeepSeek Raw Response Data: {response_data}")
                return response_data # Return full dictionary

    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"DeepSeek: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"DeepSeek config/data error: {e}") from e
    # Let RequestException and HTTPError propagate


def chat_with_mistral(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, topp=None, model=None):
    logging.debug("Mistral: Chat request started")
    loaded_config_data = load_and_log_configs()
    mistral_config = loaded_config_data.get('mistral_api', {})

    # --- API Key ---
    mistral_api_key = api_key or mistral_config.get('api_key')
    if not mistral_api_key:
        logging.error("Mistral: API key is missing.")
        raise ValueError("Mistral API Key is required but not found.")
    log_key = f"{mistral_api_key[:5]}...{mistral_api_key[-5:]}" if len(mistral_api_key) > 9 else "Provided Key"
    logging.debug(f"Mistral: Using API Key: {log_key}")

    # --- Parameters (OpenAI compatible) ---
    if model is None: model = mistral_config.get('model', 'mistral-large-latest')
    if temp is None: temp = float(mistral_config.get('temperature', 0.1))
    if topp is None: topp = float(mistral_config.get('top_p', 0.95))
    # Mistral uses 'safe_prompt' - get from config or default
    safe_prompt = bool(mistral_config.get('safe_prompt', False))
    if system_message is None: system_message = "You are a helpful AI assistant." # System message is part of messages list

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = mistral_config.get('streaming', False)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"Mistral: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {topp}, SafePrompt: {safe_prompt}")

    # --- Prepare Request ---
    headers = {
        'Authorization': f'Bearer {mistral_api_key}',
        'Content-Type': 'application/json',
        'Accept': 'application/json' # Good practice
    }

    # Handle input_data format
    if isinstance(input_data, list):
        messages = input_data
        # Ensure system message is first if provided
        if system_message and not any(msg.get('role') == 'system' for msg in messages):
             messages.insert(0, {"role": "system", "content": system_message})
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg:
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str):
         # Extract text if needed (assuming simple text or list extraction)
         text_content = extract_text_from_segments(input_data) if isinstance(input_data, list) else input_data
         messages = [
             #{"role": "system", "content": system_message}, # Mistral often prefers system in user msg or no system
             {"role": "user", "content": f"{system_message}\n\n{text_content}\n\n{custom_prompt_arg}" if custom_prompt_arg else f"{system_message}\n\n{text_content}"}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for Mistral. Expected list of messages or string.")

    data = {
        "model": model,
        "messages": messages,
        "temperature": temp,
        "top_p": topp,
        "stream": streaming,
        "safe_prompt": safe_prompt
    }
    max_tokens = mistral_config.get('max_tokens')
    if max_tokens:
        try: data["max_tokens"] = int(max_tokens)
        except: logging.warning(f"Mistral: Invalid max_tokens value in config: {max_tokens}")

    api_url = 'https://api.mistral.ai/v1/chat/completions'

    # --- Execute Request ---
    try:
        retry_count = int(mistral_config.get('api_retries', 3))
        retry_delay = float(mistral_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER retries

            if streaming:
                logging.debug("Mistral: Streaming response received.")
                # Stream format is OpenAI compatible SSE
                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                             if line.strip():
                                 yield line + "\n\n" # Pass through raw SSE line
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                         logging.error(f"Mistral: Error during stream iteration: {e}", exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    finally:
                         response.close()
                return stream_generator()
            else:
                logging.debug("Mistral: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"Mistral Raw Response Data: {response_data}")
                return response_data # Return full dictionary

    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"Mistral: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"Mistral config/data error: {e}") from e
    # Let RequestException and HTTPError propagate


def chat_with_google(api_key, input_data, custom_prompt_arg, temp=None, system_message=None, streaming=False, topp=None, topk=None, model=None): # Added model
    logging.debug("Google Gemini: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    google_config = loaded_config_data.get('google_api', {})

    # --- API Key ---
    google_api_key = api_key or google_config.get('api_key')
    if not google_api_key:
        logging.error("Google: API key is missing.")
        raise ValueError("Google API Key is required but not found.")
    log_key = f"{google_api_key[:5]}...{google_api_key[-5:]}" if len(google_api_key) > 9 else "Provided Key"
    logging.debug(f"Google: Using API Key: {log_key}")

    # --- Parameters ---
    if model is None: model = google_config.get('model', 'gemini-1.5-flash-latest')
    if temp is None: temp = float(google_config.get('temperature', 0.7))
    if topp is None: topp = float(google_config.get('top_p', 0.9)) # Gemini uses topP
    if topk is None: topk = int(google_config.get('top_k', 100)) # Gemini uses topK
    # System message handled via 'system_instruction' in Gemini API
    if system_message is None: system_message = google_config.get('system_prompt', "You are a helpful AI assistant.")

    if isinstance(streaming, str): streaming = streaming.lower() == "true"
    elif isinstance(streaming, int): streaming = bool(streaming)
    if streaming is None: streaming = google_config.get('streaming', False)
    if not isinstance(streaming, bool):
        raise ValueError(f"Invalid type for 'streaming': Expected boolean, got {type(streaming).__name__}")

    logging.debug(f"Google: Streaming: {streaming}, Model: {model}, Temp: {temp}, TopP: {topp}, TopK: {topk}")

    # --- Prepare Request (Native Gemini API format) ---
    # Convert OpenAI message format to Gemini 'contents' format
    contents = []
    current_user_parts = []

    # Function to add user parts to contents
    def flush_user_parts():
        nonlocal current_user_parts
        if current_user_parts:
            contents.append({"role": "user", "parts": current_user_parts})
            current_user_parts = []

    if isinstance(input_data, list):
        for msg in input_data:
            role = msg.get('role', '').lower()
            content = msg.get('content', '')
            if role == 'user':
                flush_user_parts() # Add previous assistant message if any
                current_user_parts.append({"text": content})
            elif role == 'assistant' or role == 'model': # Gemini uses 'model' role
                flush_user_parts() # Add user message first
                contents.append({"role": "model", "parts": [{"text": content}]})
            # System messages handled separately
    elif isinstance(input_data, str):
        # Assume input_data is the user prompt if it's just a string
        current_user_parts.append({"text": input_data})
    else:
         raise ValueError("Invalid 'input_data' format for Google. Expected list of messages or string.")

    # Append custom prompt to the last user part
    if custom_prompt_arg:
        if current_user_parts:
            current_user_parts[-1]["text"] += f"\n\n{custom_prompt_arg}"
        else: # If input_data was empty or only assistant messages
            current_user_parts.append({"text": custom_prompt_arg})

    flush_user_parts() # Add the last user message

    generation_config = {
        "temperature": temp,
        "topP": topp,
        "topK": topk,
        # "maxOutputTokens": 4096, # Optional: Get from config if needed
        # "stopSequences": ["\n"] # Optional
    }
    max_tokens_cfg = google_config.get('max_tokens')
    if max_tokens_cfg:
        try: generation_config["maxOutputTokens"] = int(max_tokens_cfg)
        except: logging.warning(f"Google: Invalid max_tokens in config: {max_tokens_cfg}")


    payload = {
        "contents": contents,
        "generationConfig": generation_config,
    }
    # Add system instruction if provided
    if system_message:
        payload["system_instruction"] = {"parts": [{"text": system_message}]}


    # Construct API URL
    stream_suffix = ":streamGenerateContent?alt=sse" if streaming else ":generateContent"
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}{stream_suffix}"
    headers = {
        'x-goog-api-key': google_api_key, # Google uses x-goog-api-key header
        'Content-Type': 'application/json',
    }

     # --- Execute Request ---
    try:
        retry_count = int(google_config.get('api_retries', 3))
        retry_delay = float(google_config.get('api_retry_delay', 1))
        # Note: Google API might return 400 for quota instead of 429 sometimes
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 503], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=payload, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER retries

            if streaming:
                logging.debug("Google: Streaming response received.")
                # Stream format is SSE, but content needs parsing
                def stream_generator():
                    try:
                        full_text_response = ""
                        for line in response.iter_lines(decode_unicode=True):
                             if line.strip().startswith('data:'):
                                 json_str = line.strip()[len('data:'):]
                                 try:
                                     data_chunk = json.loads(json_str)
                                     # Extract text from Gemini stream structure
                                     candidates = data_chunk.get('candidates', [])
                                     chunk_text = ""
                                     if candidates:
                                         content = candidates[0].get('content', {})
                                         parts = content.get('parts', [])
                                         if parts:
                                             chunk_text = parts[0].get('text', '')
                                     if chunk_text:
                                         full_text_response += chunk_text
                                         # Yield raw JSON chunk or just the text?
                                         # Yielding text for simplicity, like OpenAI delta
                                         yield f"data: {json.dumps({'choices': [{'delta': {'content': chunk_text}}]})}\n\n"
                                 except json.JSONDecodeError:
                                     logging.warning(f"Google: Could not decode JSON line: {line}")
                        logging.debug("Google Stream finished.")
                        yield "data: [DONE]\n\n" # Signal end
                    except Exception as e:
                         logging.error(f"Google: Error during stream iteration: {e}", exc_info=True)
                         error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                         yield f"data: {error_payload}\n\n"
                         yield "data: [DONE]\n\n"
                    finally:
                         response.close()
                return stream_generator()
            else:
                logging.debug("Google: Non-streaming request successful.")
                response_data = response.json()
                #logging.debug(f"Google Raw Response Data: {response_data}")
                # Need to normalize Gemini response to OpenAI-like structure for consistency?
                # Or return the raw Gemini response. Returning raw for now.
                return response_data # Return full dictionary

    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"Google: Configuration or data error: {e}", exc_info=True)
         raise ValueError(f"Google config/data error: {e}") from e
    # Let RequestException and HTTPError propagate

#
#
#######################################################################################################################