# LLM_API_Calls.py
#########################################
# General LLM API Calling Library
# This library is used to perform API Calls against commercial LLM endpoints.
#
####
####################
# Function List
#
# 1. extract_text_from_segments(segments: List[Dict]) -> str
# 2. chat_with_openai(api_key, file_path, custom_prompt_arg, streaming=None)
# 3. chat_with_anthropic(api_key, file_path, model, custom_prompt_arg, max_retries=3, retry_delay=5, streaming=None)
# 4. chat_with_cohere(api_key, file_path, model, custom_prompt_arg, streaming=None)
# 5. chat_with_groq(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None):
# 6. chat_with_openrouter(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 7. chat_with_huggingface(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
# 8. chat_with_deepseek(api_key, input_data, custom_prompt_arg, system_prompt=None, streaming=None)
#
#
####################
#
# Import necessary libraries
import json
import os
import time
from typing import List, Any, Optional, Tuple, Dict
#
# Import 3rd-Party Libraries
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from tldw_Server_API.app.core.Chat.Chat_Functions import ChatAuthenticationError, ChatRateLimitError, \
    ChatBadRequestError, ChatProviderError, ChatConfigurationError
#
# Import Local libraries
from tldw_Server_API.app.core.Utils.Utils import load_and_log_configs, logging
#
#######################################################################################################################
# Function Definitions
#

# FIXME: Update to include full arguments

# --- Helper function for safe type conversion ---
def _safe_cast(value: Any, cast_to: type, default: Any = None) -> Any:
    """Safely casts value to specified type, returning default on failure."""
    if value is None:
        return default
    try:
        return cast_to(value)
    except (ValueError, TypeError):
        logging.warning(f"Could not cast '{value}' to {cast_to}. Using default: {default}")
        return default

def extract_text_from_segments(segments):
    logging.debug(f"Segments received: {segments}")
    logging.debug(f"Type of segments: {type(segments)}")

    text = ""

    if isinstance(segments, list):
        for segment in segments:
            logging.debug(f"Current segment: {segment}")
            logging.debug(f"Type of segment: {type(segment)}")
            if 'Text' in segment:
                text += segment['Text'] + " "
            else:
                logging.warning(f"Skipping segment due to missing 'Text' key: {segment}")
    else:
        logging.warning(f"Unexpected type of 'segments': {type(segments)}")

    return text.strip()


def _parse_data_url_for_multimodal(data_url: str) -> Optional[Tuple[str, str]]:
    """Parses a data URL (e.g., data:image/png;base64,xxxx) into (mime_type, base64_data)."""
    if data_url.startswith("data:") and ";base64," in data_url:
        try:
            header, b64_data = data_url.split(";base64,", 1)
            mime_type = header.split("data:", 1)[1]
            return mime_type, b64_data
        except Exception as e:
            logging.warning(f"Could not parse data URL: {data_url[:60]}... Error: {e}")
            return None
    logging.debug(f"Data URL did not match expected format: {data_url[:60]}...")
    return None


def get_openai_embeddings(input_data: str, model: str) -> List[float]:
    """
    Get embeddings for the input text from OpenAI API.
    Args:
        input_data (str): The input text to get embeddings for.
        model (str): The model to use for generating embeddings.
    Returns:
        List[float]: The embeddings generated by the API.
    """
    loaded_config_data = load_and_log_configs()
    api_key = loaded_config_data['openai_api']['api_key']

    if not api_key:
        logging.error("OpenAI Embeddings: API key not found or is empty")
        raise ValueError("OpenAI Embeddings: API Key Not Provided/Found in Config file or is empty")

    logging.debug(f"OpenAI Embeddings: Using API Key: {api_key[:5]}...{api_key[-5:]}")
    logging.debug(f"OpenAI Embeddings: Raw input data (first 500 chars): {str(input_data)[:500]}...")
    logging.debug(f"OpenAI Embeddings: Using model: {model}")

    headers = {
        'Authorization': f'Bearer {api_key}',
        'Content-Type': 'application/json'
    }
    request_data = {
        "input": input_data,
        "model": model,
    }
    try:
        logging.debug("OpenAI Embeddings: Posting request to embeddings API")
        response = requests.post('https://api.openai.com/v1/embeddings', headers=headers, json=request_data)
        logging.debug(f"Full API response data: {response}")
        if response.status_code == 200:
            response_data = response.json()
            if 'data' in response_data and len(response_data['data']) > 0:
                embedding = response_data['data'][0]['embedding']
                logging.debug("OpenAI Embeddings: Embeddings retrieved successfully")
                return embedding
            else:
                logging.warning("OpenAI Embeddings: Embedding data not found in the response")
                raise ValueError("OpenAI Embeddings: Embedding data not available in the response")
        else:
            logging.error(f"OpenAI Embeddings: request failed with status code {response.status_code}")
            logging.error(f"OpenAI Embeddings: Error response: {response.text}")
            # Propagate HTTPError to be caught by chat_api_call's handler (if this were called from there)
            # Or raise specific error if called directly
            response.raise_for_status() # This will raise HTTPError
            # Fallback if raise_for_status doesn't cover it (it should)
            raise ValueError(f"OpenAI Embeddings: Failed to retrieve. Status code: {response.status_code}")
    except requests.RequestException as e:
        logging.error(f"OpenAI Embeddings: Error making API request: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Error making API request: {str(e)}")
    except Exception as e:
        logging.error(f"OpenAI Embeddings: Unexpected error: {str(e)}", exc_info=True)
        raise ValueError(f"OpenAI Embeddings: Unexpected error occurred: {str(e)}")


def chat_with_openai(
    model: Optional[str],                 # Mapped from 'model'
    input_data: List[Dict[str, Any]],     # Mapped from 'input_data', this is messages_payload
    api_key: Optional[str] = None,        # Mapped from 'api_key'
    custom_prompt_arg: Optional[str] = None, # Mapped from 'prompt', largely ignored now
    temp: Optional[float] = None,         # Mapped from 'temp'
    system_message: Optional[str] = None, # Mapped from 'system_message'
    streaming: Optional[bool] = None,     # Mapped from 'streaming'
    maxp: Optional[float] = None          # Mapped from 'maxp' (becomes top_p)
):
    loaded_config_data = load_and_log_configs()
    openai_config = loaded_config_data.get('openai_api', {})

    openai_api_key = api_key or openai_config.get('api_key')
    if not openai_api_key:
        logging.error("OpenAI: API key is missing.")
        raise ChatConfigurationError(provider="openai", message="OpenAI API Key is required but not found.")

    log_key = f"{openai_api_key[:5]}...{openai_api_key[-5:]}" if openai_api_key and len(openai_api_key) > 9 else "Provided Key"
    logging.debug(f"OpenAI: Using API Key: {log_key}")

    current_model = model or openai_config.get('model', 'gpt-4o-mini')
    current_temp = temp if temp is not None else float(openai_config.get('temperature', 0.7))
    current_top_p = maxp if maxp is not None else float(openai_config.get('top_p', 0.95)) # 'maxp' maps to 'top_p'
    current_streaming = streaming if streaming is not None else openai_config.get('streaming', False)
    current_max_tokens = int(openai_config.get('max_tokens', 4096)) # OpenAI uses 'max_tokens'

    if isinstance(current_streaming, str): current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int): current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="openai", message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(f"OpenAI: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}")
    if custom_prompt_arg:
        logging.warning("OpenAI: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is the messages_payload, which can contain multimodal content
    api_messages.extend(input_data)

    # --- Prepare Request ---
    headers = {
        'Authorization': f'Bearer {openai_api_key}',
        'Content-Type': 'application/json'
    }

    # Handle input_data format (assuming it's typically message list)
    if isinstance(input_data, list):
        messages = input_data # Assume it's already in OpenAI message format
        # Inject system message if not already present
        if not any(msg.get('role') == 'system' for msg in messages):
            messages.insert(0, {"role": "system", "content": system_message})
        # Append custom_prompt_arg to the last user message if provided
        if custom_prompt_arg and messages and messages[-1].get('role') == 'user':
             messages[-1]['content'] = f"{messages[-1]['content']}\n\n{custom_prompt_arg}"
        elif custom_prompt_arg: # If no user message or only system message
             messages.append({"role": "user", "content": custom_prompt_arg})
    elif isinstance(input_data, str): # If only a string is passed
         messages = [
             {"role": "system", "content": system_message},
             {"role": "user", "content": f"{input_data}\n\n{custom_prompt_arg}" if custom_prompt_arg else input_data}
         ]
    else:
        raise ValueError("Invalid 'input_data' format for OpenAI. Expected list of messages or string.")

    data = {
        "model": current_model,
        "messages": api_messages,
        "max_tokens": current_max_tokens,
        "temperature": current_temp,
        "stream": current_streaming,
        "top_p": current_top_p
    }

    # --- Execute Request ---
    api_url = 'https://api.openai.com/v1/chat/completions'
    try:
        if current_streaming:
            logging.debug("OpenAI: Posting request (streaming)")
            # Use context manager for session
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180) # Increased timeout for streaming
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip(): # Ensure line is not empty
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"OpenAI: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps({"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"OpenAI: Error during stream iteration: {e}", exc_info=True)
                        # Yield an error chunk if possible, then DONE
                        error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()
                return stream_generator()

        else:
            logging.debug("OpenAI: Posting request (non-streaming)")
            # Configure retry strategy
            retry_count = int(openai_config.get('api_retries', 3))
            retry_delay = float(openai_config.get('api_retry_delay', 1))
            retry_strategy = Retry(
                total=retry_count,
                backoff_factor=retry_delay,
                status_forcelist=[429, 500, 502, 503, 504], # Retry on these statuses
                allowed_methods=["POST"] # Important: Allow retries for POST
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            # Use context manager for session
            with requests.Session() as session:
                session.mount("https://", adapter)
                session.mount("http://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=30) # Add timeout

            logging.debug(f"Full API response status: {response.status_code}")
            response.raise_for_status() # Raise HTTPError for 4xx/5xx AFTER retries

            response_data = response.json() # Parse JSON on success
            logging.debug("OpenAI: Non-streaming request successful.")
            #logging.debug(f"OpenAI Raw Response Data: {response_data}") # Optional: Log raw response
            return response_data # Return the full dictionary

    # Let RequestException and HTTPError propagate up to chat_api_call
    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"OpenAI: Configuration or data error: {e}", exc_info=True)
         # Re-raise as ValueError which chat_api_call can map to BadRequest/Config error
         raise ValueError(f"OpenAI config/data error: {e}") from e


def chat_with_anthropic(
    model: Optional[str],                 # Mapped from 'model'
    input_data: List[Dict[str, Any]],     # Mapped from 'input_data' (messages_payload)
    api_key: Optional[str] = None,        # Mapped from 'api_key'
    custom_prompt_arg: Optional[str] = None, # Mapped from 'prompt', ignored
    temp: Optional[float] = None,         # Mapped from 'temp'
    system_prompt: Optional[str] = None,  # Mapped from 'system_message'
    streaming: Optional[bool] = None,     # Mapped from 'streaming'
    topp: Optional[float] = None,         # Mapped from 'topp' (becomes top_p)
    topk: Optional[int] = None            # Mapped from 'topk'
):
    loaded_config_data = load_and_log_configs()
    anthropic_config = loaded_config_data.get('anthropic_api', {})

    # --- API Key Resolution ---
    anthropic_api_key = api_key or anthropic_config.get('api_key')

    if not anthropic_api_key:
        logging.error("Anthropic: API key is missing.")
        raise ChatConfigurationError(provider="anthropic", message="Anthropic API Key is required but not found.")

    log_key = f"{anthropic_api_key[:5]}...{anthropic_api_key[-5:]}" if anthropic_api_key and len(anthropic_api_key) > 9 else "Provided Key"
    logging.debug(f"Anthropic: Using API Key: {log_key}")

    current_model = model or anthropic_config.get('model', 'claude-3-opus-20240229') # Opus for better multimodal
    current_temp = temp if temp is not None else float(anthropic_config.get('temperature', 0.7))
    current_top_p = topp # 'topp' from chat_api_call maps to Anthropic's 'top_p'
    current_top_k = topk # 'topk' from chat_api_call maps to Anthropic's 'top_k'
    current_streaming = streaming if streaming is not None else anthropic_config.get('streaming', False)
    current_system_prompt = system_prompt # Use the mapped 'system_prompt'
    max_tokens = int(anthropic_config.get('max_tokens_to_sample', 4096)) # Anthropic uses max_tokens_to_sample or max_tokens

    if isinstance(current_streaming, str): current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int): current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="anthropic", message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(f"Anthropic: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, TopK: {current_top_k}")
    if custom_prompt_arg:
        logging.warning("Anthropic: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    # Transform OpenAI messages_payload to Anthropic's format
    anthropic_messages = []
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content") # Can be string or list of parts

        if role not in ["user", "assistant"]:
            logging.warning(f"Anthropic: Skipping message with unsupported role: {role}")
            continue

        anthropic_content_parts = []
        if isinstance(content, str):
            anthropic_content_parts.append({"type": "text", "text": content})
        elif isinstance(content, list):
            for part in content:
                part_type = part.get("type")
                if part_type == "text":
                    anthropic_content_parts.append({"type": "text", "text": part.get("text", "")})
                elif part_type == "image_url":
                    image_url_data = part.get("image_url", {}).get("url", "")
                    parsed_image = _parse_data_url_for_multimodal(image_url_data)
                    if parsed_image:
                        mime_type, b64_data = parsed_image
                        anthropic_content_parts.append({
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": mime_type,
                                "data": b64_data
                            }
                        })
                    else:
                        logging.warning(f"Anthropic: Could not parse image_url, skipping image: {image_url_data[:60]}...")
                        # Optionally add a placeholder text for the unparsed image
                        # anthropic_content_parts.append({"type": "text", "text": "[Image data unparseable or not provided]"})

        if anthropic_content_parts:
             anthropic_messages.append({"role": role, "content": anthropic_content_parts})
        elif role == "user" and not anthropic_content_parts: # User message must have content
             logging.warning(f"Anthropic: User message has no processable content parts. Adding placeholder.")
             anthropic_messages.append({"role": role, "content": [{"type": "text", "text": "(User input was empty or only contained unparseable images)"}]})


    if not any(m['role'] == 'user' for m in anthropic_messages):
        raise ChatBadRequestError(provider="anthropic", message="No valid user messages found after processing for Anthropic.")

    headers = {
        'x-api-key': anthropic_api_key,
        'anthropic-version': '2023-06-01',
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "max_tokens": max_tokens, # Use max_tokens (new name)
        "messages": anthropic_messages,
        "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if current_system_prompt: data["system"] = current_system_prompt # Anthropic's 'system' parameter


    # --- Execute Request ---
    api_url = 'https://api.anthropic.com/v1/messages'
    try:
        # Configure retry strategy
        retry_count = int(anthropic_config.get('api_retries', 3))
        retry_delay = float(anthropic_config.get('api_retry_delay', 1))
        retry_strategy = Retry(
            total=retry_count,
            backoff_factor=retry_delay,
            status_forcelist=[429, 500, 502, 503, 504], # Retry on these statuses
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)

        with requests.Session() as session:
            session.mount("https://", adapter)
            session.mount("http://", adapter)
            response = session.post(api_url, headers=headers, json=data, stream=streaming, timeout=60)

            response.raise_for_status() # Check status AFTER potential retries

        if current_streaming:
            logging.debug("Anthropic: Streaming response received.")
            def stream_generator():
                try:
                    for line in response.iter_lines(decode_unicode=True):
                        if line and line.strip():
                            yield line + "\n\n" # Anthropic SSE format is lines like event: message_delta\ndata: {...}
                    # Anthropic stream ends when connection closes for message_stop event or error.
                    # Sending [DONE] is good for client compatibility.
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:
                    logging.error(f"Anthropic: ChunkedEncodingError during stream: {e}", exc_info=True)
                    error_payload = json.dumps({"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logging.error(f"Anthropic: Error during stream iteration: {e}", exc_info=True)
                    error_payload = json.dumps({"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                finally:
                    response.close()
            return stream_generator()
        else:
            logging.debug("Anthropic: Non-streaming request successful.")
            response_data = response.json()
            return response_data
    except (ValueError, KeyError, TypeError) as e:
         logging.error(f"Anthropic: Configuration or data error: {e}", exc_info=True)
         raise ChatBadRequestError(provider="anthropic", message=f"Anthropic config/data error: {e}") from e


def chat_with_cohere(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_prompt: Optional[str] = None,  # Mapped from 'system_message' (becomes preamble)
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None,  # Mapped from 'topp' (becomes 'p')
        topk: Optional[int] = None  # Mapped from 'topk' (becomes 'k')
):
    loaded_config_data = load_and_log_configs()
    cohere_config = loaded_config_data.get('cohere_api', {})

    cohere_api_key = api_key or cohere_config.get('api_key')
    if not cohere_api_key:
        logging.error("Cohere: API key is missing.")
        raise ChatConfigurationError(provider="cohere", message="Cohere API Key is required but not found.")

    log_key = f"{cohere_api_key[:3]}...{cohere_api_key[-3:]}" if cohere_api_key and len(
        cohere_api_key) > 5 else "Provided Key"
    logging.debug(f"Cohere: Using API Key: {log_key}")

    current_model = model or cohere_config.get('model', 'command-r')
    current_temp = temp if temp is not None else float(cohere_config.get('temperature', 0.3))
    current_p = topp  # 'topp' from chat_api_call maps to Cohere's 'p'
    current_k = topk  # 'topk' from chat_api_call maps to Cohere's 'k'
    current_streaming = streaming if streaming is not None else cohere_config.get('streaming', False)
    current_preamble = system_prompt  # 'system_prompt' from chat_api_call maps to Cohere's 'preamble'

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="cohere",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Cohere: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, P: {current_p}, K: {current_k}")
    if custom_prompt_arg:
        logging.warning(
            "Cohere: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    # Transform OpenAI messages_payload to Cohere's format
    # Cohere wants current message separate from history.
    # For now, Cohere chat will be text-only from this adapter. Multimodal for Cohere usually involves 'connectors' or document inputs.
    chat_history = []
    user_message_text = ""

    processed_messages_count = 0
    for i, msg in enumerate(input_data):
        role = msg.get("role")
        content = msg.get("content")

        # Extract text from content (string or list of parts)
        current_msg_text_parts = []
        if isinstance(content, str):
            current_msg_text_parts.append(content)
        elif isinstance(content, list):
            has_image = False
            for part in content:
                if part.get("type") == "text":
                    current_msg_text_parts.append(part.get("text", ""))
                elif part.get("type") == "image_url":
                    has_image = True
            if has_image:
                logging.warning(
                    f"Cohere: Message part {i} contained an image, which will be ignored for Cohere chat API via this adapter. Only text parts are used.")

        full_text_for_msg = "\n".join(current_msg_text_parts).strip()

        if not full_text_for_msg and role == "user":  # User message must have text
            if i == len(input_data) - 1:  # If it's the last (current) user message
                full_text_for_msg = "(User input was empty or only contained images)"
                logging.warning("Cohere: Current user message is empty after filtering for text. Sending placeholder.")
            else:  # Empty historical message
                logging.warning(
                    f"Cohere: Historical message at index {i} (role: {role}) is empty after filtering for text. Skipping.")
                continue

        if i == len(input_data) - 1 and role == "user":  # Last message, if user, is the current 'message'
            user_message_text = full_text_for_msg
        elif role == "user":
            chat_history.append({"role": "USER", "message": full_text_for_msg})
        elif role == "assistant":
            chat_history.append({"role": "CHATBOT", "message": full_text_for_msg})
        else:
            logging.warning(f"Cohere: Skipping message with unsupported role for history: {role}")

        processed_messages_count += 1

    if not user_message_text and processed_messages_count > 0:  # If the last message was not a user message or was empty
        # This case should be rare if chat() always adds a user message.
        # If input_data ends with assistant message, what is the current "user_message"?
        # Cohere API requires a "message" parameter.
        logging.warning(
            "Cohere: No current user message found at the end of input_data. Using placeholder or last history user message if any.")
        # Fallback: if chat history has a user message, could re-use last one? This is complex.
        # For now, if last message isn't user, this will likely fail Cohere's validation unless history itself is empty
        # and we send a dummy user_message_text.
        if not chat_history and not user_message_text:  # Absolutely no input
            user_message_text = "(No input provided)"

    headers = {
        'accept': 'application/json',
        'content-type': 'application/json',
        'Authorization': f'Bearer {cohere_api_key}'
    }
    data = {
        "model": current_model,
        "message": user_message_text,  # Current user turn
        # "stream": current_streaming, # Cohere uses a query param for streaming in /v1/chat
        # Parameters for generation:
        "temperature": current_temp,
    }
    if chat_history: data["chat_history"] = chat_history
    if current_preamble: data["preamble"] = current_preamble  # System prompt
    if current_p is not None: data["p"] = current_p
    if current_k is not None: data["k"] = current_k

    # Connectors for RAG (example, if you want to extend)
    # connectors = [{"id": "web-search"}]
    # data["connectors"] = connectors

    api_url = 'https://api.cohere.com/v1/chat'
    stream_param_suffix = "?stream=true" if current_streaming else ""

    try:
        retry_count = int(cohere_config.get('api_retries', 3))
        retry_delay = float(cohere_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                               status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url + stream_param_suffix, headers=headers, json=data, stream=current_streaming,
                                    timeout=180)

        response.raise_for_status()

        if current_streaming:
            logging.debug("Cohere: Streaming response received.")

            def stream_generator():
                buffer = ""
                try:
                    # Cohere's stream events are newline-separated JSON objects.
                    for chunk in response.iter_content(chunk_size=None, decode_unicode=True):  # Iterate over raw chunks
                        buffer += chunk
                        while '\n' in buffer:
                            line, buffer = buffer.split('\n', 1)
                            if line.strip():
                                try:
                                    event = json.loads(line)
                                    # Re-wrap into OpenAI-like SSE for consistency if possible
                                    # Cohere event_types: "stream-start", "text-generation", "stream-end", "search-queries-generation", "search-results", "tool-calls-generation", "tool-calls-chunk", "tool-calls"
                                    event_type = event.get("event_type")
                                    is_finished = event.get("is_finished", False)

                                    if event_type == "text-generation" and event.get("text"):
                                        delta_content = event.get("text")
                                        yield f"data: {json.dumps({'choices': [{'delta': {'content': delta_content}}]})}\n\n"
                                    elif event_type == "stream-end" or is_finished:
                                        # Extract final response if present
                                        # final_response_data = event.get("response")
                                        # if final_response_data and final_response_data.get("text"):
                                        #     yield f"data: {json.dumps({'choices': [{'message': {'role': 'assistant', 'content': final_response_data.get('text')}}]})}\n\n"
                                        logging.debug(f"Cohere stream ended with event: {event_type}")
                                        break  # Exit inner loop on stream-end
                                    # else: # Pass through other event types as-is for now
                                    # yield f"data: {line}\n\n"
                                except json.JSONDecodeError:
                                    logging.warning(f"Cohere: Could not decode JSON line from stream: {line}")
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:
                    logging.error(f"Cohere: ChunkedEncodingError during stream: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logging.error(f"Cohere: Error during stream iteration: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                finally:
                    response.close()

            return stream_generator()
        else:
            logging.debug("Cohere: Non-streaming request successful.")
            response_data = response.json()
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Cohere: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="cohere", message=f"Cohere config/data error: {e}") from e


# https://console.groq.com/docs/quickstart
def chat_with_groq(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        maxp: Optional[float] = None  # Mapped from 'maxp' (becomes top_p)
):
    logging.debug("Groq: Chat process starting...")
    loaded_config_data = load_and_log_configs()
    groq_config = loaded_config_data.get('groq_api', {})

    groq_api_key = api_key or groq_config.get('api_key')
    if not groq_api_key:
        logging.error("Groq: API key is missing.")
        raise ChatConfigurationError(provider="groq", message="Groq API Key is required but not found.")

    log_key = f"{groq_api_key[:5]}...{groq_api_key[-5:]}" if groq_api_key and len(groq_api_key) > 9 else "Provided Key"
    logging.debug(f"Groq: Using API Key: {log_key}")

    current_model = model or groq_config.get('model', 'llama3-8b-8192')
    current_temp = temp if temp is not None else float(groq_config.get('temperature', 0.2))
    current_top_p = maxp if maxp is not None else float(groq_config.get('top_p', 0.9))  # 'maxp' maps to 'top_p'
    current_streaming = streaming if streaming is not None else groq_config.get('streaming', False)
    current_max_tokens = _safe_cast(groq_config.get('max_tokens'), int)  # Groq uses 'max_tokens'

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="groq",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Groq: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}")
    if custom_prompt_arg:
        logging.warning(
            "Groq: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. Groq is OpenAI compatible, supports multimodal if model does.
    api_messages.extend(input_data)

    headers = {
        'Authorization': f'Bearer {groq_api_key}',
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "temperature": current_temp,
        "stream": current_streaming,
        "top_p": current_top_p,
    }
    if current_max_tokens is not None:
        data["max_tokens"] = current_max_tokens

    api_url = 'https://api.groq.com/openai/v1/chat/completions'
    try:
        if current_streaming:
            logging.debug("Groq: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"Groq: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"Groq: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("Groq: Posting request (non-streaming)")
            retry_count = int(groq_config.get('api_retries', 3))
            retry_delay = float(groq_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("Groq: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Groq: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="groq", message=f"Groq config/data error: {e}") from e


def chat_with_openrouter(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        top_p: Optional[float] = None,  # Mapped from 'topp' (OpenRouter uses 'top_p')
        top_k: Optional[int] = None,  # Mapped from 'topk' (OpenRouter uses 'top_k')
        minp: Optional[float] = None  # Mapped from 'minp' (OpenRouter uses 'min_p')
):
    logging.info("OpenRouter: Chat request started.")
    loaded_config_data = load_and_log_configs()
    openrouter_config = loaded_config_data.get('openrouter_api', {})

    openrouter_api_key = api_key or openrouter_config.get('api_key')
    if not openrouter_api_key:
        logging.error("OpenRouter: API key is missing.")
        raise ChatConfigurationError(provider='openrouter',
                                     message="OpenRouter API Key is required but not configured.")

    log_key = f"{openrouter_api_key[:5]}...{openrouter_api_key[-5:]}" if openrouter_api_key and len(
        openrouter_api_key) > 9 else "Provided Key"
    logging.debug(f"OpenRouter: Using API Key: {log_key}")

    current_model = model or openrouter_config.get('model', 'mistralai/mistral-7b-instruct:free')
    current_temp = temp if temp is not None else float(openrouter_config.get('temperature', 0.7))
    current_top_p = top_p if top_p is not None else float(openrouter_config.get('top_p', 0.95))
    current_top_k = top_k if top_k is not None else _safe_cast(openrouter_config.get('top_k'), int)
    current_min_p = minp  # 'minp' from chat_api_call maps to OpenRouter's 'min_p'
    current_streaming = streaming if streaming is not None else openrouter_config.get('streaming', False)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider='openrouter',
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"OpenRouter: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, TopK: {current_top_k}, MinP: {current_min_p}")
    if custom_prompt_arg:
        logging.warning(
            "OpenRouter: 'custom_prompt_arg' was provided but is generally ignored as prompts are expected to be in 'input_data' (messages_payload).")

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. OpenRouter is OpenAI compatible, supports multimodal if model does.
    api_messages.extend(input_data)

    if not any(msg.get('role') == 'user' for msg in api_messages):
        raise ChatBadRequestError(provider='openrouter', message="No user message found in the request for OpenRouter.")

    headers = {
        "Authorization": f"Bearer {openrouter_api_key}",
        "Content-Type": "application/json",
        "HTTP-Referer": openrouter_config.get("site_url", "http://localhost"),
        "X-Title": openrouter_config.get("site_name", "TLDW-API"),
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if current_min_p is not None: data["min_p"] = current_min_p  # OpenRouter uses 'min_p'

    api_url = "https://openrouter.ai/api/v1/chat/completions"
    try:
        if current_streaming:
            logging.debug("OpenRouter: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True,
                                        timeout=int(openrouter_config.get('api_timeout', 180)))
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"OpenRouter: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"OpenRouter: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("OpenRouter: Posting request (non-streaming)")
            retry_count = int(openrouter_config.get('api_retries', 3))
            retry_delay = float(openrouter_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data,
                                        timeout=int(openrouter_config.get('api_timeout', 120)))
            response.raise_for_status()  # Check HTTP status first

            response_data = response.json()
            if isinstance(response_data, dict) and 'error' in response_data:  # Check for errors in JSON payload
                error_info = response_data['error']
                error_message = error_info.get('message', 'Unknown OpenRouter error')
                logging.error(f"OpenRouter: API call failed (reported in JSON): {error_message}")
                # Raise a specific error type based on OpenRouter's error structure if possible
                # For now, generic ChatProviderError
                status_code_in_error = _safe_cast(error_info.get('code'), int, 500)
                if status_code_in_error == 401:
                    raise ChatAuthenticationError(provider='openrouter', message=error_message)
                elif status_code_in_error == 429:
                    raise ChatRateLimitError(provider='openrouter', message=error_message)
                elif 400 <= status_code_in_error < 500:
                    raise ChatBadRequestError(provider='openrouter', message=error_message)
                else:
                    raise ChatProviderError(provider='openrouter', message=error_message,
                                            status_code=status_code_in_error)

            logging.debug("OpenRouter: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"OpenRouter: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider='openrouter', message=f"OpenRouter config/data error: {e}") from e


def chat_with_huggingface(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_prompt: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None  # Mapped from 'streaming'
):
    logging.debug(f"HuggingFace Chat: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    hf_config = loaded_config_data.get('huggingface_api', {})

    hf_api_key = api_key or hf_config.get('api_key')
    if not hf_api_key:
        logging.error("HuggingFace: API key is missing.")
        raise ChatConfigurationError(provider="huggingface", message="HuggingFace API Key is required but not found.")

    log_key = f"{hf_api_key[:5]}...{hf_api_key[-5:]}" if hf_api_key and len(hf_api_key) > 9 else "Provided Key"
    logging.debug(f"HuggingFace: Using API Key: {log_key}")

    # Model ID is critical. It can be part of the API URL for serverless or a param for TGI-like.
    # The PROVIDER_PARAM_MAP passes 'model' so we use it.
    current_model_id = model or hf_config.get('model_id')  # Use 'model_id' from config as it's more specific
    if not current_model_id:
        raise ChatConfigurationError(provider="huggingface", message="HuggingFace model ID is required but not found.")

    current_temp = temp if temp is not None else float(
        hf_config.get('temperature', 0.7))  # Default from common practice
    current_streaming = streaming if streaming is not None else hf_config.get('streaming', False)
    # TGI specific params from config if needed
    current_top_p = _safe_cast(hf_config.get('top_p'), float)
    current_top_k = _safe_cast(hf_config.get('top_k'), int)
    current_max_tokens = _safe_cast(hf_config.get('max_new_tokens', 1024), int)  # TGI uses max_new_tokens

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="huggingface",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(f"HuggingFace: Model ID: {current_model_id}, Streaming: {current_streaming}, Temp: {current_temp}")
    if custom_prompt_arg:
        logging.warning("HuggingFace: 'custom_prompt_arg' was provided but is generally ignored.")

    api_messages = []
    # system_prompt here maps to generic system_message
    if system_prompt:  # This is the mapped system_message
        api_messages.append({"role": "system", "content": system_prompt})

    # input_data is messages_payload. Assumes TGI OpenAI compatibility for multimodal.
    api_messages.extend(input_data)

    headers = {
        "Authorization": f"Bearer {hf_api_key}",
        'Content-Type': 'application/json',
    }
    data = {
        "model": current_model_id,  # Model ID for TGI /v1/chat/completions endpoint
        "messages": api_messages,
        "stream": current_streaming,
    }
    if current_temp is not None: data["temperature"] = current_temp
    if current_top_p is not None: data["top_p"] = current_top_p
    if current_top_k is not None: data["top_k"] = current_top_k
    if current_max_tokens is not None: data["max_tokens"] = current_max_tokens  # OpenAI param name for TGI

    # Determine API URL. Could be Inference API or a dedicated TGI endpoint.
    # Chat_Functions.py maps 'huggingface' to chat_with_huggingface. Assume TGI-like /v1/chat/completions.
    # If it's a custom endpoint, it should be in config.
    base_api_url = hf_config.get('api_base_url', 'https://api-inference.huggingface.co')  # Default to general inference

    # Check if using standard inference or a TGI-like chat completions path
    # If api_base_url already includes /v1/chat/completions or similar, use it.
    # Otherwise, decide based on model_id or other config.
    # For simplicity, if 'api_chat_path' is in config, append it. Default to /v1/chat/completions if not.
    chat_path = hf_config.get('api_chat_path', '/v1/chat/completions')
    if base_api_url.endswith('/'): base_api_url = base_api_url[:-1]
    if chat_path.startswith('/'): chat_path = chat_path[1:]

    # If the model_id is a full URL itself (for serverless inference endpoints), use that.
    if current_model_id.startswith("https://"):
        api_url = current_model_id  # Model ID is the endpoint URL
        # For serverless, the payload might be different (e.g. "inputs": formatted_prompt)
        # This needs careful configuration. Assuming /v1/chat/completions style for now.
        # If it's a raw inference endpoint, structure is different:
        # data = {"inputs": "formatted_prompt_string", "parameters": {...}, "stream": ...}
        # This function is for CHAT, so it should target a chat-completions like endpoint.
        logging.warning(
            "HuggingFace: Model ID appears to be a full URL. Ensure it points to an OpenAI-compatible chat completions endpoint.")
        api_url = f"{base_api_url}/{chat_path}"  # Fallback for now, this might need adjustment based on exact HF setup
    else:  # model_id is just an ID like 'mistralai/Mistral-7B-v0.1'
        # For the general inference API, the path is /models/{model_id} for text-generation
        # but for /v1/chat/completions, the model is a parameter in the body.
        api_url = f"{base_api_url}/{chat_path}"

    logging.debug(f"HuggingFace: API URL: {api_url}")

    try:
        if current_streaming:
            logging.debug("HuggingFace: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"HuggingFace: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"HuggingFace: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("HuggingFace: Posting request (non-streaming)")
            retry_count = int(hf_config.get('api_retries', 3))
            retry_delay = float(hf_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("HuggingFace: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"HuggingFace: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="huggingface", message=f"HuggingFace config/data error: {e}") from e


def chat_with_deepseek(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None  # Mapped from 'topp' (becomes top_p)
):
    logging.debug("DeepSeek: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    deepseek_config = loaded_config_data.get('deepseek_api', {})

    deepseek_api_key = api_key or deepseek_config.get('api_key')
    if not deepseek_api_key:
        logging.error("DeepSeek: API key is missing.")
        raise ChatConfigurationError(provider="deepseek", message="DeepSeek API Key is required but not found.")

    log_key = f"{deepseek_api_key[:5]}...{deepseek_api_key[-5:]}" if deepseek_api_key and len(
        deepseek_api_key) > 9 else "Provided Key"
    logging.debug(f"DeepSeek: Using API Key: {log_key}")

    current_model = model or deepseek_config.get('model', 'deepseek-chat')
    current_temp = temp if temp is not None else float(
        deepseek_config.get('temperature', 0.1))  # DeepSeek often uses low temp
    current_top_p = topp if topp is not None else float(deepseek_config.get('top_p', 0.95))
    current_streaming = streaming if streaming is not None else deepseek_config.get('streaming', False)
    current_max_tokens = _safe_cast(deepseek_config.get('max_tokens'), int)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="deepseek",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"DeepSeek: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}")
    if custom_prompt_arg:
        logging.warning("DeepSeek: 'custom_prompt_arg' was provided but is generally ignored.")

    # The old logic for reading file from input_data is removed. input_data is messages_payload.
    # if os.path.isfile(input_data): ... This is no longer valid.

    api_messages = []
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. DeepSeek is OpenAI compatible.
    api_messages.extend(input_data)

    headers = {
        'Authorization': f'Bearer {deepseek_api_key}',
        'Content-Type': 'application/json'
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "stream": current_streaming,
        "temperature": current_temp,
        "top_p": current_top_p,
    }
    if current_max_tokens is not None:
        data["max_tokens"] = current_max_tokens

    api_url = 'https://api.deepseek.com/chat/completions'
    try:
        if current_streaming:
            logging.debug("DeepSeek: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"DeepSeek: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"DeepSeek: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("DeepSeek: Posting request (non-streaming)")
            retry_count = int(deepseek_config.get('api_retries', 3))
            retry_delay = float(deepseek_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("DeepSeek: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"DeepSeek: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="deepseek", message=f"DeepSeek config/data error: {e}") from e


def chat_with_mistral(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message'
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None  # Mapped from 'topp' (becomes top_p)
):
    logging.debug("Mistral: Chat request started")
    loaded_config_data = load_and_log_configs()
    mistral_config = loaded_config_data.get('mistral_api', {})

    mistral_api_key = api_key or mistral_config.get('api_key')
    if not mistral_api_key:
        logging.error("Mistral: API key is missing.")
        raise ChatConfigurationError(provider="mistral", message="Mistral API Key is required but not found.")

    log_key = f"{mistral_api_key[:5]}...{mistral_api_key[-5:]}" if mistral_api_key and len(
        mistral_api_key) > 9 else "Provided Key"
    logging.debug(f"Mistral: Using API Key: {log_key}")

    current_model = model or mistral_config.get('model', 'mistral-large-latest')
    current_temp = temp if temp is not None else float(mistral_config.get('temperature', 0.1))
    current_top_p = topp if topp is not None else float(mistral_config.get('top_p', 0.95))
    current_streaming = streaming if streaming is not None else mistral_config.get('streaming', False)
    safe_prompt = bool(mistral_config.get('safe_prompt', False))
    current_max_tokens = _safe_cast(mistral_config.get('max_tokens'), int)

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="mistral",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Mistral: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, SafePrompt: {safe_prompt}")
    if custom_prompt_arg:
        logging.warning("Mistral: 'custom_prompt_arg' was provided but is generally ignored.")

    # The old logic for extract_text_from_segments is removed. input_data is messages_payload.

    api_messages = []
    # Mistral API uses system message as the first message in the list if role is "system"
    if system_message:
        api_messages.append({"role": "system", "content": system_message})

    # input_data is messages_payload. Mistral API supports OpenAI message format including multimodal.
    api_messages.extend(input_data)

    headers = {
        'Authorization': f'Bearer {mistral_api_key}',
        'Content-Type': 'application/json',
        'Accept': 'application/json'
    }
    data = {
        "model": current_model,
        "messages": api_messages,
        "temperature": current_temp,
        "top_p": current_top_p,
        "stream": current_streaming,
        "safe_prompt": safe_prompt
    }
    if current_max_tokens is not None:
        data["max_tokens"] = current_max_tokens

    api_url = 'https://api.mistral.ai/v1/chat/completions'
    try:
        if current_streaming:
            logging.debug("Mistral: Posting request (streaming)")
            with requests.Session() as session:
                response = session.post(api_url, headers=headers, json=data, stream=True, timeout=180)
                response.raise_for_status()

                def stream_generator():
                    try:
                        for line in response.iter_lines(decode_unicode=True):
                            if line and line.strip():
                                yield line + "\n\n"
                        yield "data: [DONE]\n\n"
                    except requests.exceptions.ChunkedEncodingError as e:
                        logging.error(f"Mistral: ChunkedEncodingError during stream: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    except Exception as e:
                        logging.error(f"Mistral: Error during stream iteration: {e}", exc_info=True)
                        error_payload = json.dumps(
                            {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                        yield f"data: {error_payload}\n\n"
                        yield "data: [DONE]\n\n"
                    finally:
                        response.close()

                return stream_generator()
        else:
            logging.debug("Mistral: Posting request (non-streaming)")
            retry_count = int(mistral_config.get('api_retries', 3))
            retry_delay = float(mistral_config.get('api_retry_delay', 1))
            retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay,
                                   status_forcelist=[429, 500, 502, 503, 504], allowed_methods=["POST"])
            adapter = HTTPAdapter(max_retries=retry_strategy)
            with requests.Session() as session:
                session.mount("https://", adapter)
                response = session.post(api_url, headers=headers, json=data, timeout=120)
            response.raise_for_status()
            response_data = response.json()
            logging.debug("Mistral: Non-streaming request successful.")
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Mistral: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="mistral", message=f"Mistral config/data error: {e}") from e


def chat_with_google(
        model: Optional[str],  # Mapped from 'model'
        input_data: List[Dict[str, Any]],  # Mapped from 'input_data' (messages_payload)
        api_key: Optional[str] = None,  # Mapped from 'api_key'
        custom_prompt_arg: Optional[str] = None,  # Mapped from 'prompt', ignored
        temp: Optional[float] = None,  # Mapped from 'temp'
        system_message: Optional[str] = None,  # Mapped from 'system_message' (becomes system_instruction)
        streaming: Optional[bool] = None,  # Mapped from 'streaming'
        topp: Optional[float] = None,  # Mapped from 'topp' (becomes topP for Gemini)
        topk: Optional[int] = None  # Mapped from 'topk' (becomes topK for Gemini)
):
    logging.debug("Google Gemini: Chat request process starting...")
    loaded_config_data = load_and_log_configs()
    google_config = loaded_config_data.get('google_api', {})

    google_api_key = api_key or google_config.get('api_key')
    if not google_api_key:
        logging.error("Google Gemini: API key is missing.")
        raise ChatConfigurationError(provider="google", message="Google API Key is required but not found.")

    log_key = f"{google_api_key[:5]}...{google_api_key[-5:]}" if google_api_key and len(
        google_api_key) > 9 else "Provided Key"
    logging.debug(f"Google Gemini: Using API Key: {log_key}")

    current_model = model or google_config.get('model', 'gemini-1.5-flash-latest')  # Use flash for speed/cost
    current_temp = temp if temp is not None else float(google_config.get('temperature', 0.7))
    current_top_p = topp  # 'topp' from chat_api_call maps to Gemini's 'topP'
    current_top_k = topk  # 'topk' from chat_api_call maps to Gemini's 'topK'
    current_streaming = streaming if streaming is not None else google_config.get('streaming', False)
    current_system_instruction_text = system_message  # 'system_message' from chat_api_call maps to Gemini's system_instruction
    max_output_tokens = _safe_cast(google_config.get('max_output_tokens', 8192), int)  # Gemini uses maxOutputTokens

    if isinstance(current_streaming, str):
        current_streaming = current_streaming.lower() == "true"
    elif isinstance(current_streaming, int):
        current_streaming = bool(current_streaming)
    if not isinstance(current_streaming, bool):
        raise ChatConfigurationError(provider="google",
                                     message=f"Invalid type for 'streaming': Expected boolean, got {type(current_streaming).__name__}")

    logging.debug(
        f"Google Gemini: Model: {current_model}, Streaming: {current_streaming}, Temp: {current_temp}, TopP: {current_top_p}, TopK: {current_top_k}")
    if custom_prompt_arg:
        logging.warning("Google Gemini: 'custom_prompt_arg' was provided but is generally ignored.")

    # Transform OpenAI messages_payload to Gemini's 'contents' format
    gemini_contents = []
    for msg in input_data:
        role = msg.get("role")
        content = msg.get("content")  # Can be string or list of parts

        gemini_role = "user" if role == "user" else "model" if role == "assistant" else None
        if not gemini_role:
            logging.warning(f"Google Gemini: Skipping message with unsupported role: {role}")
            continue

        gemini_parts = []
        if isinstance(content, str):
            gemini_parts.append({"text": content})
        elif isinstance(content, list):
            for part in content:
                part_type = part.get("type")
                if part_type == "text":
                    gemini_parts.append({"text": part.get("text", "")})
                elif part_type == "image_url":
                    image_url_data = part.get("image_url", {}).get("url", "")
                    parsed_image = _parse_data_url_for_multimodal(image_url_data)
                    if parsed_image:
                        mime_type, b64_data = parsed_image
                        gemini_parts.append({
                            "inline_data": {
                                "mime_type": mime_type,
                                "data": b64_data
                            }
                        })
                    else:
                        logging.warning(f"Google Gemini: Could not parse image_url, skipping: {image_url_data[:60]}...")
                        # Optionally add a placeholder text
                        # gemini_parts.append({"text": "[Image data unparseable or not provided]"})

        if gemini_parts:
            gemini_contents.append({"role": gemini_role, "parts": gemini_parts})
        elif gemini_role == "user" and not gemini_parts:
            logging.warning(f"Google Gemini: User message has no processable content parts. Adding placeholder.")
            gemini_contents.append({"role": gemini_role,
                                    "parts": [{"text": "(User input was empty or only contained unparseable images)"}]})

    if not any(c['role'] == 'user' for c in gemini_contents):
        raise ChatBadRequestError(provider="google",
                                  message="No valid user messages found after processing for Google Gemini.")

    generation_config = {}
    if current_temp is not None: generation_config["temperature"] = current_temp
    if current_top_p is not None: generation_config["topP"] = current_top_p
    if current_top_k is not None: generation_config["topK"] = current_top_k
    if max_output_tokens is not None: generation_config["maxOutputTokens"] = max_output_tokens

    payload = {
        "contents": gemini_contents,
    }
    if generation_config: payload["generationConfig"] = generation_config
    if current_system_instruction_text:
        payload["system_instruction"] = {"parts": [{"text": current_system_instruction_text}]}

    stream_suffix = ":streamGenerateContent?alt=sse" if current_streaming else ":generateContent"
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/{current_model}{stream_suffix}"
    headers = {
        'x-goog-api-key': google_api_key,
        'Content-Type': 'application/json',
    }

    try:
        retry_count = int(google_config.get('api_retries', 3))
        retry_delay = float(google_config.get('api_retry_delay', 1))
        retry_strategy = Retry(total=retry_count, backoff_factor=retry_delay, status_forcelist=[429, 500, 503],
                               allowed_methods=["POST"])  # Google uses 400 for some quota issues
        adapter = HTTPAdapter(max_retries=retry_strategy)
        with requests.Session() as session:
            session.mount("https://", adapter)
            response = session.post(api_url, headers=headers, json=payload, stream=current_streaming, timeout=180)

        response.raise_for_status()

        if current_streaming:
            logging.debug("Google Gemini: Streaming response received.")

            def stream_generator():
                try:
                    for line in response.iter_lines(decode_unicode=True):
                        if line and line.strip().startswith('data:'):
                            json_str = line.strip()[len('data:'):]
                            try:
                                data_chunk = json.loads(json_str)
                                # Gemini stream format is a list of GenerateContentResponse
                                # Each response has candidates -> content -> parts -> text
                                chunk_text = ""
                                if isinstance(data_chunk, list):  # Sometimes it's a list of chunks
                                    for sub_chunk in data_chunk:
                                        candidates = sub_chunk.get('candidates', [])
                                        if candidates and candidates[0].get('content', {}).get('parts', []):
                                            chunk_text += candidates[0]['content']['parts'][0].get('text', '')
                                else:  # Single chunk object
                                    candidates = data_chunk.get('candidates', [])
                                    if candidates and candidates[0].get('content', {}).get('parts', []):
                                        chunk_text = candidates[0]['content']['parts'][0].get('text', '')

                                if chunk_text:
                                    # Yield in OpenAI-like delta format for consistency downstream
                                    yield f"data: {json.dumps({'choices': [{'delta': {'content': chunk_text}}]})}\n\n"
                            except json.JSONDecodeError:
                                logging.warning(f"Google Gemini: Could not decode JSON line from stream: {line}")
                    yield "data: [DONE]\n\n"
                except requests.exceptions.ChunkedEncodingError as e:
                    logging.error(f"Google Gemini: ChunkedEncodingError during stream: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream connection error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                except Exception as e:
                    logging.error(f"Google Gemini: Error during stream iteration: {e}", exc_info=True)
                    error_payload = json.dumps(
                        {"error": {"message": f"Stream iteration error: {str(e)}", "type": "stream_error"}})
                    yield f"data: {error_payload}\n\n"
                    yield "data: [DONE]\n\n"
                finally:
                    response.close()

            return stream_generator()
        else:
            logging.debug("Google Gemini: Non-streaming request successful.")
            response_data = response.json()
            # Normalize Gemini response to OpenAI-like if needed by callers, or document the raw format.
            # For now, returning raw.
            return response_data
    except (ValueError, KeyError, TypeError) as e:
        logging.error(f"Google Gemini: Configuration or data error: {e}", exc_info=True)
        raise ChatBadRequestError(provider="google", message=f"Google Gemini config/data error: {e}") from e

#
#
#######################################################################################################################